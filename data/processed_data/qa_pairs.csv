question,answer,context
What are the main differences between DeepSeek-R1-Zero and DeepSeek-R1 models?,"DeepSeek-R1-Zero is trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, which leads to remarkable reasoning capabilities but also challenges like poor readability and language mixing. DeepSeek-R1 addresses these issues by incorporating multi-stage training and cold-start data before RL, enhancing its reasoning performance to be comparable to OpenAI-o1-1217 on reasoning tasks.","DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via
Reinforcement Learning
DeepSeek-AI
research@deepseek.com
Abstract
We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.
DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super-
vised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.
Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing
reasoning behaviors. However, it encounters challenges such as poor readability, and language
mixing. To address these issues and further enhance reasoning performance, we introduce
DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-
R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the
research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models
(1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.
AIME 2"
What is the accuracy percentage of DeepSeek-R1 on the MATH-500 benchmark?,The accuracy percentage of DeepSeek-R1 on the MATH-500 benchmark is 90.8%.,"024
(Pass@1)Codeforces
(Percentile)GPQA Diamond
(Pass@1)MATH-500
(Pass@1)MMLU
(Pass@1)SWE-bench Verified
(Resolved)020406080100Accuracy / Percentile (%)79.896.3
71.597.3
90.8
49.279.296.6
75.796.4
91.8
48.972.690.6
62.194.3
87.4
36.863.693.4
60.090.0
85.2
41.6
39.258.7 59.190.2
88.5
42.0DeepSeek-R1 OpenAI-o1-1217 DeepSeek-R1-32B OpenAI-o1-mini DeepSeek-V3
Figure 1|Benchmark performance of DeepSeek-R1.arXiv:2501.12948v1  [cs.CL]  22 Jan 2025Contents
1 Introduction 3
1.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.2 Summary of Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2 Approach 5
2.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2.2 DeepSeek-R1-Zero: Reinforcement Learning on the Base Model . . . . . . . . . . 5
2.2.1 Reinforcement Learning Algorithm . . . . . . . . . . . . . . . . . . . . . . 5
2.2.2 Reward Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . "
What is the focus of section 2.3 in the document?,"Section 2.3 focuses on DeepSeek-R1, specifically discussing Reinforcement Learning with Cold Start, including aspects like Cold Start, Reasoning-oriented Reinforcement Learning, Rejection Sampling and Supervised Fine-Tuning, and Reinforcement Learning for all Scenarios.",". . . 6
2.2.3 Training Template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.2.4 Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero 6
2.3 DeepSeek-R1: Reinforcement Learning with Cold Start . . . . . . . . . . . . . . . 9
2.3.1 Cold Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.3.2 Reasoning-oriented Reinforcement Learning . . . . . . . . . . . . . . . . . 10
2.3.3 Rejection Sampling and Supervised Fine-Tuning . . . . . . . . . . . . . . . 10
2.3.4 Reinforcement Learning for all Scenarios . . . . . . . . . . . . . . . . . . . 11
2.4 Distillation: Empower Small Models with Reasoning Capability . . . . . . . . . . 11
3 Experiment 11
3.1 DeepSeek-R1 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
3.2 Distilled Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
4 Discussion 14
4.1 Distillation v.s. Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . 14
"
What recent development in Large Language Models (LLMs) has been shown to enhance accuracy on reasoning tasks and align with social values?,"Post-training has emerged as an important component of the full training pipeline, enhancing accuracy on reasoning tasks, aligning with social values, and adapting to user preferences, while requiring relatively minimal computational resources compared to pre-training.","4.2 Unsuccessful Attempts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
5 Conclusion, Limitations, and Future Work 16
A Contributions and Acknowledgments 20
21. Introduction
In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and
evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap
towards Artificial General Intelligence (AGI).
Recently, post-training has emerged as an important component of the full training pipeline.
It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt
to user preferences, all while requiring relatively minimal computational resources against
pre-training. In the context of reasoning capabilities, OpenAI’s o1 (OpenAI, 2024b) series models
were the first to introduce inference-time scaling by increasing the length of the Chain-of-
Thought reasoning process. This approach has achieved significant improvements in various
reasoning tasks, such as mathematics, c"
What is the main goal of the research discussed in the text regarding language model reasoning capabilities?,"The main goal of the research is to explore the potential of language models (LLMs) to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure reinforcement learning (RL) process.","oding, and scientific reasoning. However, the challenge
of effective test-time scaling remains an open question for the research community. Several prior
works have explored various approaches, including process-based reward models (Lightman
et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024),
and search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh
et al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning
performance comparable to OpenAI’s o1 series models.
In this paper, we take the first step toward improving language model reasoning capabilities
using pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop
reasoning capabilities without any supervised data, focusing on their self-evolution through
a pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ
GRPO (Shao et al., 2024) as the RL framework to improve model performa"
What improvements does DeepSeek-R1-Zero show in reasoning benchmarks after training?,"After training, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks, with the pass@1 score on AIME 2024 increasing from 15.6% to 71.0%, and further improving to 86.7% with majority voting, matching the performance of OpenAI-o1-0912.","nce in reasoning.
During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting
reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance
on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to
71.0%, and with majority voting, the score further improves to 86.7%, matching the performance
of OpenAI-o1-0912.
However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language
mixing. To address these issues and further enhance reasoning performance, we introduce
DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training
pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the
DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-
Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection
sampling on the RL checkpoint, combined with supervised data from DeepS"
What is the significance of the distilled 14B model mentioned in the text?,"The distilled 14B model outperforms the state-of-the-art open-source QwQ-32B-Preview by a large margin, indicating its superior reasoning capabilities.","eek-V3 in domains
such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model.
After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking
into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to
as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217.
We further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-
32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying
RL on it. This demonstrates that the reasoning patterns discovered by larger base models are cru-
cial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey
et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source
QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a
new record on the reasoning benchmarks among dense models.
31.1. Contributions
Post-T"
What is the significance of applying reinforcement learning directly to the base model in the development of DeepSeek-R1-Zero?,"Applying reinforcement learning directly to the base model, without relying on supervised fine-tuning, allows the model to explore chain-of-thought for solving complex problems. This approach has led to the development of DeepSeek-R1-Zero, which demonstrates capabilities such as self-verification, reflection, and generating long chains of thought. It marks a significant milestone by validating that reasoning capabilities of large language models can be incentivized purely through reinforcement learning, paving the way for future advancements in this area.","raining: Large-Scale Reinforcement Learning on the Base Model
•We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as
a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for
solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-
R1-Zero demonstrates capabilities such as self-verification, reflection, and generating
long CoTs, marking a significant milestone for the research community. Notably, it is the
first open research to validate that reasoning capabilities of LLMs can be incentivized
purely through RL, without the need for SFT. This breakthrough paves the way for future
advancements in this area.
•We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL
stages aimed at discovering improved reasoning patterns and aligning with human pref-
erences, as well as two SFT stages that serve as the seed for the model’s reasoning and
non-reasoning capabilities. We believe the pipeline will "
What is the performance of the DeepSeek-R1-Distill-Qwen-7B model on the AIME 2024 benchmark?,The DeepSeek-R1-Distill-Qwen-7B model achieves a performance score of 55.5% on the AIME 2024 benchmark.,"benefit the industry by creating
better models.
Distillation: Smaller Models Can Be Powerful Too
•We demonstrate that the reasoning patterns of larger models can be distilled into smaller
models, resulting in better performance compared to the reasoning patterns discovered
through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit
the research community to distill better smaller models in the future.
•Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models
that are widely used in the research community. The evaluation results demonstrate that
the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-
R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Addi-
tionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500,
and 57.2% on LiveCodeBench. These results significantly outperform previous open-
source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B"
How does DeepSeek-R1 perform on the AIME 2024 reasoning task compared to OpenAI-o1-1217?,"DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-o1-1217.",", 8B, 14B,
32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.
1.2. Summary of Evaluation Results
•Reasoning tasks : (1) DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly
surpassing OpenAI-o1-1217. On MATH-500, it attains an impressive score of 97.3%,
performing on par with OpenAI-o1-1217 and significantly outperforming other models. (2)
On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks,
as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in
the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than
DeepSeek-V3, which could help developers in real world tasks.
•Knowledge : On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-
R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores
of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its
performance is slightly below that of OpenAI-o1-1217 on these benchmarks"
What is the win-rate of DeepSeek-R1 on the AlpacaEval 2.0 benchmark?,DeepSeek-R1 achieves a length-controlled win-rate of 87.6% on the AlpacaEval 2.0 benchmark.,", DeepSeek-R1
surpasses other closed-source models, demonstrating its competitive edge in educational
tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,
demonstrating its capability in handling fact-based queries. A similar trend is observed
where OpenAI-o1 surpasses 4o on this benchmark.
4•Others : DeepSeek-R1 also excels in a wide range of tasks, including creative writing,
general question answering, editing, summarization, and more. It achieves an impressive
length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on Are-
naHard, showcasing its strong ability to intelligently handle non-exam-oriented queries.
Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring
long-context understanding, substantially outperforming DeepSeek-V3 on long-context
benchmarks.
2. Approach
2.1. Overview
Previous work has heavily relied on large amounts of supervised data to enhance model
performance. In this study, we demonstrate that reasoning capabi"
What is DeepSeek-R1-Zero and how does it differ from DeepSeek-R1?,"DeepSeek-R1-Zero is a model that applies reinforcement learning (RL) directly to the base model without any supervised fine-tuning (SFT) data. In contrast, DeepSeek-R1 applies RL starting from a checkpoint that has been fine-tuned with thousands of long Chain-of-Thought (CoT) examples.","lities can be significantly
improved through large-scale reinforcement learning (RL), even without using supervised
fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with
the inclusion of a small amount of cold-start data. In the following sections, we present: (1)
DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and
(2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of
long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to
small dense models.
2.2. DeepSeek-R1-Zero: Reinforcement Learning on the Base Model
Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as ev-
idenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works
heavily depended on supervised data, which are time-intensive to gather. In this section, we
explore the potential of LLMs to develop reasoning capabilities without any supervised data"
What is the main advantage of using Group Relative Policy Optimization (GRPO) in reinforcement learning?,"The main advantage of using Group Relative Policy Optimization (GRPO) in reinforcement learning is that it saves training costs by foregoing the critic model, which is typically the same size as the policy model, and instead estimates the baseline from group scores."," ,
focusing on their self-evolution through a pure reinforcement learning process. We start with a
brief overview of our RL algorithm, followed by the presentation of some exciting results, and
hope this provides the community with valuable insights.
2.2.1. Reinforcement Learning Algorithm
Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group
Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is
typically the same size as the policy model, and estimates the baseline from group scores instead.
Specifically, for each question 𝑞, GRPO samples a group of outputs {𝑜1,𝑜2,···,𝑜𝐺}from the old
policy𝜋𝜃𝑜𝑙𝑑and then optimizes the policy model 𝜋𝜃by maximizing the following objective:
J𝐺𝑅𝑃𝑂(𝜃)=E[𝑞∼𝑃(𝑄),{𝑜𝑖}𝐺
𝑖=1∼𝜋𝜃𝑜𝑙𝑑(𝑂|𝑞)]
1
𝐺𝐺∑︁
𝑖=1
min𝜋𝜃(𝑜𝑖|𝑞)
𝜋𝜃𝑜𝑙𝑑(𝑜𝑖|𝑞)𝐴𝑖, clip𝜋𝜃(𝑜𝑖|𝑞)
𝜋𝜃𝑜𝑙𝑑(𝑜𝑖|𝑞), 1−𝜀, 1+𝜀
𝐴𝑖
−𝛽D𝐾𝐿 
𝜋𝜃||𝜋𝑟𝑒𝑓
,(1)
D𝐾𝐿 
𝜋𝜃||𝜋𝑟𝑒𝑓=𝜋𝑟𝑒𝑓(𝑜𝑖|𝑞)
𝜋𝜃(𝑜𝑖|𝑞)−log𝜋𝑟𝑒𝑓(𝑜𝑖|𝑞)
𝜋𝜃(𝑜𝑖|𝑞)−1, (2)
where𝜀and𝛽are hyper-parameters, and 𝐴𝑖is the advantag"
What is the purpose of the reward system in training DeepSeek-R1-Zero?,"The reward system in training DeepSeek-R1-Zero serves as the source of the training signal, which determines the optimization direction of reinforcement learning (RL). It consists of a rule-based reward system that mainly includes accuracy rewards to evaluate the response.","e, computed using a group of
rewards{𝑟1,𝑟2,...,𝑟𝐺}corresponding to the outputs within each group:
𝐴𝑖=𝑟𝑖−m𝑒𝑎𝑛({𝑟1,𝑟2,···,𝑟𝐺})
s𝑡𝑑({𝑟1,𝑟2,···,𝑟𝐺}). (3)
5A conversation between User and Assistant. The user asks a question, and the Assistant solves it.
The assistant first thinks about the reasoning process in the mind and then provides the user
with the answer. The reasoning process and answer are enclosed within <think> </think> and
<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think>
<answer> answer here </answer>. User: prompt. Assistant:
Table 1|Template for DeepSeek-R1-Zero. prompt will be replaced with the specific reasoning
question during training.
2.2.2. Reward Modeling
The reward is the source of the training signal, which decides the optimization direction of RL.
To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two
types of rewards:
•Accuracy rewards : The accuracy reward model evaluates whether the response is correct.
For example, "
Why is a format reward model used in the training of DeepSeek-R1-Zero?,"A format reward model is used to enforce the model to put its thinking process between '<think>' and '</think>' tags, ensuring that the model adheres to a specified format for clarity and consistency.","in the case of math problems with deterministic results, the model is required
to provide the final answer in a specified format (e.g., within a box), enabling reliable
rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be
used to generate feedback based on predefined test cases.
•Format rewards : In addition to the accuracy reward model, we employ a format reward
model that enforces the model to put its thinking process between ‘<think>’ and ‘</think>’
tags.
We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero,
because we find that the neural reward model may suffer from reward hacking in the large-scale
reinforcement learning process, and retraining the reward model needs additional training
resources and it complicates the whole training pipeline.
2.2.3. Training Template
To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides
the base model to adhere to our specified instructions. As depicted in Tabl"
What was the initial and final pass@1 score of DeepSeek-R1-Zero on the AIME 2024 benchmark during the RL training process?,"The initial pass@1 score of DeepSeek-R1-Zero on the AIME 2024 benchmark was 15.6%, and it increased to an impressive 71.0% by the end of the RL training process.","e 1, this template
requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer.
We intentionally limit our constraints to this structural format, avoiding any content-specific
biases—such as mandating reflective reasoning or promoting particular problem-solving strate-
gies—to ensure that we can accurately observe the model’s natural progression during the RL
process.
2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero
Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-
R1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated,
DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the
RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant
increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels
comparable to OpenAI-o1-0912. This significant improvement highlights the efficacy of our RL
algo"
What is the primary achievement of the DeepSeek-R1-Zero model as highlighted in the text?,"The primary achievement of the DeepSeek-R1-Zero model is its ability to attain robust reasoning capabilities without the need for any supervised fine-tuning data, demonstrating effective learning and generalization through reinforcement learning (RL) alone.","rithm in optimizing the model’s performance over time.
Table 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAI’s o1-0912
models across a variety of reasoning-related benchmarks. The findings reveal that RL empowers
6ModelAIME 2024 MATH-500GPQA LiveCodeCodeForcesDiamond Bench
pass@1 cons@64 pass@1 pass@1 pass@1 rating
OpenAI-o1-mini 63.6 80.0 90.0 60.0 53.8 1820
OpenAI-o1-0912 74.4 83.3 94.8 77.3 63.4 1843
DeepSeek-R1-Zero 71.0 86.7 95.9 73.3 50.0 1444
Table 2|Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related
benchmarks.
Figure 2|AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample
16 responses and calculate the overall average accuracy to ensure a stable evaluation.
DeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised
fine-tuning data. This is a noteworthy achievement, as it underscores the model’s ability to
learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-"
How does majority voting impact the performance of DeepSeek-R1-Zero on the AIME benchmark?,"Majority voting increases DeepSeek-R1-Zero's performance on the AIME benchmark from 71.0% to 86.7%, surpassing the performance of OpenAI-o1-0912.","
R1-Zero can be further augmented through the application of majority voting. For example,
when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zero’s performance
escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The
ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without
majority voting, highlights its strong foundational capabilities and its potential for further
advancements in reasoning tasks.
Self-evolution Process of DeepSeek-R1-Zero The self-evolution process of DeepSeek-R1-Zero
is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities
autonomously. By initiating RL directly from the base model, we can closely monitor the model’s
progression without the influence of the supervised fine-tuning stage. This approach provides
a clear view of how the model evolves over time, particularly in terms of its ability to handle
complex reasoning tasks.
As depicted in Figure 3, the thinking tim"
How does DeepSeek-R1-Zero improve its ability to solve complex reasoning tasks?,"DeepSeek-R1-Zero improves its ability to solve complex reasoning tasks by leveraging extended test-time computation, which allows it to generate hundreds to thousands of reasoning tokens. This enables the model to explore and refine its thought processes in greater depth, leading to the emergence of sophisticated behaviors such as reflection and the exploration of alternative problem-solving approaches.","e of DeepSeek-R1-Zero shows consistent improve-
7Figure 3|The average response length of DeepSeek-R1-Zero on the training set during the RL
process. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time.
ment throughout the training process. This improvement is not the result of external adjustments
but rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the
ability to solve increasingly complex reasoning tasks by leveraging extended test-time compu-
tation. This computation ranges from generating hundreds to thousands of reasoning tokens,
allowing the model to explore and refine its thought processes in greater depth.
One of the most remarkable aspects of this self-evolution is the emergence of sophisticated
behaviors as the test-time computation increases. Behaviors such as reflection—where the model
revisits and reevaluates its previous steps—and the exploration of alternative approaches to
problem-solving arise spontaneously. These behaviors a"
"What is the ""aha moment"" in the context of DeepSeek-R1-Zero's training?","The ""aha moment"" refers to a phenomenon observed during the training of DeepSeek-R1-Zero where the model learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior demonstrates the model's growing reasoning abilities and highlights the sophisticated outcomes that can emerge from reinforcement learning.","re not explicitly programmed but instead
emerge as a result of the model’s interaction with the reinforcement learning environment. This
spontaneous development significantly enhances DeepSeek-R1-Zero’s reasoning capabilities,
enabling it to tackle more challenging tasks with greater efficiency and accuracy.
Aha Moment of DeepSeek-R1-Zero A particularly intriguing phenomenon observed during
the training of DeepSeek-R1-Zero is the occurrence of an “aha moment”. This moment, as
illustrated in Table 3, occurs in an intermediate version of the model. During this phase,
DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial
approach. This behavior is not only a testament to the model’s growing reasoning abilities
but also a captivating example of how reinforcement learning can lead to unexpected and
sophisticated outcomes.
This moment is not only an “aha moment” for the model but also for the researchers
observing its behavior. It underscores the power and beauty of reinfor"
"What is the concept of ""cement learning"" in reinforcement learning?","Cement learning in reinforcement learning refers to the approach of not explicitly teaching the model how to solve a problem. Instead, the model is provided with the right incentives, allowing it to autonomously develop advanced problem-solving strategies. This method highlights the potential of reinforcement learning to enhance the intelligence of artificial systems, leading to more autonomous and adaptive models in the future.","cement learning: rather
than explicitly teaching the model on how to solve a problem, we simply provide it with the
right incentives, and it autonomously develops advanced problem-solving strategies. The
“aha moment” serves as a powerful reminder of the potential of RL to unlock new levels of
intelligence in artificial systems, paving the way for more autonomous and adaptive models in
the future.
8Question: If 𝑎 >1, then the sum of the real solutions of√︁
𝑎−√
𝑎+𝑥=𝑥is equal to
Response: <think>
To solve the equation√︁
𝑎−√
𝑎+𝑥=𝑥, let’s start by squaring both ···√︁
𝑎−√
𝑎+𝑥2
=𝑥2=⇒𝑎−√
𝑎+𝑥=𝑥2.
Rearrange to isolate the inner square root term:
(𝑎−𝑥2)2=𝑎+𝑥=⇒𝑎2−2𝑎𝑥2+(𝑥2)2=𝑎+𝑥=⇒𝑥4−2𝑎𝑥2−𝑥+(𝑎2−𝑎)=0
. . .
Wait, wait. Wait. That’s an aha moment I can flag here.
Let’s reevaluate this step-by-step to identify if the correct sum can be ···
We started with the equation:√︁
𝑎−√
𝑎+𝑥=𝑥
First, let’s square both sides:
𝑎−√
𝑎+𝑥=𝑥2=⇒√
𝑎+𝑥=𝑎−𝑥2
Next, I could square both sides again, treating the equation: ···
. . .
Table 3|An interest"
What are some of the challenges faced by DeepSeek-R1-Zero?,"DeepSeek-R1-Zero faces challenges such as poor readability and language mixing, despite its strong reasoning capabilities and unexpected powerful reasoning behaviors.","ing “aha moment” of an intermediate version of DeepSeek-R1-Zero. The
model learns to rethink using an anthropomorphic tone. This is also an aha moment for us,
allowing us to witness the power and beauty of reinforcement learning.
Drawback of DeepSeek-R1-Zero Although DeepSeek-R1-Zero exhibits strong reasoning
capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces
several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability,
and language mixing. To make reasoning processes more readable and share them with the
open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly
cold-start data.
2.3. DeepSeek-R1: Reinforcement Learning with Cold Start
Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can
reasoning performance be further improved or convergence accelerated by incorporating a small
amount of high-quality data as a cold start? 2) How can we train a user-friendly model that
"
What is the purpose of collecting a small amount of long CoT data in the cold start phase of training DeepSeek-R1?,"The purpose of collecting a small amount of long CoT data in the cold start phase is to fine-tune the model as the initial RL actor, preventing the early unstable cold start phase of RL training from the base model.","not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong
general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The
pipeline consists of four stages, outlined as follows.
2.3.1. Cold Start
Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from
the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data
to fine-tune the model as the initial RL actor. To collect such data, we have explored several
approaches: using few-shot prompting with a long CoT as an example, directly prompting
models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-
Zero outputs in a readable format, and refining the results through post-processing by human
annotators.
In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as
the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data
9include:
•Readabilit"
What is a key limitation of DeepSeek-R1-Zero mentioned in the text?,"A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading, as responses may mix multiple languages or lack markdown formatting to highlight answers for users.","y: A key limitation of DeepSeek-R1-Zero is that its content is often not suitable
for reading. Responses may mix multiple languages or lack markdown formatting to
highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1,
we design a readable pattern that includes a summary at the end of each response and
filters out responses that are not reader-friendly. Here, we define the output format as
|special_token|<reasoning_process>|special_token|<summary>, where the reasoning
process is the CoT for the query, and the summary is used to summarize the reasoning
results.
•Potential: By carefully designing the pattern for cold-start data with human priors, we
observe better performance against DeepSeek-R1-Zero. We believe the iterative training is
a better way for reasoning models.
2.3.2. Reasoning-oriented Reinforcement Learning
After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale
reinforcement learning training process as employed in DeepSeek-R1-Zero."
What strategy is used to address language mixing during RL training in reasoning-intensive tasks?,"To address language mixing during RL training, a language consistency reward is introduced, which is calculated as the proportion of target language words in the CoT. This reward aligns with human preferences and makes the output more readable, despite a slight degradation in the model's performance."," This phase focuses
on enhancing the model’s reasoning capabilities, particularly in reasoning-intensive tasks such
as coding, mathematics, science, and logic reasoning, which involve well-defined problems with
clear solutions. During the training process, we observe that CoT often exhibits language mixing,
particularly when RL prompts involve multiple languages. To mitigate the issue of language
mixing, we introduce a language consistency reward during RL training, which is calculated
as the proportion of target language words in the CoT. Although ablation experiments show
that such alignment results in a slight degradation in the model’s performance, this reward
aligns with human preferences, making it more readable. Finally, we combine the accuracy of
reasoning tasks and the reward for language consistency by directly summing them to form the
final reward. We then apply RL training on the fine-tuned model until it achieves convergence
on reasoning tasks.
2.3.3. Rejection Sampling and Supervised Fine-Tuning"
How is the dataset expanded in the reasoning-oriented RL stage for SFT data collection?,"In the reasoning-oriented RL stage for SFT data collection, the dataset is expanded by incorporating additional data that uses a generative reward model. This involves feeding the ground-truth and model predictions into DeepSeek-V3 for judgment, in addition to the rule-based rewards used in the previous stage.","
When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT
(Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which
primarily focuses on reasoning, this stage incorporates data from other domains to enhance the
model’s capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we
generate the data and fine-tune the model as described below.
Reasoning data We curate reasoning prompts and generate reasoning trajectories by perform-
ing rejection sampling from the checkpoint from the above RL training. In the previous stage,
we only included data that could be evaluated using rule-based rewards. However, in this stage,
we expand the dataset by incorporating additional data, some of which use a generative reward
model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment.
Additionally, because the model output is sometimes chaotic and difficult to read, we have
filtered out chain-of-thought"
What is the purpose of the secondary reinforcement learning stage mentioned in the text?,The purpose of the secondary reinforcement learning stage is to further align the model with human preferences.," with mixed languages, long parapraphs, and code blocks. For
each prompt, we sample multiple responses and retain only the correct ones. In total, we collect
about 600k reasoning related training samples.
10Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition,
and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of
DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential
chain-of-thought before answering the question by prompting. However, for simpler queries,
such as “hello” we do not provide a CoT in response. In the end, we collected a total of
approximately 200k training samples that are unrelated to reasoning.
We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about
800k samples.
2.3.4. Reinforcement Learning for all Scenarios
To further align the model with human preferences, we implement a secondary reinforcement
learning stage aimed at improving the model’s helpfulness an"
"What methodology is used for training the model's reasoning capabilities in math, code, and logical reasoning domains?","The methodology used for training the model's reasoning capabilities in math, code, and logical reasoning domains is outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process.","d harmlessness while simultane-
ously refining its reasoning capabilities. Specifically, we train the model using a combination
of reward signals and diverse prompt distributions. For reasoning data, we adhere to the
methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the
learning process in math, code, and logical reasoning domains. For general data, we resort to
reward models to capture human preferences in complex and nuanced scenarios. We build
upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and train-
ing prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the
assessment emphasizes the utility and relevance of the response to the user while minimizing
interference with the underlying reasoning process. For harmlessness, we evaluate the entire
response of the model, including both the reasoning process and the summary, to identify and
mitigate any potential risks, biases, or harmful content that may arise du"
What method is used to enhance the reasoning abilities of smaller models like Qwen and Llama?,"The reasoning abilities of smaller models like Qwen and Llama are enhanced through a straightforward distillation method, where these models are fine-tuned using 800k samples curated with DeepSeek-R1.","ring the generation
process. Ultimately, the integration of reward signals and diverse data distributions enables us
to train a model that excels in reasoning while prioritizing helpfulness and harmlessness.
2.4. Distillation: Empower Small Models with Reasoning Capability
To equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly
fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using
the 800k samples curated with DeepSeek-R1, as detailed in §2.3.3. Our findings indicate that
this straightforward distillation method significantly enhances the reasoning abilities of smaller
models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-
14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its
reasoning capability is slightly better than that of Llama-3.1.
For distilled models, we apply only SFT and do not include an RL stage, even though
incorporating RL could substantially boost model p"
What is the primary goal of the research described in the text?,"The primary goal of the research is to demonstrate the effectiveness of the distillation technique, while leaving the exploration of the reinforcement learning (RL) stage to the broader research community.","erformance. Our primary goal here is to
demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL
stage to the broader research community.
3. Experiment
Benchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema
et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al.,
2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al.,
2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI,
112024d), Aider1, LiveCodeBench (Jain et al., 2024) (2024-08 – 2025-01), Codeforces2, Chinese
National High School Mathematics Olympiad (CNMO 2024)3, and American Invitational Math-
ematics Examination 2024 (AIME 2024) (MAA, 2024). In addition to standard benchmarks, we
also evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we
adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li
et al., 20"
What evaluation prompt format is used for MMLU-Redux in the text?,"The Zero-Eval prompt format (Lin, 2024) is used for MMLU-Redux in a zero-shot setting.","24), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we
only feed the final summary to evaluation to avoid the length bias. For distilled models, we
report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and
LiveCodeBench.
Evaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as
MMLU, DROP , GPQA Diamond, and SimpleQA are evaluated using prompts from the simple-
evals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a
zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts
are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot
may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation
protocols with default prompts provided by their creators. For code and math benchmarks, the
HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++,
C#, JavaScript, TypeScript, PHP , and Bash). M"
What is the maximum token output for DeepSeek-R1 in the benchmarks?,"The maximum token output for DeepSeek-R1 in the benchmarks is capped at 32,768 tokens.","odel performance on LiveCodeBench is evaluated
using CoT format, with data collected between August 2024 and January 2025. The Codeforces
dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases,
after which the expected ratings and percentages of competitors are calculated. SWE-Bench
verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related
benchmarks are measured using a ""diff"" format. DeepSeek-R1 outputs are capped at a maximum
of 32,768 tokens for each benchmark.
Baselines We conduct comprehensive evaluations against several strong baselines, including
DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217.
Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its perfor-
mance based on official reports. For distilled models, we also compare the open-source model
QwQ-32B-Preview (Qwen, 2024a).
Evaluation Setup We set the maximum generation length to 32,768 tokens for the mode"
"What evaluation method is used to assess long-output reasoning models, and what parameters are set for generating responses?","The evaluation method used is pass@𝑘 evaluation, with a sampling temperature of 0.6 and a top-𝑝 value of 0.95 to generate 𝑘 responses for each question. Pass@1 is calculated to provide more reliable performance estimates.","ls.
We found that using greedy decoding to evaluate long-output reasoning models results in
higher repetition rates and significant variability across different checkpoints. Therefore, we
default to pass@ 𝑘evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature.
Specifically, we use a sampling temperature of 0.6and a top- 𝑝value of 0.95 to generate 𝑘
responses (typically between 4and 64, depending on the test set size) for each question. Pass@1
is then calculated as
pass@1 =1
𝑘𝑘∑︁
𝑖=1𝑝𝑖,
where𝑝𝑖denotes the correctness of the 𝑖-th response. This method provides more reliable
performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang
et al., 2022) using 64 samples, denoted as cons@64.
1https://aider.chat
2https://codeforces.com
3https://www.cms.org.cn/Home/comp/comp/cid/12.html
123.1. DeepSeek-R1 Evaluation
Benchmark (Metric)Claude-3.5- GPT-4o DeepSeek OpenAI OpenAI DeepSeek
Sonnet-1022 0513 V3 o1-mini o1-1217 R1
Architecture - - MoE - - MoE
# Activated Pa"
What is the highest recorded score for the MATH-500 (Pass@1) metric in the provided data?,The highest recorded score for the MATH-500 (Pass@1) metric is 97.3.,"rams - - 37B - - 37B
# Total Params - - 671B - - 671B
EnglishMMLU (Pass@1) 88.3 87.2 88.5 85.2 91.8 90.8
MMLU-Redux (EM) 88.9 88.0 89.1 86.7 - 92.9
MMLU-Pro (EM) 78.0 72.6 75.9 80.3 - 84.0
DROP (3-shot F1) 88.3 83.7 91.6 83.9 90.2 92.2
IF-Eval (Prompt Strict) 86.5 84.3 86.1 84.8 - 83.3
GPQA Diamond (Pass@1) 65.0 49.9 59.1 60.0 75.7 71.5
SimpleQA (Correct) 28.4 38.2 24.9 7.0 47.0 30.1
FRAMES (Acc.) 72.5 80.5 73.3 76.9 - 82.5
AlpacaEval2.0 (LC-winrate) 52.0 51.1 70.0 57.8 - 87.6
ArenaHard (GPT-4-1106) 85.2 80.4 85.5 92.0 - 92.3
CodeLiveCodeBench (Pass@1-COT) 38.9 32.9 36.2 53.8 63.4 65.9
Codeforces (Percentile) 20.3 23.6 58.7 93.4 96.6 96.3
Codeforces (Rating) 717 759 1134 1820 2061 2029
SWE Verified (Resolved) 50.8 38.8 42.0 41.6 48.9 49.2
Aider-Polyglot (Acc.) 45.3 16.0 49.6 32.9 61.7 53.3
MathAIME 2024 (Pass@1) 16.0 9.3 39.2 63.6 79.2 79.8
MATH-500 (Pass@1) 78.3 74.6 90.2 90.0 96.4 97.3
CNMO 2024 (Pass@1) 13.1 10.8 43.2 67.6 - 78.8
ChineseCLUEWSC (EM) 85.4 87.9 90.9 89.9 - 92.8
C-Eval (EM) 76.7 76.0 86.5 68."
What is the primary reason for DeepSeek-R1's improved performance on education-oriented knowledge benchmarks compared to DeepSeek-V3?,"The primary reason for DeepSeek-R1's improved performance on education-oriented knowledge benchmarks compared to DeepSeek-V3 is its enhanced accuracy in STEM-related questions, achieved through large-scale reinforcement learning.","9 - 91.8
C-SimpleQA (Correct) 55.4 58.7 68.0 40.3 - 63.7
Table 4|Comparison between DeepSeek-R1 and other representative models.
For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA
Diamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This im-
provement is primarily attributed to enhanced accuracy in STEM-related questions, where signif-
icant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1
excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis
capabilities. This highlights the potential of reasoning models in AI-driven search and data
analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,
demonstrating its capability in handling fact-based queries. A similar trend is observed where
OpenAI-o1 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than
DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to ref"
What improvements are linked to the inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training in DeepSeek-R1?,"The inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training in DeepSeek-R1 is linked to its impressive results on IF-Eval, a benchmark designed to assess a model’s ability to follow format instructions.","use
answering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an
accuracy of over 70%.
DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a
model’s ability to follow format instructions. These improvements can be linked to the inclusion
of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL
training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard,
indicating DeepSeek-R1’s strengths in writing tasks and open-domain question answering. Its
significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale
RL, which not only boosts reasoning capabilities but also improves performance across diverse
domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an
average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that
13DeepSeek-R1 avoids introducing length bias during GPT-based evaluations,"
How does DeepSeek-R1 perform on math tasks compared to OpenAI-o1-1217 and other models?,"On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217, surpassing other models by a large margin."," further solidifying
its robustness across multiple tasks.
On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217,
surpassing other models by a large margin. A similar trend is observed on coding algorithm
tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these
benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1
on Aider but achieves comparable performance on SWE Verified. We believe the engineering
performance of DeepSeek-R1 will improve in the next version, as the amount of related RL
training data currently remains very limited.
3.2. Distilled Model Evaluation
ModelAIME 2024 MATH-500GPQA LiveCodeCodeForcesDiamond Bench
pass@1 cons@64 pass@1 pass@1 pass@1 rating
GPT-4o-0513 9.3 13.4 74.6 49.9 32.9 759
Claude-3.5-Sonnet-1022 16.0 26.7 78.3 65.0 38.9 717
OpenAI-o1-mini 63.6 80.0 90.0 60.0 53.8 1820
QwQ-32B-Preview 50.0 60.0 90.6 54.5 41.9 1316
DeepSeek-R1-Distill-Qwen-1.5B 28.9 52.7 83.9 33.8 16.9 954
DeepSeek-R1-D"
What is the impact of distillation on DeepSeek-R1 models according to Table 5?,"Distillation of DeepSeek-R1’s outputs enables the efficient DeepSeek-R1-7B model to outperform non-reasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-Preview on all evaluation metrics, and DeepSeek-R1-32B and DeepSeek-R1-70B significantly exceed o1-mini on most benchmarks, demonstrating the strong potential of distillation.","istill-Qwen-7B 55.5 83.3 92.8 49.1 37.6 1189
DeepSeek-R1-Distill-Qwen-14B 69.7 80.0 93.9 59.1 53.1 1481
DeepSeek-R1-Distill-Qwen-32B 72.6 83.3 94.3 62.1 57.2 1691
DeepSeek-R1-Distill-Llama-8B 50.4 80.0 89.1 49.0 39.6 1205
DeepSeek-R1-Distill-Llama-70B 70.0 86.7 94.5 65.2 57.5 1633
Table 5|Comparison of DeepSeek-R1 distilled models and other comparable models on
reasoning-related benchmarks.
As shown in Table 5, simply distilling DeepSeek-R1’s outputs enables the efficient DeepSeek-
R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-
reasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-
Preview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly
exceed o1-mini on most benchmarks. These results demonstrate the strong potential of distilla-
tion. Additionally, we found that applying RL to these distilled models yields significant further
gains. We believe this warrants further exploration and therefore present o"
What were the results of the large-scale RL training on the Qwen-32B-Base model compared to the distilled model?,"The large-scale RL training on the Qwen-32B-Base model resulted in DeepSeek-R1-Zero-Qwen-32B, which showed slightly lower performance compared to the distilled model, DeepSeek-R1-Distill-Qwen-32B, across various benchmarks. For example, in the MATH-500 benchmark, the distilled model achieved a pass@1 score of 72.6, while the RL-trained model scored 47.0.","nly the results of the
simple SFT-distilled models here.
4. Discussion
4.1. Distillation v.s. Reinforcement Learning
In Section 3.2, we can see that by distilling DeepSeek-R1, the small model can achieve impressive
results. However, there is still one question left: can the model achieve comparable performance
through the large-scale RL training discussed in the paper without distillation?
To answer this question, we conduct large-scale RL training on Qwen-32B-Base using math,
code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The
experimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale
14ModelAIME 2024 MATH-500 GPQA Diamond LiveCodeBench
pass@1 cons@64 pass@1 pass@1 pass@1
QwQ-32B-Preview 50.0 60.0 90.6 54.5 41.9
DeepSeek-R1-Zero-Qwen-32B 47.0 60.0 91.6 55.0 40.2
DeepSeek-R1-Distill-Qwen-32B 72.6 83.3 94.3 62.1 57.2
Table 6|Comparison of distilled and RL Models on Reasoning-Related Benchmarks.
RL training, achieves performance on pa"
What are the two conclusions drawn from the performance comparison of DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-Zero-Qwen-32B?,"The two conclusions are: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on large-scale reinforcement learning require enormous computational power and may not achieve the performance of distillation. Second, while distillation strategies are economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger-scale reinforcement learning.","r with QwQ-32B-Preview. However, DeepSeek-R1-
Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than
DeepSeek-R1-Zero-Qwen-32B across all benchmarks.
Therefore, we can draw two conclusions: First, distilling more powerful models into smaller
ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in
this paper require enormous computational power and may not even achieve the performance
of distillation. Second, while distillation strategies are both economical and effective, advancing
beyond the boundaries of intelligence may still require more powerful base models and larger-
scale reinforcement learning.
4.2. Unsuccessful Attempts
In the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along
the way. We share our failure experiences here to provide insights, but this does not imply that
these approaches are incapable of developing effective reasoning models.
Process Reward Model (PRM) PRM is a reasonabl"
What are the main limitations of PRM in solving reasoning tasks?,"The main limitations of PRM in solving reasoning tasks are: 1) It is challenging to explicitly define a fine-grain step in general reasoning. 2) Determining whether the current intermediate step is correct is difficult, as automated annotation may not yield satisfactory results and manual annotation is not scalable. 3) Introducing a model-based PRM leads to reward hacking, requiring additional training resources for retraining the reward model and complicating the training pipeline.","e method to guide the model toward better
approaches for solving reasoning tasks (Lightman et al., 2023; Uesato et al., 2022; Wang et al.,
2023). However, in practice, PRM has three main limitations that may hinder its ultimate suc-
cess. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second,
determining whether the current intermediate step is correct is a challenging task. Automated
annotation using models may not yield satisfactory results, while manual annotation is not con-
ducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward
hacking (Gao et al., 2022), and retraining the reward model needs additional training resources
and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good
ability to rerank the top-N responses generated by the model or assist in guided search (Snell
et al., 2024), its advantages are limited compared to the additional computational overhead it
introduces during the lar"
What is the main challenge encountered when scaling up the training using Monte Carlo Tree Search (MCTS) in the described experiments?,"The main challenge is that, unlike chess where the search space is relatively well-defined, token generation presents an exponentially larger search space, making it difficult to scale up the training.","ge-scale reinforcement learning process in our experiments.
Monte Carlo Tree Search (MCTS) Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Sil-
ver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time
compute scalability. This approach involves breaking answers into smaller parts to allow the
model to explore the solution space systematically. To facilitate this, we prompt the model to
generate multiple tags that correspond to specific reasoning steps necessary for the search. For
training, we first use collected prompts to find answers via MCTS guided by a pre-trained value
model. Subsequently, we use the resulting question-answer pairs to train both the actor model
and the value model, iteratively refining the process.
However, this approach encounters several challenges when scaling up the training. First,
unlike chess, where the search space is relatively well-defined, token generation presents an
15exponentially larger search space. To address this, we set a max"
What is a significant challenge when using MCTS with a pre-trained value model during inference?,"A significant challenge is iteratively boosting model performance through self-search, as training a fine-grained value model is inherently difficult and can lead to the model getting stuck in local optima.","imum extension limit for each
node, but this can lead to the model getting stuck in local optima. Second, the value model
directly influences the quality of generation since it guides each step of the search process.
Training a fine-grained value model is inherently difficult, which makes it challenging for the
model to iteratively improve. While AlphaGo’s core success relied on training a value model to
progressively enhance its performance, this principle proves difficult to replicate in our setup
due to the complexities of token generation.
In conclusion, while MCTS can improve performance during inference when paired with a
pre-trained value model, iteratively boosting model performance through self-search remains a
significant challenge.
5. Conclusion, Limitations, and Future Work
In this work, we share our journey in enhancing model reasoning abilities through reinforcement
learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start
data, achieving strong performance across va"
What is the performance of DeepSeek-R1-Distill-Qwen-1.5B compared to GPT-4o and Claude-3.5-Sonnet on math benchmarks?,DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o and Claude-3.5-Sonnet on math benchmarks with scores of 28.9% on AIME and 83.9% on MATH.,"rious tasks. DeepSeek-R1 is more powerful,
leveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves
performance comparable to OpenAI-o1-1217 on a range of tasks.
We further explore distillation the reasoning capability to small dense models. We use
DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small
dense models. The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o
and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other
dense models also achieve impressive results, significantly outperforming other instruction-
tuned models based on the same underlying checkpoints.
In the future, we plan to invest in research across the following directions for DeepSeek-R1.
•General Capability: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3
in tasks such as function calling, multi-turn, complex role-playing, and JSON output.
Moving forward, we plan to explore how long CoT"
What is a current limitation of DeepSeek-R1 when handling queries in languages other than Chinese and English?,"DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. It might use English for reasoning and responses, even if the query is in a language other than English or Chinese."," can be leveraged to enhance tasks in
these fields.
•Language Mixing: DeepSeek-R1 is currently optimized for Chinese and English, which
may result in language mixing issues when handling queries in other languages. For
instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is
in a language other than English or Chinese. We aim to address this limitation in future
updates.
•Prompting Engineering: When evaluating DeepSeek-R1, we observe that it is sensitive
to prompts. Few-shot prompting consistently degrades its performance. Therefore, we
recommend users directly describe the problem and specify the output format using a
zero-shot setting for optimal results.
•Software Engineering Tasks: Due to the long evaluation times, which impact the effi-
ciency of the RL process, large-scale RL has not been applied extensively in software
engineering tasks. As a result, DeepSeek-R1 has not demonstrated a huge improvement
over DeepSeek-V3 on software engineering benchmarks. Future versions "
What methods are proposed to improve efficiency in the RL process according to the text?,The text proposes implementing rejection sampling on software engineering data or incorporating asynchronous evaluations during the RL process to improve efficiency.,"will address
this by implementing rejection sampling on software engineering data or incorporating
asynchronous evaluations during the RL process to improve efficiency.
16References
AI@Meta. Llama 3.1 model card, 2024. URL https://github.com/meta-llama/llama-m
odels/blob/main/models/llama3_1/MODEL_CARD.md .
Anthropic. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/claude-3
-5-sonnet .
M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P . de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda,
N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P . Mishkin,
B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P . Tillet,
F. P . Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss,
A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,
A. N. Carr, J. Leike, J. Achiam, V . Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,
M. Murati, K. Mayer, P . Welinder, B. McGrew, D. "
"What is the focus of the paper by Amodei, McCandlish, Sutskever, and Zaremba published in 2021?","The paper by Amodei, McCandlish, Sutskever, and Zaremba, published in 2021, focuses on evaluating large language models trained on code.","Amodei, S. McCandlish, I. Sutskever, and
W. Zaremba. Evaluating large language models trained on code. CoRR , abs/2107.03374, 2021.
URL https://arxiv.org/abs/2107.03374 .
A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten,
A. Yang, A. Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.
Y. Dubois, B. Galambosi, P . Liang, and T. B. Hashimoto. Length-controlled alpacaeval: A simple
way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024.
X. Feng, Z. Wan, M. Wen, S. M. McAleer, Y. Wen, W. Zhang, and J. Wang. Alphazero-like
tree-search can guide large language model decoding and training, 2024. URL https:
//arxiv.org/abs/2309.17179 .
L. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization, 2022. URL
https://arxiv.org/abs/2210.10760 .
A. P . Gema, J. O. J. Leang, G. Hong, A. Devoto, A. C. M. Mancino, R. Saxena, X. He, Y. Zhao,
X. Du, M. R. G. Madani, C. Barale, R. McHardy, J. Harris, J. Kaddour, E. van Kr"
What is the title of the paper by Y. He and colleagues mentioned in the text?,"The title of the paper by Y. He and colleagues is ""Chinese simpleqa: A chinese factuality evaluation for large language models.""","ieken, and
P . Minervini. Are we done with mmlu? CoRR , abs/2406.04127, 2024. URL https://doi.or
g/10.48550/arXiv.2406.04127 .
Google. Our next-generation model: Gemini 1.5, 2024. URL https://blog.google/techno
logy/ai/google-gemini-next-generation-model-february-2024 .
Y. He, S. Li, J. Liu, Y. Tan, W. Wang, H. Huang, X. Bu, H. Guo, C. Hu, B. Zheng, et al. Chi-
nese simpleqa: A chinese factuality evaluation for large language models. arXiv preprint
arXiv:2411.07140, 2024.
D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring
massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.
Y. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, et al. C-Eval: A
multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint
arXiv:2305.08322, 2023.
N. Jain, K. Han, A. Gu, W. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica.
Livecodebench: Holistic and contamination free evaluatio"
What is the focus of the paper by A. Kumar et al. mentioned in the text?,The paper by A. Kumar et al. focuses on training language models to self-correct via reinforcement learning.,"n of large language models for code.
CoRR, abs/2403.07974, 2024. URL https://doi.org/10.48550/arXiv.2403.07974 .
17S. Krishna, K. Krishna, A. Mohananey, S. Schwarcz, A. Stambler, S. Upadhyay, and M. Faruqui.
Fact, fetch, and reason: A unified evaluation of retrieval-augmented generation. CoRR ,
abs/2409.12941, 2024. doi: 10.48550/ARXIV.2409.12941. URL https://doi.org/10.485
50/arXiv.2409.12941 .
A. Kumar, V . Zhuang, R. Agarwal, Y. Su, J. D. Co-Reyes, A. Singh, K. Baumli, S. Iqbal, C. Bishop,
R. Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv
preprint arXiv:2409.12917, 2024.
H. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin. CMMLU: Measur-
ing massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212 ,
2023.
T. Li, W.-L. Chiang, E. Frick, L. Dunlap, T. Wu, B. Zhu, J. E. Gonzalez, and I. Stoica. From
crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv
preprint arXiv:2406.11939,"
What is the title of the paper by H. Lightman and colleagues published in 2023?,"The title of the paper by H. Lightman and colleagues published in 2023 is ""Let’s verify step by step."""," 2024.
H. Lightman, V . Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman,
I. Sutskever, and K. Cobbe. Let’s verify step by step. arXiv preprint arXiv:2305.20050, 2023.
B. Y. Lin. ZeroEval: A Unified Framework for Evaluating Language Models, July 2024. URL
https://github.com/WildEval/ZeroEval .
MAA. American invitational mathematics examination - aime. In American Invitational
Mathematics Examination -AIME 2024 , February 2024. URL https://maa.org/math
-competitions/american-invitational-mathematics-examination-aime .
OpenAI. Hello GPT-4o, 2024a. URL https://openai.com/index/hello-gpt-4o/ .
OpenAI. Learning to reason with llms, 2024b. URL https://openai.com/index/learnin
g-to-reason-with-llms/ .
OpenAI. Introducing SimpleQA, 2024c. URL https://openai.com/index/introducing
-simpleqa/ .
OpenAI. Introducing SWE-bench verified we’re releasing a human-validated subset of swe-
bench that more, 2024d. URL https://openai.com/index/introducing-swe-bench
-verified/ .
Qwen. Qwq: Reflect deeply on t"
"What is the focus of the paper titled ""Deepseekmath"" by Z. Shao et al.?","The paper ""Deepseekmath"" by Z. Shao et al. focuses on pushing the limits of mathematical reasoning in open language models.","he boundaries of the unknown, 2024a. URL https://qwenlm
.github.io/blog/qwq-32b-preview/ .
Qwen. Qwen2.5: A party of foundation models, 2024b. URL https://qwenlm.github.io/b
log/qwen2.5 .
D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman.
GPQA: A graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022 , 2023.
Z. Shao, P . Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. Li, Y. Wu, and D. Guo. Deepseekmath:
Pushing the limits of mathematical reasoning in open language models. arXiv preprint
arXiv:2402.03300, 2024.
D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre,
D. Kumaran, T. Graepel, T. P . Lillicrap, K. Simonyan, and D. Hassabis. Mastering chess and
shogi by self-play with a general reinforcement learning algorithm. CoRR , abs/1712.01815,
2017a. URL http://arxiv.org/abs/1712.01815 .
18D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker,
M. Lai, A. Bolton, Y. "
"What is the title of the paper authored by Chen, T. P., Lillicrap, F., Hui, L., Sifre, G., van den Driessche, T., Graepel, and D. Hassabis published in 2017?","The title of the paper is ""Mastering the game of go without human knowledge.""","Chen, T. P . Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and
D. Hassabis. Mastering the game of go without human knowledge. Nat. , 550(7676):354–359,
2017b. doi: 10.1038/NATURE24270. URL https://doi.org/10.1038/nature24270 .
C. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm test-time compute optimally can be more
effective than scaling model parameters, 2024. URL https://arxiv.org/abs/2408.033
14.
T. Trinh, Y. Wu, Q. Le, H. He, and T. Luong. Solving olympiad geometry without human
demonstrations. Nature, 2024. doi: 10.1038/s41586-023-06747-5.
J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and
I. Higgins. Solving math word problems with process-and outcome-based feedback. arXiv
preprint arXiv:2211.14275, 2022.
P . Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: A label-
free step-by-step verifier for llms in mathematical reasoning. arXiv preprint arXiv:2312.08935 ,
2023.
X. Wang, J. Wei, D. Schuurmans, Q. Le, E. C"
"What is the focus of the paper by S. Narang, A. Chowdhery, and D. Zhou mentioned in the text?","The paper by S. Narang, A. Chowdhery, and D. Zhou focuses on how self-consistency improves chain of thought reasoning in language models.","hi, S. Narang, A. Chowdhery, and D. Zhou.
Self-consistency improves chain of thought reasoning in language models. arXiv preprint
arXiv:2203.11171, 2022.
Y. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang, T. Li,
M. Ku, K. Wang, A. Zhuang, R. Fan, X. Yue, and W. Chen. Mmlu-pro: A more robust and
challenging multi-task language understanding benchmark. CoRR , abs/2406.01574, 2024.
URL https://doi.org/10.48550/arXiv.2406.01574 .
C. S. Xia, Y. Deng, S. Dunn, and L. Zhang. Agentless: Demystifying llm-based software
engineering agents. arXiv preprint, 2024.
H. Xin, Z. Z. Ren, J. Song, Z. Shao, W. Zhao, H. Wang, B. Liu, L. Zhang, X. Lu, Q. Du, W. Gao,
Q. Zhu, D. Yang, Z. Gou, Z. F. Wu, F. Luo, and C. Ruan. Deepseek-prover-v1.5: Harnessing
proof assistant feedback for reinforcement learning and monte-carlo tree search, 2024. URL
https://arxiv.org/abs/2408.08152 .
J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following
evaluation for lar"
Who are the core contributors mentioned in the document for the ge language models project?,"The core contributors mentioned are Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z.F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, and Ziyi Gao.","ge language models. arXiv preprint arXiv:2311.07911, 2023.
19Appendix
A. Contributions and Acknowledgments
Core Contributors
Daya Guo
Dejian Yang
Haowei Zhang
Junxiao Song
Ruoyu Zhang
Runxin Xu
Qihao Zhu
Shirong Ma
Peiyi Wang
Xiao Bi
Xiaokang Zhang
Xingkai Yu
Yu Wu
Z.F. Wu
Zhibin Gou
Zhihong Shao
Zhuoshu Li
Ziyi Gao
Contributors
Aixin Liu
Bing Xue
Bingxuan Wang
Bochao Wu
Bei Feng
Chengda Lu
Chenggang Zhao
Chengqi Deng
Chong Ruan
Damai Dai
Deli Chen
Dongjie Ji
Erhang Li
Fangyun Lin
Fucong Dai
Fuli Luo*
Guangbo Hao
Guanting Chen
Guowei Li
H. Zhang
Hanwei Xu
Honghui Ding
Huazuo Gao
Hui QuHui Li
Jianzhong Guo
Jiashi Li
Jingchang Chen
Jingyang Yuan
Jinhao Tu
Junjie Qiu
Junlong Li
J.L. Cai
Jiaqi Ni
Jian Liang
Jin Chen
Kai Dong
Kai Hu*
Kaichao You
Kaige Gao
Kang Guan
Kexin Huang
Kuai Yu
Lean Wang
Lecong Zhang
Liang Zhao
Litong Wang
Liyue Zhang
Lei Xu
Leyi Xia
Mingchuan Zhang
Minghua Zhang
Minghui Tang
Mingxu Zhou
Meng Li
Miaojun Wang
Mingming Li
Ning Tian
Panpan Huang
Peng Zhang
Qiancheng Wang
Qinyu Chen
Qiushi Du
R"
What is the common theme or pattern in the list of names provided in the text?,"The list consists of names, primarily of Chinese origin, which are likely authors or contributors to a technical document or publication. The names are presented in a format that includes both full names and initials, suggesting a formal listing typical in academic or professional contexts.","uiqi Ge*
Ruisong Zhang
Ruizhe Pan
Runji Wang
R.J. Chen
R.L. Jin
20Ruyi Chen
Shanghao Lu
Shangyan Zhou
Shanhuang Chen
Shengfeng Ye
Shiyu Wang
Shuiping Yu
Shunfeng Zhou
Shuting Pan
S.S. Li
Shuang Zhou
Shaoqing Wu
Shengfeng Ye
Tao Yun
Tian Pei
Tianyu Sun
T. Wang
Wangding Zeng
Wen Liu
Wenfeng Liang
Wenjun Gao
Wenqin Yu*
Wentao Zhang
W.L. Xiao
Wei An
Xiaodong Liu
Xiaohan Wang
Xiaokang Chen
Xiaotao Nie
Xin Cheng
Xin Liu
Xin Xie
Xingchao Liu
Xinyu Yang
Xinyuan Li
Xuecheng Su
Xuheng Lin
X.Q. Li
Xiangyue Jin
Xiaojin Shen
Xiaosha Chen
Xiaowen Sun
Xiaoxiang Wang
Xinnan Song
Xinyi Zhou
Xianzu Wang
Xinxia Shan
Y.K. Li
Y.Q. WangY.X. Wei
Yang Zhang
Yanhong Xu
Yao Li
Yao Zhao
Yaofeng Sun
Yaohui Wang
Yi Yu
Yichao Zhang
Yifan Shi
Yiliang Xiong
Ying He
Yishi Piao
Yisong Wang
Yixuan Tan
Yiyang Ma*
Yiyuan Liu
Yongqiang Guo
Yuan Ou
Yuduan Wang
Yue Gong
Yuheng Zou
Yujia He
Yunfan Xiong
Yuxiang Luo
Yuxiang You
Yuxuan Liu
Yuyang Zhou
Y.X. Zhu
Yanping Huang
Yaohui Li
Yi Zheng
Yuchen Zhu
Yunxian Ma
Ying Tang
Yukun Zha
Yuting Yan
Z.Z. R"
How are the authors listed within each role in the document?,The authors are listed alphabetically by their first name within each role.,"en
Zehui Ren
Zhangli Sha
Zhe Fu
Zhean Xu
Zhenda Xie
Zhengyan Zhang
Zhewen Hao
Zhicheng Ma
Zhigang Yan
Zhiyu Wu
Zihui Gu
21Zijia Zhu
Zijun Liu*
Zilin Li
Ziwei Xie
Ziyang Song
Zizheng PanZhen Huang
Zhipeng Xu
Zhongyu Zhang
Zhen Zhang
Within each role, authors are listed alphabetically by the first name. Names marked with *
denote individuals who have departed from our team.
22"
What is the main advantage of the DualPipe algorithm in pipeline parallelism?,"The main advantage of the DualPipe algorithm is its ability to achieve full overlap of forward and backward computation-communication phases, which also reduces pipeline bubbles.","<h1>DualPipe</h1>
<p>DualPipe is an innovative bidirectional pipeline parallelism algorithm introduced in the DeepSeek-V3 Technical Report. It achieves full overlap of forward and backward computation-communication phases, also reducing pipeline bubbles. For detailed information on computation-communication overlap, please refer to the profile data.</p>
<p>Pipeline Bubbles and Memory Usage Comparison</p>
<p>| Method    | Bubble                  | Parameter | Activation |
|:---------:|:-----------------------:|:---------:|:----------:|
| 1F1B      | (PP-1)(𝐹+𝐵)            | 1×        | PP         |
| ZB1P      | (PP-1)(𝐹+𝐵-2𝑊)         | 1×        | PP         |
| DualPipe  | (PP/2-1)(𝐹&amp;𝐵+𝐵-3𝑊)     | 2×        | PP+1       |</p>
<p>𝐹 denotes the execution time of a forward chunk, 𝐵 denotes the execution time of a full backward chunk, 𝑊 denotes the execution time of a ""backward for weights"" chunk, and 𝐹&amp;𝐵 denotes the execution time of two mutually overlapped forward and backward chunks.</p>
<h3>About</h3"
"What tool was used to capture the profiling data in the DeepSeek Infra framework, and how can it be visualized?",The profiling data in the DeepSeek Infra framework was captured using the PyTorch Profiler. It can be visualized by navigating to chrome://tracing in the Chrome browser or edge://tracing in the Edge browser.,">
<p>A bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training</p>
<p><code>DualPipe was created and developed by Jiashi Li and Chengqi Deng and Wenfeng Liang.</code></p>
<h1>Profiling Data in DeepSeek Infra</h1>
<p>Here, we publicly share profiling data from our training and inference framework to help the community better understand the communication-computation overlap strategies and low-level implementation details. The profiling data was captured using the PyTorch Profiler. After downloading, you can visualize it directly by navigating to chrome://tracing in the Chrome browser (or edge://tracing in the Edge browser). Notice that we simulate an absolutely balanced MoE routing strategy for profiling.</p>
<h2>Training</h2>
<p>The training profile data demonstrates our overlapping strategy for a pair of individual forward and backward chunks in DualPipe. Each chunk contains 4 MoE (Mixture of Experts) layers. The parallel configuration aligns with DeepSeek-V3 pretr"
What is the batch size used for decoding in the profile described?,The batch size used for decoding in the profile is 128 requests per GPU.,"aining settings: EP64, TP1 with 4K sequence length. And the PP communication is not included during profilng for simplicity.</p>
<h2>Inference</h2>
<h3>Prefilling</h3>
<p>For prefilling, the profile employs EP32 and TP1 (in line with DeepSeek V3/R1 ’s actual online deployment), with a prompt length set to 4K and a batch size of 16K tokens per GPU. In our prefilling stage, we utilize two micro-batches to overlap computation and all-to-all communication, while ensuring that the attention computation load is balanced across the two micro-batches — meaning that the same prompt may be split between them.</p>
<h3>Decoding</h3>
<p>For decoding, the profile employs EP128, TP1, and a prompt length of 4K (closely matching the actual online deployment configuration), with a batch size of 128 requests per GPU. Similar to prefilling, decoding also leverages two micro-batches for overlapping computation and all-to-all communication. However, unlike in prefilling, the all-to-all communication during decoding does not occupy"
What strategy is used in Expert Parallelism Load Balancer (EPLB) to ensure load balancing across different GPUs?,"The Expert Parallelism Load Balancer (EPLB) uses a redundant experts strategy that duplicates heavy-loaded experts and heuristically packs them to GPUs to ensure load balancing across different GPUs. Additionally, it attempts to place experts of the same group on the same node to reduce inter-node data traffic."," GPU SMs: after RDMA messages are issued, all GPU SMs are freed, and the system waits for the all-to-all communication to complete after the computation has finished. For more information about the all-to-all implementation, please refer to DeepEP.</p>
<h1>Expert Parallelism Load Balancer (EPLB)</h1>
<p>When using expert parallelism (EP), different experts are assigned to different GPUs. Because the load of different experts may vary depending on the current workload, it is important to keep the load of different GPUs balanced. As described in the DeepSeek-V3 paper, we adopt a redundant experts strategy that duplicates heavy-loaded experts. Then, we heuristically pack the duplicated experts to GPUs to ensure load balancing across different GPUs. Moreover, thanks to the group-limited expert routing used in DeepSeek-V3, we also attempt to place the experts of the same group to the same node to reduce inter-node data traffic, whenever possible.</p>
<p>To facilitate reproduction and deployment, we open-source our"
What is the purpose of the hierarchical load balancing policy in the EP load balancing algorithm?,"The hierarchical load balancing policy is used when the number of server nodes divides the number of expert groups. It ensures balanced loads by evenly packing expert groups to nodes, replicating experts within each node, and then packing the replicated experts to individual GPUs to achieve load balance across different GPUs. This policy can be applied during the prefilling stage with a smaller expert-parallel size."," deployed EP load balancing algorithm in eplb.py. The algorithm computes a balanced expert replication and placement plan based on the estimated expert loads. Note that the exact method to predict the loads of experts is out of this repo's scope. A common method is to use moving average of historical statistics.</p>
<h2>The Algorithm</h2>
<p>The load balancing algorithm comes with two policies used for different cases.</p>
<h2>Hierarchical Load Balancing</h2>
<p>When the number of server nodes divides the number of expert groups, we use the hierarchical load balancing policy to harness the group-limited expert routing. We first pack the expert groups to nodes evenly, ensuring the loads of different nodes are balanced. Then, we replicate the experts within each node. Finally, we pack the replicated experts to individual GPUs to ensure different GPUs are load-balanced. The hierarchical load balancing policy can be used in prefilling stage with a smaller expert-parallel size.</p>
<h3>Global Load Balancing</h3>
<"
"What is the Fire-Flyer File System (3FS) designed for, and what technologies does it leverage?","The Fire-Flyer File System (3FS) is designed for high-performance distributed file system needs, specifically addressing the challenges of AI training and inference workloads. It leverages modern SSDs and RDMA networks to provide a shared storage layer that simplifies the development of distributed applications.","p>In other cases, we use the global load balancing policy that replicates the experts globally regardless of expert groups, and pack the replicated experts to individual GPUs. This policy can be adopted in decoding stage with a larger expert-parallel size.</p>
<h1>Fire-Flyer File system</h1>
<p>The Fire-Flyer File System (3FS) is a high-performance distributed file system designed to address the challenges of AI training and inference workloads. It leverages modern SSDs and RDMA networks to provide a shared storage layer that simplifies development of distributed applications. Key features and benefits of 3FS include:</p>
<ul>
<li>
<p>Performance and Usability</p>
<ul>
<li>Disaggregated Architecture Combines the throughput of thousands of SSDs and the network bandwidth of hundreds of storage nodes, enabling applications to access storage resource in a locality-oblivious manner.</li>
<li>Strong Consistency Implements Chain Replication with Apportioned Queries (CRAQ) for strong consistency, making application c"
What is the purpose of the KVCache for Inference as mentioned in the text?,"The KVCache for Inference provides a cost-effective alternative to DRAM-based caching, offering high throughput and significantly larger capacity.","ode simple and easy to reason about.</li>
<li>File Interfaces Develops stateless metadata services backed by a transactional key-value store (e.g., FoundationDB). The file interface is well known and used everywhere. There is no need to learn a new storage API.</li>
</ul>
</li>
<li>
<p>Diverse Workloads</p>
<ul>
<li>Data Preparation Organizes outputs of data analytics pipelines into hierarchical directory structures and manages a large volume of intermediate outputs efficiently.</li>
<li>Dataloaders Eliminates the need for prefetching or shuffling datasets by enabling random access to training samples across compute nodes.</li>
<li>Checkpointing Supports high-throughput parallel checkpointing for large-scale training.</li>
<li>KVCache for Inference Provides a cost-effective alternative to DRAM-based caching, offering high throughput and significantly larger capacity.</li>
</ul>
</li>
</ul>
<h2>Performance</h2>
<ol>
<li>Peak throughput</li>
</ol>
<p>The following figure demonstrates the throughput of read stre"
What was the aggregate read throughput achieved during the read stress test on the 3FS cluster?,The aggregate read throughput achieved during the read stress test on the 3FS cluster was approximately 6.6 TiB/s.,"ss test on a large 3FS cluster. This cluster consists of 180 storage nodes, each equipped with 2×200Gbps InfiniBand NICs and sixteen 14TiB NVMe SSDs. Approximately 500+ client nodes were used for the read stress test, with each client node configured with 1x200Gbps InfiniBand NIC. The final aggregate read throughput reached approximately 6.6 TiB/s with background traffic from training jobs.</p>
<ol>
<li>GraySort</li>
</ol>
<p>We evaluated smallpond using the GraySort benchmark, which measures sort performance on large-scale datasets. Our implementation adopts a two-phase approach: (1) partitioning data via shuffle using the prefix bits of keys, and (2) in-partition sorting. Both phases read/write data from/to 3FS.</p>
<p>The test cluster comprised 25 storage nodes (2 NUMA domains/node, 1 storage service/NUMA, 2×400Gbps NICs/node) and 50 compute nodes (2 NUMA domains, 192 physical cores, 2.2 TiB RAM, and 1×200 Gbps NIC/node). Sorting 110.5 TiB of data across 8,192 partitions completed in 30 minutes and 14 seco"
What is the purpose of KVCache in the LLM inference process?,KVCache is used to optimize the LLM inference process by avoiding redundant computations through caching the key and value vectors of previous tokens in the decoder layers.,"nds, achieving an average throughput of 3.66 TiB/min.</p>
<ol>
<li>KVCache</li>
</ol>
<p>KVCache is a technique used to optimize the LLM inference process. It avoids redundant computations by caching the key and value vectors of previous tokens in the decoder layers. The top figure demonstrates the read throughput of all KVCache clients, highlighting both peak and average values, with peak throughput reaching up to 40 GiB/s. The bottom figure presents the IOPS of removing ops from garbage collection (GC) during the same time period.</p>"
What is the significance of Deepseek's approach in the AI industry?,"The significance of Deepseek's approach in the AI industry lies in its focus on improving algorithms rather than pushing for better hardware, which has allowed them to achieve better or equal performance to competitors at a fraction of the training and inference cost.","<p><source name=""https://medium.com/@visithkumarapperuma/deepseek-v3-a-game-changer-in-a-i-heres-why-it-matters-75591957ca07""></p>
<p>author - Visith Kumarapperuma</p>
<h1>Deepseek V3: A Game-Changer in A.I. Here’s Why It Matters</h1>
<p>Currently, the AI models from the Chinese startup Deepseek are causing quite a stir in the AI space. Their latest reasoning model, Deepseek r1, shows better or equal performance to competitors. But above all, they achieved it with a fraction of the training and inference cost.
DeepSeek’s AI Assistant overtook ChatGPT to become the most downloaded free app on the U.S. App Store. This development has led to market concerns about A.I. investments to major U.S. tech companies. Impacting share prices of tech firms including Nvidia.</p>
<h2>So what made Deepseek such a big impact to A.I. ?</h2>
<p>The significance of Deepseek as a disruptor in the industry lies in its approach. Unlike other companies that pushed for better hardware, Deepseek improved the algorithms. Thus achieving "
What strategies did Deepseek use to make the training of the Deepseek V3 model more efficient?,"Deepseek made the training of the Deepseek V3 model more efficient by using 8-bit instead of 32-bit to save memory, compressing key value indices to achieve 93% compression ratios, performing multi-token prediction instead of single-token prediction to double inference speeds, and using the MOE model to decompose a big model into smaller models that can run on consumer-grade hardware.","better results at a software level.
Note that the following details are for the Deepseek V3 model.
• Deepseek said it trained a model using a data centre of some 2,000 of Nvidia H800 GPUs.
• Time duration 2 months with the cost of the *final training run being ~$5.5 million
This ~$5.5M reflects the “rental” cost for the GPU hours needed to train DeepSeek‑V3. It does not include:
1. The capital expenditure for owning the hardware.
2. Costs associated with prior research, ablation studies, or experiments on alternative architectures/algorithms/data.</p>
<h3>Deepseek made training more efficient (45 times more efficient)</h3>
<ul>
<li>Use 8-bit instead of 32-bit to save memory.</li>
<li>Compress key value indices which eat up a lot of VRAM; they got 93% compression ratios.</li>
<li>Do multi-token prediction instead of single-token prediction -&gt; doubled inference speeds</li>
<li>The MOE model decomposes a big model into small models that can run on consumer-grade hardware.</li>
</ul>
<h2>Summary of how Deepsee"
What strategies were used in k v3 to improve training efficiency for the frontier model?,"The k v3 model used a Mixture-of-Experts (MoE) architecture with sparse activation, Multi-head Latent Attention (MLA) to compress the Key-Value cache, FP8 mixed precision training to reduce memory usage and accelerate training, and an auxiliary loss-free strategy for load balancing in the MoE architecture to improve performance.","k v3 was so efficient at training the frontier model</h2>
<ol>
<li>Model Architecture
The model employs a Mixture-of-Experts (MoE) architecture, where only 37B parameters fire for each token out of the total 671B. This sparse activation significantly reduces compute requirements compared to dense models.
The model uses Multi-head Latent Attention (MLA). This compresses the Key-Value cache, reducing memory usage and enabling more efficient training.</li>
<li>FP8 Mixed Precision Training:
They implemented an FP8 mixed precision training framework. Which reduces memory usage and accelerates training compared to higher precision formats.
Reduced memory footprint by up to 50% compared to traditional FP16/FP32 formats.
They use fine-grained quantisation strategies and increased accumulation precision to maintain accuracy.</li>
<li>Load Balancing Strategy
They pioneered an auxiliary loss-free strategy for load balancing in the MoE architecture. This improved performance without the drawbacks of traditional auxiliary"
What are some of the key optimizations in the HAI-LLM training framework for the Deepseek v3 model?,"The HAI-LLM training framework includes several key optimizations such as the DualPipe algorithm for efficient pipeline parallelism, efficient cross-node all-to-all communication kernels to fully utilize network bandwidth, and careful memory optimizations to avoid using costly tensor parallelism."," loss methods.</li>
<li>Training Framework
They developed a custom training framework called HAI-LLM with several optimisations:
DualPipe algorithm for efficient pipeline parallelism. This reduces pipeline bubbles and overlapping computation and communication.
Efficient cross-node all-to-all communication kernels to fully utilise network bandwidth.
Careful memory optimisations to avoid using costly tensor parallelism.</li>
</ol>
<h2>Breakdown of the costs of the Deepseek v3 model</h2>
<p>Deepseek’s flagship model v3 showcases an architecture with a 671B parameter MOE (Mixture of Agents) with 37B active parameters per token
- Their success stems from breakthrough engineering: using MoE architecture, implementing FP8 mixed precision training, and developing a custom HAI-LLM framework.
- Deepseek excels at reasoning and math, surpassing GPT-4 and Claude 3.5 Sonnet.
- For writing and coding tasks, Claude 3.5 Sonnet maintains a slight lead.
- Deepseek pre-trained this model on 14.8 trillion high-quality data, taki"
"How many GPU hours were used to train the Llama 403b model, and what was the approximate cost?","The Llama 403b model was trained using 30,840,000 GPU hours, and the approximate cost was around $6 million.","ng 2,788,000 GPU hours on the Nvidia h800s cluster, costing around only $6 million
- the Llama 403b was trained on 11x of that, taking 30,840,000 GPU hours, also on 15 trillion tokens.
<code>So how true is the claim of $5.5 million, or is it another marketing trick?</code></p>
<ol>
<li>Underlying FLOP calculations
Model Details:</li>
<li>Active Parameters: 37B (using FP8 precision)</li>
<li>FLOPs per token: Using the rule of thumb “6 FLOPs per parameter per token.”
<code>37B×6 = 222B FLOPs per token</code></li>
<li>Total Training Tokens: Approximately 14.8 trillion tokens</li>
<li>Total FLOPs required:
<code>222 B FLOPs/token×14.8 T tokens ≈ 3.3×10²⁴ FLOPs</code></li>
</ol>
<h3>GPU FLOP Capacity (H800/H100):</h3>
<p>An H100 is roughly estimated to deliver about.
3.958×10¹⁵ FLOPs (per second or per some standardised interval — here used as a comparative metric).
Ideal (Perfect Efficiency) GPU hours.
(Dividing total required FLOPs by per‑GPU capability gives)
<code>3.3×10²⁴ / 3.958×10¹⁵ ​≈ 8.33×10⁸ seconds⇒≈0.4"
How many GPU hours were required for the pre-training stage of DeepSeek-V3?,"The pre-training stage of DeepSeek-V3 required a total of 2,664K GPU hours."," million GPU hour</code>
Note: This “perfect efficiency” scenario is a lower bound. Real-world training is less efficient.
2. Adjusting for Real‑World Inefficiencies (Comparison with Llama 3.1)
Reference Model: Llama 3.1 (405B parameters, 15 T tokens) reportedly required 30.84 M GPU hours in practice.
Recalculating FLOPs for Llama 3.1:
<code>Using the same math: 3.64×10²⁵ FLOPs required</code>
Scaling Efficiency
Using the ratio of FLOPs needed for DeepSeek‑V3 versus Llama 3.1. and assuming similar inefficiencies.
The estimate adjusts to roughly 2.79M GPU hours for DeepSeek‑V3 training.
3. DeepSeek‑V3 Reported Training Breakdown
According to the DeepSeek‑V3 paper
Pre‑training Stage:
- Per Trillion Tokens: 180K H800 GPU hours
- Overall Pre‑training: Total of 2,664K GPU hours
- This stage was completed in less than two months using a cluster of 2,048 H800 GPUs.
Context Length Extension:
- Additional 119K GPU hours
Post‑training:
- An extra 5K GPU hours
Total GPU Hours:
<code>2,664 K+119 K+5 K≈2.788M GPU hours</c"
"How many GPU hours were used for the pre-training stage of DeepSeek-V3, and what was the cost associated with it?","The pre-training stage of DeepSeek-V3 used 2,664K GPU hours. Assuming a rental price of $2 per GPU hour, the cost associated with the pre-training stage would be approximately $5.328 million.","ode>
4. Cost Estimation
Assumed GPU Rental Price: $2 per GPU hour
Total Rental Cost:
<code>2.788M GPU hours×$2/hour≈$5.576 million</code>
as stated in Deepseek paper
During the pre‑training stage, training DeepSeek‑V3 on each trillion tokens requires only 180K H800 GPU hours… Consequently, our pre‑training stage is completed in less than two months and costs 2664K GPU hours. Combined with 119K GPU hours for the context length extension and 5K GPU hours for post‑training, DeepSeek‑V3 costs only 2.788M GPU hours for its full training. Assuming the rental price of the H800 GPU is $2 per GPU hour, our total training costs amount to only $5.576M.
5. Summary
Theoretical (Perfect Efficiency) Estimate: ~0.4 M GPU hours (using idealised FLOP counts and assuming perfect hardware utilisation0
Adjusted (Real‑World) Estimate (via Llama 3.1 comparison): ~2.79 GPU hours
DeepSeek‑V3 Reported Breakdown:
Pre‑training: 2,664K GPU hours
Context Extension: 119K GPU hours
Post‑training: 5K GPU hours
Total: ~2.788 M GPU hours</p>
<"
What is the estimated cost at $2 per GPU hour?,The estimated cost at $2 per GPU hour is approximately $5.576 million.,h3>Cost (at $2 per GPU hour): ~$5.576 million</h3>
"What is the core principle of the DeepSeek-V3 model based on, and how does it relate to other models?",The core principle of the DeepSeek-V3 model resembles that of the standard Transformer and well-known large language models (LLMs). It is beneficial to have general knowledge of previously released large language models like LLaMA to understand DeepSeek-V3.,"<p><source name=""https://medium.com/@jjjy213/deepseek-v3-explained-fdac83ba280c""/>
author - Ataka jeong</p>
<ol>
<li>Introduction
How could the DeepSeek-V3 model achieve incredible performance and economical training as an open source model? In this paper review, we will explore the various features that were invented and applied to build the DeepSeek-V3 model. The way the paper presents the model may seem complicated to people who are unfamiliar with the new conceptualization invented by DeepSeek. However, its core principle still resembles that of the standard Transformer and well-known LLMs. It will be incredibly helpful to have general knowledge of previously released large language models like LLaMA. I will also add my own interpretation of the DeekSeek model in this story.
Let’s dive into the new features of model architecture step by step.</li>
<li>Model Architecture
First of all, we will investigate the core architecture of DeekSeek-V3 model. The DeekSeek-V3 model has inherited most parts of model fro"
What is Multi-Head Latent Attention (MLA) and how does it improve the model's performance?,"Multi-Head Latent Attention (MLA) is an enhanced version of the conventional attention module used in transformer models. It improves the model's performance by compressing the input vector, which enhances speed and memory usage in the attention block. This compression is achieved by reducing the data's dimensionality while preserving the information it contains, similar to techniques like Principal Component Analysis (PCA).","m previous V2 model. These parts of model were elaborated more in V2 paper, but it is worth noting the principle how V3 model was built upon. While they used the structure of the ordinary transformer block like Llama, its attention and Feed-Forward Network were more sophisticated to boost the model performance. The overview of the Transformer block is as shown in the following diagram.
The two main components are Multi-Head Latent Attention(MLA) and DeepSeekMoE.</li>
<li>2.1 Multi-Head Latent Attention(MLA)
What is Multi-Head Latent Attention(MLA)? You might noticed that “Latent” is only additional word to conventional attention module. MLA improved the speed and memory usage in the attention block by compressing the input vector. From a data analysis perspective, the data can be compressed into a lower dimension while preserving the information it contains. One of the well-known techniques is Principal component analysis (PCA), which reduces the dimension of the data and maintains variance to retain its info"
What is the purpose of using a compressed vector in the DeepSeek model?,The purpose of using a compressed vector in the DeepSeek model is to improve both speed by reducing data copying and memory efficiency by using a smaller compressed vector.,"rmation. In latent diffusion model, the input data is compressed by variational autoencoder and reconstructed in initial dimension. The Multi-Head Latent Attention(MLA) applied this principle to compress and decompress the input data. By storing a compressed vector for the KV cache, the DeepSeek model can improve both speed by reducing data copying and memory efficiency by using a smaller compressed vector. The weight matrix for compression is additionally required, because human cannot compress it manually, but AI should learn it how the compression should be done. Applying RoPE to the compressed vector is not mathematically compatible, which was shown in detail in V2 paper, they used decoupled RoPE. As illustrated in the figure above, RoPE is applied to query and key, but also the query and key without RoPE. The RoPE-applied query and key are then concatenated with their respective non-RoPE counterparts. Finally, the query and key are obtained as normal transformer block, then the computation of dot-product"
What is DeekSeekMoE and how does it function in the context of AI?,"DeekSeekMoE is a model where the Feed-Forward Network is divided into multiple experts instead of being one large network. Each expert specializes in a certain domain, handling specific groups of tokens they are familiar with, rather than processing the entire range of tokens. This specialization allows for improved performance, as certain experts are activated based on the input sequence, contributing to the output. Shared experts, on the other hand, are generalists and are activated for all kinds of tokens."," attention will not be different from original one. But we could reach this point with a more economical KV cache thanks to the lower dimension of data.</li>
<li>2.2 DeekSeekMoE
Secondly, you can note that Feed-Forward Network is unusual as it was split into a lot of experts, rather than one large FFN. They called it as DeekSeekMoE. Like humans in a group, the AI also needs to specialized in certain domain to improve the performance. Thus, the mixture of experts come into play here. Each expert can specialize in certain domain, in this case, the group of tokens that they are familiar with, instead of coping with entire range of tokens alone. Dependent on the input sequence(tokens), the certain experts are selected to be activated and they contribute to make output. Shared experts are generalist and are activated for all kind of tokens. Then it might be interesting to know by what algorithm we can select the experts? We need to assign a vector to each expert which determines the range of tokens(domain) that ex"
What is the role of the centroid vector eᵢ in the expert selection process?,"The centroid vector eᵢ is learned during training and represents the type of input tokens the expert is specialized in. It encodes the knowledge domain of the expert, and the dot product uₜᵀ eᵢ quantifies the similarity between the input vector uₜ and the expert's domain, determining the score for selecting the expert.","perts can deal with well. And we give score to each expert to check how similar the domain of expert and input token are. If the score is high, then we should select the expert and let them activated to make output. Well, it sounds quite simple. Let’s see the math behind it.
eᵢ is a centroid vector. It is learned during training and represents the type of input tokens the expert specialized in. Each expert’s centroid vector encodes the knowledge domain it specializes in.
uₜ is input vector to FFN. The dot product uₜᵀ eᵢ quantifies the similarity between the input vector uₜ​ and the centroid (or domain) of expert eᵢ, effectively measuring the alignment of the input data with the expert’s specialized domain. So, the sᵢ = Sigmoid(uₜᵀ eᵢ) represents the score for each i-th expert, determining whether the expert should be selected. By gating value gᵢ, which select Kᵣ experts with high score by Topk algorithm. We add all outputs of selected experts and shared experts, then we arrive to the final output.
2.3 Multi-T"
How does DeepSeek improve the conventional method of Multi-Token Prediction (MTP) in transformers?,"DeepSeek improves the conventional method of Multi-Token Prediction (MTP) by implementing sequential MTP instead of the previous parallel MTP. It constructs independent MTP modules where the previous output of the Transformer block is concatenated into the subsequent MTP module, allowing the entire system of MTP modules to collectively enable multi-token prediction.","oken Prediction
In a standard transformer, the model generates a token each time, and this new token is fed back into decoder as input. Since this way restricts the efficiency and the speed of convergence during training, many researchers have made effort to come up with a method to generate multiple tokens each time. DeepSeek improved the conventional way of Multi-Token Prediction(MTP). Instead of previous parallel MTP, DeepSeek decided sequential MTP. They construct independent MTP modules, where the previous output of the Transformer block is concatenated into the subsequent MTP module, as illustrated in the following figure.
As shown in the figure, the structure of MTP modules is akin to RNN model. But, unlike RNN, which preserve hidden states of nodes, the MTP modules send output of prediction to the subsequent module. Even though a single Transformer block cannot generate multiple tokens, the entire system of MTP modules collectively enables multi-token prediction. As it compares additional tokens per p"
Why did DeepSeek generate only one additional token during training despite the potential benefits of generating more?,"DeepSeek generated only one additional token during training due to the computational cost associated with using many MTP modules, necessitating a compromise between the benefits of MTP and computational cost.","rediction, it provides more information for weight updates during training, leading to more efficient learning and faster convergence. The model can proactively learn and prepare for the additional tokens.
In actual training, DeepSeek opted to generate only one additional token, presumably due to the computational cost which is caused by using many MTP modules. It necessitate the compromise between the benefits of MTP and computational cost. During inference, the MTP modules are discarded, generating only one token per prediction.</li>
<li>Infrastructure
3.1 DualPipe
Since the U.S. did not export great GPUs like the NVIDIA H100 to China, DeepSeek researchers had to devise innovative methods to accelerate model training using the weaker H800 GPUs. Since they succeeded, NVIDIA’s stock price briefly plunged, as people believed that high-performance GPUs would no longer be necessary for training LLM. Because the DeepSeek model was trained on 2048 H800 GPUs, communication between GPUs accounts for large portion of"
"What is a ""bubble"" in the context of GPU networking during model training?","A ""bubble"" refers to the waiting time that occurs when GPUs have to wait for new data to be copied from other GPUs during simultaneous use, causing training inefficiencies."," training time. Therefore, enhanching networking between GPUs has to play crucial role to reduce training time. When we use many GPUs simultaneously, the GPUs have to wait for a certain amount of time until new data is copied from other GPU. This waiting time, which causes training inefficiencies, is known as a “bubble,” and we should minimize it as much as possible. DeepSeek invented a innovative method to reduce bubble.
During model training, data flows through the model in forward and backward processes. In forward process, data goes from the input layer to the output layer. On the other hand, during the backward process data moves from the output layer to the input later, updating weights based on the information to minimize the loss. Prior to DeepSeek, researchers found that the backward process can be split into two processes, which are backward for input and backward for weight, in order to remove more bubbles. The backward for input is computation of the gradient of the loss with respect to the input "
What is the purpose of combining the forward process and the backward for input in DeepSeek's DualPipe?,"The purpose of combining the forward process and the backward for input in DeepSeek's DualPipe is to reduce the number of communications between GPUs during training. This is achieved by initiating training data from two devices in opposite directions, allowing for more efficient use of resources and reducing idle time for devices.","data, whereas the backward for weight calculates gradient of the loss with respect to the weight. The backward for input must be completed ahead of the backward for weight, because it is necessary to compute the backward for weight. Mathematically, the chain rule is applied to the calculation of backpropagation, where the backward for input is used for the calculation of backward for weight.
In such process, it is certain that an enormous number of communications between GPUs is required. In order to reduce the number of communication, the DeepSeek’s DualPipe combines the forward process and the backward for input by initiating training data from two devices in the opposite directions as illustrated in following figure.</li>
</ol>
<p>The batch 0 is the initial data, which starts processing on the device 0 and continues on the subsequent devices. In a conventional training plan, the device 7 remains idle, waiting for the batch 0 to be copied onto it. However, DualPipe makes the device 7 start training with oth"
What is the purpose of mixed precision training in the context of the DeepSeek-V3 model?,"The purpose of mixed precision training in the DeepSeek-V3 model is to improve training and memory efficiency while maintaining model accuracy. This involves reducing precision in parts of the model where heavy computations are executed, such as matrix multiplication, while preserving high precision for lighter computations like matrix addition and data storage.","er batch data in the opposite direction. This allows us to combine them as a chunk and continuously copy them together on other devices to reduce communication between GPUs. With weaker H800 GPUs, they couldn’t improve the speed of the GPUs, but reduce the communication between GPUs to accelerate the training.</p>
<p>3.2 Mixed precision training
Mixed precision training is already prevalent technique of LLM to improve training and memory efficiency while maintaining the model accuracy. In mixed precision training, it is critical task to find out which parts of model are less significant for the model accuracy and reduce the precision that parts. In DeepSeek-V3 model, the researchers have found that they should reduce precision in the parts of model where heavy computations are executed, such as matrix multiplication. In contrast, they preserved high precision for matrix addition and storing data, which are relatively lightweight computation. The mixed precision training of DeepSeek is shown in the following f"
What method does DeepSeek implement to address overflow and underflow issues in quantization?,"DeepSeek implements Fine-Grained Quantization, where values are grouped, and each group has its own scaling factor. This approach allows each group of values to have a more suitable scaling factor, helping to avert overflow and underflow.","igure.</p>
<p>While reducing the precision by the method above, overflow and underflow arise as a impediment. If the numerical values are quantized in lower precision like FP8 format, the values are clipped to the certain representable range. While computation in lower precision, the values can easily exceed the range during the computation. Scaling the values can mitigate the overflow and underflow by adjusting the values and lead to more proper representation in limited range. But, static scaling, which applies fixed scaling factor to all values, can still cause overflow and underflow for many values. To cope with this issue, DeepSeek implemented Fine-Grained Quantization. In this method, the values are grouped, and each group has its own scaling factor. This approach allows the each group of values to have a more suitable scaling factor, by which the overflow and underflow can be averted.
Another issue of quantization is that the small errors can be accumulated and become more serious problem later. In ord"
What is the purpose of using high precision for intermediate values in the context described?,"The purpose of using high precision for intermediate values is to prevent the accumulation of errors when summing a lot of values with error. By storing some values in high precision, the errors of these values are not accumulated on a large scale, as the small group of values do not contribute to a large error.","er to avoid that a lot of values with error are summed and their errors are accumulated, intermediate values are copied in high precision, if the number of values reaches the interval. It means that some values are grouped, and their values are stored in high precision. Then, the errors of values aren’t accumulated on a large scale, because the small group of values don’t contribute to large error.
These two techniques to prevent quantization error are visualized in following figure.</p>
<ol>
<li>Reinforcement Learning
After supervised fine-tuning, DeepSeek additionally implemented reinforcement learning. A reward model has to be built and trained for reinforcement learning, which gives feedback to the model and determine the direction of learning. The rule-based reward model(RM) and model-based reward model(RM) were employed.
The rule-based RM is applied to the questions with specific rules, such as math problems and LeetCode problems. In these domains, the specific rules are used to verify the correctness o"
What innovative idea does the DeepSeek model incorporate that differs from conventional models?,"The DeepSeek model includes the chain-of-thought in the reward, whereas conventional models only included the final reward based on the answer.","f the answers and the questions about logical reasoning are involved. However, for many questions, the answer cannot be verified by a specific rule. In those cases where no rule is provided, the model-based RM determines, whether the answer matches the ground-truth answer. Another innovative idea of DeepSeek is including the chain-of-thought to the reward, whereas conventional models only included final reward based on the answer.
DeepSeek-V3 model, as V2 model did, adopted Group Relative POlicy Optimization (GRPO). This GRPO algorithm maximizes the following objective by updating the policy model π.</li>
</ol>
<p>Maximize this objective by updating the weights of the model based on the reward.</p>
<p>Advantage is defined as the normalized reward.
In LLM case, the policy model π is model itself, and θ is weights of the model. q is question and o is output of the model. We can interpret the policy model(LLM) outputs a probability distribution over tokens, where the policy π(o|q) is a probability of output o gi"
What is the purpose of using KL divergence in the GRPO algorithm?,"The KL divergence is used in the GRPO algorithm to measure the difference between the current policy model and the reference policy model (initial base model). It is minimized to ensure that the policy model does not deviate too far from the base model, thereby preventing the model from forgetting basic language understanding and important knowledge learned during pre-training and fine-tuning.","ven the question q. Therefore, the policy model is LLM itself. If the output o is right answer, we should reinforce the probability of that model makes this output o. So we need to maximize π(o|q) by multiplying advantage(normalized reward). If the output o is correct, the advantage (reward) will be a positive value and the policy will be reinforced. Otherwise, it will be negative and π(o|q) should be minimized. Plus, we have a fine-tuned model as the initial base model and do not want it to go too far from this base model, which might cause model to forget basic language understanding and important knowledge that the model learned during pre-training and fine-tuning. To implement this safety concerns, GRPO algorithm used KL divergence and epsilon parameter. The KL divergence measures the difference between current policy model and reference policy model(initial base model). So the KL divergence term should be minimized to maximized the GRPO objective. And we pick minimum between the original policy and the c"
What is the potential benefit of using the DeepSeek-V3 model in AI development?,"The DeepSeek-V3 model offers the potential benefit of efficient training with cheaper GPUs, making it more economical to train compared to other models like the OpenAI model. Additionally, its open-source nature allows AI researchers to directly use the DeepSeek models and implement innovative ideas and designs in their own models. This can lead to improved model performance and efficient training processes.","lipped policy in (1-ε, 1+ε), by which the model cannot deviate too much from 1. So, the current policy cannot differ a lot from the old policy, restricting the effect of reinforcement learning. This GRPO algorithm based on rule-based and model-based reward model enhances model performance and reasoning capability.</p>
<ol>
<li>Conclusion
DeepSeek-V3 model offered great opportunity for efficient training with cheaper GPUs. It is unclear that its performance exceeds the OpenAI model, but DeepSeek is way more economical to train and open-source model. AI researchers can directly use DeekSeek models and they can also implement the innovative ideas and designs in their own model, because the new methods of DeepSeek and source code are opened. Seemingly, the DeepSeek researchers have potential to come up with more advanced idea to improve the model performance and efficient training process. In AI development, a lower training cost almost always implies better model accuracy later on, as the data and model can easi"
What concern is expressed about the performance of a good AI model in relation to the Chinese government?,The concern is that the performance of a good AI model might be undermined by the censorship and suppression of the Chinese government.,"ly be scaled up at a lower cost. I hope that the performance of a good AI model does not have to be undermined by the censorship and suppression of the Chinese government.</li>
</ol>"
What role does the cluster manager play in the 3FS system?,"The cluster manager in the 3FS system handles membership changes, distributes cluster configuration to other services and clients, and manages the election of a primary manager among multiple deployed cluster managers. It also receives heartbeats from metadata and storage services to monitor their status.","<h1>Design Notes</h1>
<h2>Design and implementation</h2>
<p>The 3FS system has four components: cluster manager, metadata service, storage service and client. All components are connected in an RDMA network (InfiniBand or RoCE).</p>
<p>Metadata and storage services send heartbeats to cluster manager. Cluster manager handles membership changes and distributes cluster configuration to other services and clients. Multiple cluster managers are deployed and one of them is elected as the primary. Another manager is promoted as primary when the primary fails. Cluster configuration is typically stored in a reliable distributed coordination service, such as ZooKeeper or etcd. In our production environment, we use the same key-value store as file metadata to reduce dependencies.</p>
<p>File metadata operations (e.g. open or create files/directories) are sent to metadata services, which implement the file system semantics. Metadata services are stateless, since file metadata are stored in a transactional key-value store"
What replication method does the storage service implement to ensure strong consistency in the system described?,The storage service implements Chain Replication with Apportioned Queries (CRAQ) to ensure strong consistency.," (e.g. FoundationDB). Clients can connect to any metadata service.</p>
<p>Each storage service manages a few local SSDs and provides a chunk store interface. The storage service implements Chain Replication with Apportioned Queries (CRAQ) to ensure strong consistency. CRAQ’s write-all-read-any approach helps to unleash the throughput of SSDs and RDMA network. A 3FS file is split into equally sized chunks, which are replicated over multiple SSDs.</p>
<p>Two clients are developed for applications: FUSE client and native client. Most applications use FUSE client, which has a low adoption barrier. Performance-critical applications are integrated with the native client.</p>
<h2>File system interfaces</h2>
<p>Object store is becoming a popular option for data analytics and machine learning. However, file system semantics and a unified namespace where files are organized in directories provide greater flexibility for applications.</p>
<ul>
<li>
<p><em>Atomic directory manipulation</em> An object store can approximat"
What is a common pattern in internal applications for handling temporary directories and files?,"A common pattern in internal applications involves creating a temporary directory, writing files to it, and then moving the directory to its final location.","e hierarchical directory structures by using slashes (/) in object keys. However, it doesn’t natively support operations like atomically moving files/directories, or recursively deleting entire directories. Actually a common pattern in our internal applications involves creating a temporary directory, writing files to it, and then moving the directory to its final location. When handling a large number of small files, the recursive delete for directories is crucial. Without it, applications have to traverse each directory and remove files one by one.</p>
</li>
<li>
<p><em>Symbolic and hard links</em> Our applications utilize symbolic and hard links to create lightweight snapshots of dynamically updated datasets, where new data is appended as individual files.</p>
</li>
<li>
<p><em>Familiar interface</em> The file interface is well known and used everywhere. There is no need to learn a new storage API. Many datasets are stored as CSV/Parquet files. Adapting file-based data loaders to use the 3FS FUSE client or"
What are the performance limitations of FUSE when used for file system client development?,"The performance limitations of FUSE include memory copy overhead, where data transfer between kernel and user spaces consumes memory bandwidth and increases latency, and primitive multi-threading support, where lock contention prevents FUSE's I/O processing capability from scaling with the number of threads."," native client is straightforward.</p>
</li>
</ul>
<h3>Limitations of FUSE</h3>
<p>FUSE (Filesystem in Userspace) simplifies file system client development by redirecting I/O operations to user-space processes through the FUSE kernel module. It creates the illusion that applications are accessing the remote file system as if it were a local file system. However, it has performance limitations:</p>
<ul>
<li>
<p><em>Memory copy overhead</em> The user-space file system daemon cannot access application memory. Data transfer between kernel and user spaces consumes memory bandwidth and increases end-to-end latency.</p>
</li>
<li>
<p><em>Primitive multi-threading support</em> When an application initiates I/O requests, FUSE places these requests into a multi-threaded shared queue, protected by a spin lock. The user-space file system daemon then retrieves and processes requests from this queue. Due to lock contention, FUSE’s I/O processing capability fails to scale with the number of threads. Our benchmark results in"
"What is a limitation of FUSE on Linux 5.x regarding write operations, and how do applications work around it?","FUSE on Linux 5.x does not support concurrent writes to the same file. Applications work around this limitation by writing to multiple files concurrently, which maximizes the total throughput.","dicate that FUSE only handles approximately 400K 4KiB reads per second. Further increasing concurrency does not improve performance as lock contention intensifies. <code>perf</code> profiling reveals that the kernel-space spin lock consumes a significant amount of CPU time.</p>
</li>
</ul>
<p>Most applications, e.g. data analytics, perform large block writes on 3FS or they can buffer data in memory and flush it to 3FS when write buffer is full. However, FUSE on Linux 5.x does not support concurrent writes to the same file<a href=""https://elixir.bootlin.com/linux/v5.4.284/source/fs/fuse/file.c#L1573"">^1</a>. Applications overcome this limitation by writing to multiple files concurrently, maximizing the total throughput.</p>
<p>Read operations exhibit more complex patterns. Some training jobs require random access to dataset samples, with read sizes varying from a few kilobytes to several megabytes per sample. And samples are typically not 4K-aligned in files. Data loaders are specifically designed to fetch bat"
What are the challenges associated with developing a file system client as a VFS kernel module?,"Developing a file system client as a VFS kernel module is challenging because kernel module development is more complex than user-space system programming. Bugs in kernel modules are difficult to diagnose and can lead to catastrophic failures, such as machine crashes without log messages for debugging. Additionally, upgrading a kernel module requires stopping all processes using the file system cleanly; otherwise, a machine restart is necessary.","ches of samples. But they perform poorly when handling small random reads on FUSE-mounted 3FS. Bandwidth of SSDs and RDMA network are not fully utilized.</p>
<h3>Asynchronous zero-copy API</h3>
<p>Implementing the file system client as a VFS kernel module avoids performance issues mentioned above. But kernel module development is significantly more challenging than user-space system programming. Bugs are difficult to diagnose and can lead to catastrophic failures in production environments. For example, machines may crash and leave no log message for debugging. When upgrading a kernel module, all processes using the file system must be stopped cleanly; otherwise, a machine restart is required.</p>
<p>For these reasons, we have chosen to implement a native client within the FUSE daemon. This client offers an interface that supports asynchronous zero-copy I/O operations. File meta operations are still handled by FUSE daemon (e.g. open/close/stat files). Applications call <code>open()</code> to obtain a file des"
"What is the purpose of the Iov data structure in the asynchronous, zero-copy API inspired by Linux io_uring?","The Iov data structure is a large memory region used for zero-copy read/write operations, shared between the user process and the native client. It is managed by the client for InfiniBand memory registration, and all read data is read into Iov, while all write data should be written to Iov before calling the API.","criptor (fd) and register it via native API. They can then perform I/O operations on the file with native client. This approach ensures consistency in metadata operations with the POSIX API, making it easier to migrate existing code.</p>
<p>The asynchronous, zero-copy API is inspired by Linux <code>io_uring</code>. Below are the key data structures in the API:</p>
<ul>
<li>
<p><em>Iov</em> A large memory region for zero-copy read/write operations, shared between the user process and the native client. InfiniBand memory registration is managed by the client. In native API, all read data will be read into Iov, and all write data should be written to Iov before calling the API.</p>
</li>
<li>
<p><em>Ior</em> A small shared ring buffer for communication between user process and native client. The usage of Ior is similar to Linux <code>io_uring</code>, where the user process enqueues read/write requests, and the native client dequeues these requests for completion. The requests are executed in batches, with their "
Why are multiple rings recommended for multi-threaded applications in the context of I/O processing?,"Multiple rings are recommended for multi-threaded applications because sharing a ring requires synchronization, which can impact performance. Using multiple rings allows for parallel processing without the need for synchronization, thus improving performance.","sizes controlled by the <code>io_depth</code> parameter. Multiple batches are processed in parallel, whether from different rings or the same ring. However, multiple rings are still recommended for multi-threaded applications, as sharing a ring requires synchronization, which can impact performance.</p>
</li>
</ul>
<p>Within the native client, multiple threads are spawned to fetch I/O requests from the Iors. These requests are batched and dispatched to storage services, reducing RPC overhead caused by small read requests.</p>
<h2>File metadata store</h2>
<h3>Location of file chunks</h3>
<p>3FS divides file data into equally sized chunks and stripes them across multiple replication chains (replication chains and chain tables are defined in Section <a href=""#data-placement"">Data placement</a>). Users can specify the chain table, chunk size, and stripe size for files on a per-directory basis. Each chunk is independently stored on multiple storage services, with its chunk ID generated by concatenating the file’s "
What strategy does the metadata service use when creating a new file to ensure balanced data distribution across chains and SSDs?,"The metadata service uses a round-robin strategy to select consecutive replication chains from the designated chain table, based on the stripe size, and then generates a random seed to shuffle the selected chains. This allocation strategy ensures balanced data distribution across chains and SSDs.","inode id and chunk index.</p>
<p>When creating a new file, the metadata service employs a round-robin strategy to select consecutive replication chains from the designated chain table, based on the stripe size. Next, a random seed is generated to shuffle the selected chains. This allocation strategy ensures balanced data distribution across chains and SSDs.</p>
<p>When an application opens a file, the client contacts the meta service to obtain the file’s data layout information. Then the client can independently compute chunk IDs and chains for data operations, minimizing the involvement of the meta service in the critical path.</p>
<h3>File metadata on transactional key-value store</h3>
<p>3FS uses FoundationDB as its distributed storage system for metadata. FoundationDB provides a key-value store interface and supports transactions with Serializable Snapshot Isolation (SSI). 3FS stores all metadata as key-value pairs in FoundationDB. Meta services follow a stateless architecture, greatly enhancing maintaina"
What are the two core structures that make up the file system metadata mentioned in the text?,The two core structures that make up the file system metadata are inodes and directory entries.,"bility by allowing administrators to seamlessly upgrade or restart services without disruption. When clients experience request failures or timeouts, they can automatically fail over to other available services.</p>
<p>The file system metadata primarily consists of two core structures: inodes and directory entries. Inodes store attribute information for files, directories, and symbolic links, each identified by a globally unique 64-bit identifier that increments monotonically. Inode keys are constructed by concatenating the ""INOD"" prefix with the inode id, which is encoded in little-endian byte order to spread inodes over multiple FoundationDB nodes. The inode values vary by its type:</p>
<ul>
<li>
<p>All inode types contain basic attributes: ownership, permissions, access/modification/change times.</p>
</li>
<li>
<p>Additional attributes for file inodes: file length, chunk size, selected range in chain table, shuffle seed.</p>
</li>
<li>
<p>Additional attributes for directory inodes: the parent directory’s i"
What is required to prevent loops when moving directories in a file system?,"To prevent loops when moving directories, the parent’s inode id is required. This ensures that when moving a directory like <code>dir_a/dir_b</code> to <code>dir_c/</code>, it is verified that <code>dir_c</code> is not a descendant of <code>dir_b</code> by checking all ancestors of <code>dir_c</code> upward.","node id, default layout configurations for subdirectories/files (chain table, chunk size, stripe size). The parent’s inode id is required to detect loops when moving directories. When moving <code>dir_a/dir_b</code> to <code>dir_c/</code>, we need to ensure that <code>dir_c</code> is not a descendant of <code>dir_b</code>, which can be achieved by checking all ancestors of <code>dir_c</code> upward.</p>
</li>
<li>
<p>Additional attributes for symbolic link inodes: target path string.</p>
</li>
</ul>
<p>Directory entry keys are composed of a ""DENT"" prefix, the parent inode ID, and the entry name. Directory entry values store the target inode id and inode type. All entries within a directory naturally form a contiguous key range, allowing efficient directory listing via range queries.</p>
<p>The meta operations leverage FoundationDB’s transactions:</p>
<ul>
<li>
<p>Read-only transactions used for metadata queries: fstat, lookup, listdir etc.</p>
</li>
<li>
<p>Read-write transactions used for metadata updates: c"
How does FoundationDB handle concurrent transaction conflicts in write transactions?,FoundationDB tracks the read/write key sets to form conflict detection sets and automatically retries the transaction when concurrent transaction conflicts are detected. This design allows multiple meta services to process requests in parallel while maintaining file system metadata consistency.,"reate, link, unlink, rename etc.</p>
</li>
</ul>
<p>For write transactions, FoundationDB tracks the read/write key sets to form conflict detection sets. When concurrent transaction conflicts are detected, the meta service automatically retries the transaction. This design enables multiple meta services to process requests in parallel while maintaining file system metadata consistency.</p>
<h3>Dynamic file attributes</h3>
<p>On most local file systems, deleting an opened file is deferred until all associated file descriptors are closed. Consequently, it is necessary to track all file descriptors of the file. Training jobs open a large number of files during startup. Storing all file descriptors would impose heavy load on meta service and FoundationDB. Since training jobs do not depend on this feature, 3FS does not track file descriptors opened in read-only mode.</p>
<p>3FS maintains a file session for each file descriptor (fd) opened in write mode since deleting write opened files may lead to unreclaimable gar"
How does the 3FS meta service handle file length updates for files being actively written to by multiple clients?,"The 3FS meta service handles file length updates by having clients periodically report the maximum write position of each file opened in write mode. If this position exceeds the length stored in the inode and there is no concurrent truncate operation, it is adopted as the new file length. This method ensures eventual consistency for file lengths due to the possibility of concurrent writes from multiple clients. When processing close/fsync operations, the meta service obtains the precise file length by querying the ID and length of the last chunk.","bage chunks from concurrent writes. When a file with active write sessions is deleted, meta service delays the deletion until all its fds are closed. To prevent lingering sessions from offline clients, the 3FS meta service periodically checks client liveness and cleans up sessions of offline clients.</p>
<p>The file length is stored in the inode. For files being actively updated, the length stored in inode may diverge from the actual length. Clients periodically (5 seconds by default) report to meta service maximum write position of each file opened in write mode. If this position exceeds the length in inode and there is no concurrent truncate operation, this position is adopted as the new file length.</p>
<p>Due to the possibility of concurrent writes from multiple clients, the method described above ensures only eventual consistency for file lengths. When processing close/fsync operations, the meta service obtains the precise file length by querying the ID and length of the last chunk from the storage servi"
How does the meta service mitigate transaction conflicts when updating a file's length in a distributed file system?,The meta service mitigates transaction conflicts by distributing file length update tasks across multiple meta services using inode IDs and the rendezvous hash algorithm. This approach helps to manage concurrent updates and reduce the likelihood of conflicts.,"ce. Since file data is striped across multiple chains, this operation incurs non-negligible overhead.</p>
<p>Concurrent updates to the same file’s length by multiple meta services may cause transaction conflicts and lead to repeated file length computation. To mitigate this, meta service distributes file length update tasks across multiple meta services using inode IDs and the rendezvous hash algorithm.</p>
<p>Our production environments use a large stripe size: 200. For small files, the number of chains containing file chunks is well below this number. The number of potentially used chains is stored in file inode and used as a hint when updating the length. It starts with an initial value of 16 and is doubled each time additional file chunks are written to more chains. This allows us to avoid querying all 200 chains when updating lengths of small files. This optimization can also be extended to the deletion of small files.</p>
<h2>Chunk storage system</h2>
<p>The design goal of chunk storage system is to ach"
How does CRAQ handle write and read requests in the data placement strategy described?,"In CRAQ, write requests are sent to the head target and propagated along a chain, while read requests can be sent to any of the storage targets. This allows for even distribution of read traffic among all targets in a chain for better load balance.","ieve the highest bandwidth possible even when there are storage medium failures. The read/write throughput of 3FS should scale linearly with the number of SSDs and bisection network bandwidth between clients and storage services. Applications access storage services in a locality-oblivious manner.</p>
<h3>Data placement</h3>
<p>Each file chunk is replicated over a chain of storage targets using chain replication with apportioned queries (CRAQ). In CRAQ write requests are sent to the head target and propagated along a chain. Read requests can be sent to any of the storage target. Usually the read traffic is evenly distributed among all targets in a chain for better load balance. Multiple storage targets are created on each SSD and the targets join different chains.</p>
<p>Suppose there are 6 nodes: A, B, C, D, E, F. Each node has 1 SSD. Create 5 storage targets on each SSD: 1, 2, ... 5. Then there are 30 targets in total: A1, A2, A3, ..., F5. If each chunk has 3 replicas, a chain table is constructed as follow"
What is the target 2 value for chain 4 in version 1?,The target 2 value for chain 4 in version 1 is <code>E2</code>.,"s.</p>
<p>| Chain | Version | Target 1 (head) | Target 2 | Target 3 (tail) |
| :---: | :-----: | :-------------: | :------: | :-------------: |
|   1   |    1    |      <code>A1</code>       |   <code>B1</code>   |      <code>C1</code>       |
|   2   |    1    |      <code>D1</code>       |   <code>E1</code>   |      <code>F1</code>       |
|   3   |    1    |      <code>A2</code>       |   <code>B2</code>   |      <code>C2</code>       |
|   4   |    1    |      <code>D2</code>       |   <code>E2</code>   |      <code>F2</code>       |
|   5   |    1    |      <code>A3</code>       |   <code>B3</code>   |      <code>C3</code>       |
|   6   |    1    |      <code>D3</code>       |   <code>E3</code>   |      <code>F3</code>       |
|   7   |    1    |      <code>A4</code>       |   <code>B4</code>   |      <code>C4</code>       |
|   8   |    1    |      <code>D4</code>       |   <code>E4</code>   |      <code>F4</code>       |
|   9   |    1    |      <code>A5</code>       |   <code>B5</code>   |      <cod"
What is the purpose of creating different chain tables in a storage system?,"Different chain tables are created to support various data placement requirements, such as separating storage targets for batch/offline jobs and online services. This allows for optimized data management by using mutually exclusive nodes and SSDs for different types of workloads.","e>C5</code>       |
|  10   |    1    |      <code>D5</code>       |   <code>E5</code>   |      <code>F5</code>       |</p>
<p>Each chain has a version number. The version number is incremented if the chain is changed (e.g. a storage target is offline). Only the primary cluster manager makes changes to chain tables.</p>
<p>A few chain tables can be constructed to support different data placement requirements. For example, two chain tables can be created, one for batch/offline jobs and another for online services. The two tables consist of storage targets on mutually exclusive nodes and SSDs.</p>
<p>Logically, the state of each chain changes independently. Each chain can be included in multiple chain tables. The concept of chain table is created to let metadata service pick a table for each file and stripe file chunks across chains in the table.</p>
<h3>Balanced traffic during recovery</h3>
<p>Suppose read traffic is evenly distributed among all storage targets in the above chain table. When A fails its read r"
What strategy is suggested to mitigate the performance impact when an SSD fails and its read traffic is redirected?,"The strategy suggested is to have more SSDs share the redirected traffic. Specifically, when an SSD like A fails, each of the other SSDs receives 1/5 of A’s read traffic, thereby distributing the load and reducing the performance impact.","equests would be redirected to B and C. Under heavy load the read bandwidth of B, C is immediately saturated and B, C become the bottleneck of the entire system. Replacing a failed SSD and syncing data to the new SSD can take several hours. The read throughput is impaired during this period.</p>
<p>To reduce the performance impact, we can have more SSDs share the redirected traffic. In the following chain table, A is paired with every other SSDs. When A fails, each of the other SSDs receives 1/5 of A’s read traffic.</p>
<p>| Chain | Version | Target 1 (head) | Target 2 | Target 3 (tail) |
| :---: | :-----: | :-------------: | :------: | :-------------: |
|   1   |    1    |      <code>B1</code>       |   <code>E1</code>   |      <code>F1</code>       |
|   2   |    1    |      <code>A1</code>       |   <code>B2</code>   |      <code>D1</code>       |
|   3   |    1    |      <code>A2</code>       |   <code>D2</code>   |      <code>F2</code>       |
|   4   |    1    |      <code>C1</code>       |   <code>D3</"
What is CRAQ and what is it optimized for?,CRAQ is a write-all-read-any replication protocol that is optimized for read-heavy workloads.,"code>   |      <code>E2</code>       |
|   5   |    1    |      <code>A3</code>       |   <code>C2</code>   |      <code>F3</code>       |
|   6   |    1    |      <code>A4</code>       |   <code>B3</code>   |      <code>E3</code>       |
|   7   |    1    |      <code>B4</code>       |   <code>C3</code>   |      <code>F4</code>       |
|   8   |    1    |      <code>B5</code>       |   <code>C4</code>   |      <code>E4</code>       |
|   9   |    1    |      <code>A5</code>       |   <code>C5</code>   |      <code>D4</code>       |
|  10   |    1    |      <code>D5</code>       |   <code>E5</code>   |      <code>F5</code>       |</p>
<p>To achieve maximum read throughput during recovery, the load balance problem can be formulated as a balanced incomplete block design. The optimal solution is obtained by using integer programming solver.</p>
<h3>Data replication</h3>
<p>CRAQ is a write-all-read-any replication protocol optimized for read-heavy workloads. Utilizing read bandwidth of all replicas is critical to"
What happens if the chain version in a write request does not match the latest known version in an all-flash storage system?,"If the chain version in a write request does not match the latest known version, the storage service will reject the request."," achieve highest read throughput in an all-flash storage system.</p>
<p>When a write request is received by a storage service, it goes through the following steps:</p>
<ol>
<li>
<p>The service checks if the chain version in write request matches with the latest known version; reject the request if it’s not. The write request could be sent by a client or a predecessor in the chain.</p>
</li>
<li>
<p>The service issues RDMA Read operations to pull write data. If the client/predecessor fails, the RDMA Read operations may time out and the write is aborted.</p>
</li>
<li>
<p>Once the write data is fetched into local memory buffer, a lock for the chunk to be updated is acquired from a lock manager. Concurrent writes to the same chunk are blocked. All writes are serialized at the head target.</p>
</li>
<li>
<p>The service reads the committed version of the chunk into memory, applies the update, and stores the updated chunk as a pending version. A storage target may store two versions of a chunk: a committed version "
What happens when a write request reaches the tail of the chain in a storage service?,"When a write request reaches the tail of the chain, the committed version is atomically replaced by the pending version, and an acknowledgment message is sent to the predecessor. The current chain version is also stored as a field in the chunk metadata.","and a pending version. Each version has a monotonically-increasing version number. The version numbers of committed version and pending versions are <code>v</code> and <code>u</code> respectively, and satisfy <code>u = v + 1</code>.</p>
</li>
<li>
<p>If the service is the tail, the committed version is atomically replaced by the pending version and an acknowledgment message is sent to the predecessor. Otherwise, the write request is forwarded to the successor. When the committed version is updated, the current chain version is stored as a field in the chunk metadata.</p>
</li>
<li>
<p>When an acknowledgment message arrives at a storage service, the service replaces the committed version with the pending version and continues to propagate the message to its predecessor. The local chunk lock is then released.</p>
</li>
</ol>
<p>Suppose there are 3 targets in the chain: <code>A, B, C</code>. A write request has just entered step 5 at <code>A</code>. <code>A</code> forwards the request to successor <code>B</code>"
What happens when a write request is forwarded to a new successor in a chain after a failure?,"When a write request is forwarded to a new successor, such as <code>C</code>, after a failure of <code>B</code>, <code>C</code> may initially reject the request if it has not yet received the latest chain table. However, <code>A</code> can continue to forward the request to <code>C</code> until <code>C</code> receives the updated chain table and accepts the request.",". Then <code>B</code> instantly fails and the forwarded write request is lost. When cluster manager detects <code>B</code>’s failure, it marks <code>B</code> as offline and moves it to the end of chain and broadcasts the updated chain table. Once <code>A</code> receives the latest chain table, it forwards the write request to the new successor <code>C</code>. <code>C</code> may not receive the latest chain table yet and rejects the request. But <code>A</code> can keep forwarding the request to <code>C</code>. Eventually <code>C</code> gets the latest chain table and accepts the request.</p>
<p>When a read request arrives at a storage service:</p>
<ol>
<li>
<p>When the service only has a committed version of the chunk, this version is returned to the client.</p>
</li>
<li>
<p>Unlike CRAQ, our implementation does not issue version query to the tail target. When there are both committed and pending versions, the service replies a special status code to notify the client. The client may wait for a short interval "
How does the cluster manager detect fail-stop failures in the system?,"The cluster manager detects fail-stop failures by relying on heartbeats. It declares a service failed if it does not receive heartbeats from it for a configurable interval (e.g., T seconds). Additionally, a service will stop processing requests and exit if it cannot communicate with the cluster manager for T/2 seconds. The heartbeat acts as a request to renew a lease granted by the manager.","and retry. Or the client can issue a relaxed read request to get the pending version.</p>
</li>
</ol>
<h3>Failure detection</h3>
<p>The cluster manager relies on heartbeats to detect fail-stop failures. Cluster manager declares a service failed if it does not receive heartbeats from it for a configurable interval (e.g. T seconds). A service stops processing requests and exits if it cannot communicate with cluster manager for T/2 seconds. The heartbeat can be seen as a request to *renew a lease* granted by the manager.</p>
<p>The metadata services are stateless. The list of online meta services provided by cluster manager is a simple service discovery mechanism that helps clients create connections to metadata services. If one meta service is down, the clients may switch to any other metadata service.</p>
<p>Cluster manager plays a more critical role in membership changes of storage services. It maintains a global view of chain tables and storage targets’ states. Each storage target has a public state and a lo"
"What does the ""serving"" public state indicate about a service's ability to handle requests?","The ""serving"" public state indicates that the service is alive and capable of handling both read and write requests, meaning it is actively serving client requests.","cal state.</p>
<p>Public state indicates if it’s ready to serve read requests and if write requests would be propagated to it. Public states are stored with chain tables and distributed to services and clients.</p>
<p>| Public State | Read | Write | Notes                                           |
| :----------- | :--: | :---: | :---------------------------------------------- |
| serving      |  Y   |   Y   | service alive and serving client requests       |
| syncing      |  N   |   Y   | service alive and data recovery is in progress  |
| waiting      |  N   |   N   | service alive and data recovery not started yet |
| lastsrv      |  N   |   N   | service down and it was the last serving target |
| offline      |  N   |   N   | service down or storage medium failure          |</p>
<p>Local state is only known by storage services and cluster manager, and it’s stored in the memory of cluster manager. If a storage target has medium failure, the related service sets the target’s local state to offline in hear"
What happens to a storage target when it is marked offline?,"When a storage target is marked offline, it is moved to the end of the chain.","tbeat. If a storage service is down, storage targets managed by the service are marked offline.</p>
<p>| Local State | Notes                                                |
| :---------- | :--------------------------------------------------- |
| up-to-date  | service alive and serving client requests            |
| online      | service alive and target in syncing or waiting state |
| offline     | service down or storage medium failure               |</p>
<p>A storage target can change from one public state to another in response to the latest local state. The local state plays the role of a triggering event. The cluster manager periodically scans every chain and updates the public states of targets on the chain according to a state-transition table.</p>
<ul>
<li>
<p>The chain version is incremented if the chain is updated.</p>
</li>
<li>
<p>If a storage target is marked offline, it’s moved to the end of chain.</p>
</li>
<li>
<p>If a storage service finds public state of any local storage target is lastsrv "
What happens to the storage target's local state once data recovery is completed in a syncing state?,"Once the data recovery of a storage target in syncing state is completed, the storage service sets the target’s local state to ""up-to-date"" in subsequent heartbeat messages sent to the cluster manager.","or offline, it exits immediately. The service may be isolated from the cluster manager by network partition error.</p>
</li>
<li>
<p>Once the data recovery of a storage target in syncing state is completed, the storage service set the target’s local state to up-to-date in subsequent heartbeat messages sent to cluster manager.</p>
</li>
</ul>
<p>| Local State | Current Public State | Predecessor’s Public State | Next Public State |
| :---------- | :------------------- | :------------------------- | :---------------- |
| up-to-date  | serving              | (any)                      | serving           |
|             | syncing              | (any)                      | serving           |
|             | waiting              | (any)                      | waiting           |
|             | lastsrv              | (any)                      | serving           |
|             | offline              | (any)                      | waiting           |
| online      | serving              | (any)                 "
"What state does a system transition to if it is currently in the ""offline"" state and ""syncing""?","The system transitions to the ""offline"" state if it is currently in the ""offline"" state and ""syncing"".","     | serving           |
|             | syncing              | serving                    | syncing           |
|             |                      | not serving                | waiting           |
|             | waiting              | serving                    | syncing           |
|             |                      | not serving                | waiting           |
|             | lastsrv              | (any)                      | serving           |
|             | offline              | (any)                      | waiting           |
| offline     | serving              | has no predecessor         | lastsrv           |
|             |                      | has predecessor            | offline           |
|             | syncing              | (any)                      | offline           |
|             | waiting              | (any)                      | offline           |
|             | lastsrv              | (any)                      | lastsrv           |
|             | offline      "
What happens to storage targets when a storage service exits or a storage medium failure occurs?,"When a storage service exits or a storage medium failure occurs, all related storage targets are marked as offline and moved to the end of chains by the cluster manager. Once the service restarts, each target enters into the recovery process independently.","        | (any)                      | offline           |</p>
<h3>Data recovery</h3>
<p>When a storage service exits (e.g. process crashes or restarts during upgrade), or a storage medium failure occurs, all related storage targets will be marked as offline and moved to the end of chains by cluster manager. Once the service restarts, each target on the service enters into the recovery process independently. The entire recovery process overlaps with normal activity and minimizes any interruption.</p>
<p>When a previously offline storage service starts:</p>
<ol>
<li>
<p>The service periodically pulls latest chain tables from cluster manager. But it does not send heartbeats until all its storage targets have been marked offline in the latest chain tables. This ensures all its targets would go through the data recovery process.</p>
</li>
<li>
<p>When a write request arrives during recovery, the request is always a full-chunk-replace write. The local committed version is updated and any existing pending version i"
What happens when a storage service finds a previously offline successor is online?,"When a storage service finds a previously offline successor is online, it starts to forward normal write requests to the successor.","s abandoned. Since current service is the tail, an acknowledgment message is sent to the predecessor. The full state of the predecessor is copied to the returning service through a continuous stream of full-chunk-replace writes.</p>
</li>
<li>
<p>Before the data recovery of a storage target starts, the predecessor sends a dump-chunkmeta request to the returning service. Then the service iterates the local chunk metadata store to collect the ids, chain versions and committed/pending version numbers of all chunks on the target, and replies the collected metadata to the predecessor.</p>
</li>
<li>
<p>When a sync-done message arrives, the service knows that the storage target is up-to-date. It sets local state of the target to up-to-date in heartbeat messages sent to cluster manager.</p>
</li>
</ol>
<p>When a storage service finds a previously offline successor is online:</p>
<ol>
<li>
<p>The service starts to forward normal write requests to the successor. Clients may only update a portion of the chunk, but the "
What is the process for transferring selected chunks to a successor in the described system?,"The process for transferring selected chunks to a successor involves several steps: First, the service sends a dump-chunkmeta request to the successor to receive the metadata of all chunks on the successor target. It then collects the chunk metadata on its local target and compares the two sets of metadata to decide which chunks should be transferred. The selected chunks are transferred by issuing full-chunk-replace write requests. For each chunk, a chunk lock is acquired, and the chain version, committed version number, and chunk content are read and transferred to the successor. Finally, the chunk lock is released, and once all required chunks have been transferred, a sync-done message is sent to the successor.","forwarded write requests should contain the whole chunk, i.e. a full-chunk-replace write.</p>
</li>
<li>
<p>The service sends a dump-chunkmeta request to the successor. Once the metadata of all chunks on the successor target are received, it collects the chunk metadata on its local target. Then it compares the two copies of chunk metadata to decide which chunks should be transferred.</p>
</li>
<li>
<p>The selected chunks are transferred to the successor by issuing full-chunk-replace write requests.</p>
</li>
<li>
<p>The chunk lock is first acquired for each chunk.</p>
</li>
<li>
<p>The chain version, committed version number and chunk content are read and transferred to successor by sending a full-chunk-replace request.</p>
</li>
<li>
<p>The chunk lock is released.</p>
</li>
</ol>
<p>4. When all required chunks have been transferred, a sync-done message is sent to the successor.</p>
<p>The rules used to decide which chunks should be transferred are:</p>
<ul>
<li>
<p>If a chunk only exists on the local target,"
What happens if the chain version of a local chunk replica is greater than that of the remote chunk replica?,"If the chain version of a local chunk replica is greater than that of the remote chunk replica, the local chunk should be transferred."," it should be transferred.</p>
</li>
<li>
<p>If a chunk only exists on the remote target, it should be removed.</p>
</li>
<li>
<p>If the chain version of local chunk replica is greater than that of the remote chunk replica, it should be transferred.</p>
</li>
<li>
<p>If the chain versions of local/remote chunk replicas are the same but local committed version number does not equal to the remote pending version number, it should be transferred.</p>
</li>
<li>
<p>Otherwise, two chunk replicas are either the same or being updated by in-progress write requests.</p>
</li>
</ul>
<h3>Chunks and the metadata</h3>
<p>File chunks are stored in the chunk engine. On each SSD, the persistent storage of the chunk engine consists of a fixed number of data files for storing chunk data, and a RocksDB instance for maintaining chunk metadata and other system information. Additionally, the chunk engine maintains an in-memory cache of chunk metadata to enhance query performance. A chunk allocator is implemented for fast allocatio"
"What is the purpose of the ""commit"" operation in the chunk engine interface?","The ""commit"" operation is used to commit the updated chunk metadata to RocksDB via write batches to ensure atomic updates, and it synchronously refreshes the chunk metadata cache.","n of new chunks. The chunk engine interface provides thread-safe access through the following operations:</p>
<ol>
<li>
<p><em>open/close</em> Initializes the engine by loading metadata from RocksDB and reconstructing chunk allocator states.</p>
</li>
<li>
<p><em>get</em> Retrieves chunk metadata and reference-counted handle through a hashmap cache, enabling concurrent access with O(1) average complexity.</p>
</li>
<li>
<p><em>update</em> Implements copy-on-write (COW) semantics by allocating new chunks before modifying data. Old chunks remain readable until all handles are released.</p>
</li>
<li>
<p><em>commit</em> Commit the updated chunk metadata to RocksDB via write batches to ensure atomic updates; synchronously refresh the chunk metadata cache.</p>
</li>
</ol>
<p>The chunk data will ultimately be stored on physical blocks. Physical block sizes range from 64KiB to 64MiB in increments of powers of two, totaling 11 distinct sizes. The allocator will assign physical blocks whose sizes most closely match th"
How does the system handle the allocation of new physical blocks when no available blocks remain?,"When no available physical blocks remain, the system uses the <code>fallocate()</code> function to allocate a contiguous large space in physical files, creating 256 new physical blocks. This approach helps reduce disk fragmentation.","e actual chunk size. A resource pool is constructed for each physical block size, with each pool containing 256 physical files. The usage status of physical blocks is maintained in memory using bitmaps. When a physical block is reclaimed, its bitmap flag is set to 0. The actual storage space of the block remains preserved and will be prioritized for subsequent allocations. When no available physical blocks remain, <code>fallocate()</code> will be used to allocate a contiguous large space in physical files, creating 256 new physical blocks - this approach helps reduce disk fragmentation.</p>
<p>When performing write operations on a chunk, the allocator first assigns a new physical block. The system then reads existing chunk data into a buffer, applies the update, and writes the updated buffer to the newly allocated block. An optimized process is implemented for appends, where data is directly added in-place at the end of the existing block. A new copy of metadata is constructed from the new block's location an"
How are the new chunk metadata and statuses of physical blocks updated in the system described?,The new chunk metadata and statuses of both new and old physical blocks are atomically updated in RocksDB.,"d existing chunk metadata. Subsequently, both the new chunk metadata and statuses of new and old physical blocks are atomically updated in RocksDB.</p>"
"What is the main contribution of the paper titled ""DIMSUM: Discourse in Mathematical Reasoning as a Supervision Module""?","The main contribution of the paper is the introduction of discourse structure as a novel information source to help models reason better on the GSM8k dataset, particularly for models with less data or inferior training. The paper demonstrates that incorporating discourse structure can improve performance significantly, even for models that have likely memorized the dataset, and enhances large model performance on out-of-distribution examples.","arXiv:2503.04685v1  [cs.CL]  6 Mar 2025DIMSUM: Discourse in Mathematical Reasoning as a Supervisi on Module
Krish Sharma1,2,†, Niyar R Barman1,2,†, Nicholas Asher1, Akshay Chaturvedi1
1IRIT, Toulouse, France
2NIT Silchar, India
Abstract
We look at reasoning on GSM8k, a dataset
of short texts presenting primary school, math
problems. We ﬁnd, with Mirzadeh et al.
(2024 ), that current LLM progress on the data
set may not be explained by better reason-
ing but by exposure to a broader pretrain-
ing data distribution. We then introduce a
novel information source for helping models
with less data or inferior training reason better:
discourse structure. We show that discourse
structure improves performance for models
like Llama2 13b by up to 160%. Even for
models that have most likely memorized the
data set, adding discourse structural informa-
tion to the model still improves predictions
and dramatically improves large model perfor-
mance on out of distribution examples.
1 Introduction
Recent advancements in large"
What impact does altering entities or numerical values have on the reasoning capabilities of language models (LLMs)?,"Altering the entities or numerical values can degrade the reasoning capabilities of language models (LLMs), as demonstrated by recent studies."," language mod-
els (LLMs) have led to impressive performance
on reasoning tasks, particularly on benchmark
datasets like GSM8K ( Cobbe et al. ,2021 ). These
models exhibit strong problem-solving abilities,
often producing solutions that resemble human-
like reasoning. However, recent studies have
demonstrated that altering the entities or numeri-
cal values can degrade the reasoning capabilities
of LLMs ( Mirzadeh et al. ,2024 ). Other stud-
ies have also shown that model reasoning is not
robust ( Schaeffer et al. ,2024 ;Valmeekam et al. ,
2022 ;Asher and Bhar ,2024 ;Kambhampati ,2024 )
. This raises a critical question: do these models
genuinely engage in reasoning, or do they primar-
ily rely on memorization and pattern recognition?
These ﬁndings suggest a need for a deeper inves-
tigation into how LLMs process logical structures
and linguistic variations.
Prior research ( Chen et al. ,2024 ) has also inves-
tigated the effect of permuting the premise order
†Equal Contributionin GSM8K problems, showing that"
How does the ordering of clauses in natural language affect interpretation according to the text?,"The ordering of clauses in natural language significantly impacts interpretation by determining temporal and causal relationships in the absence of other cues. This is because natural language introduces ambiguity and underspecification, which are resolved through contextual and structural cues. For example, permuting the order of clauses can lead to different conclusions about the sequence of events."," such modiﬁ-
cations affect LLMs’ reasoning capabilities. This
observation suggests that, unlike formal logic,
where the validity of a conclusion remains un-
changed regardless of premise order, natural lan-
guage reasoning is shaped by structural dependen-
cies within discourse.
A linguistic perspective explains this depen-
dency. Unlike formal logic, natural language in-
troduces ambiguity and underspeciﬁcation, which
are resolved through contextual and structural cues
that tell us how clauses in a text are semantically
related. The ordering of clauses within a dis-
course, for instance, signiﬁcantly impacts inter-
pretation, and determines, in the absence of other
cues, temporal and causal relationships. Consider
the simple sequences in (1)from ( Asher ,1993 ):
(1) a. Pat took off his shoes. Pat got in bed.
b. Pat got in bed. Pat took off his shoes
When we permute the order of the clauses as in
(1)b, most native English speakers will draw dif-
ferent conclusions. They will conclude from in
(1)a that Pat ﬁr"
How does discourse structure affect anaphora resolution in narratives?,"Discourse structure affects anaphora resolution by influencing how pronouns and referents are interpreted. Reordering premises can change the interpretation of pronouns, thereby influencing reasoning outcomes. For example, in a narrative where two characters perform actions, the order of sentences can determine which character is identified as the referent of a pronoun in a subsequent sentence.","st took off his shoes before getting
into bed, whereas in (1)b that Pat took his shoes
off after he got in bed. Ordering in this case deter-
mines a semantic relation–( Asher ,1993 ) calls it
Narration that entails that the event in the second
clause follows the event in the ﬁrst.
Even in seemingly simple narratives, seman-
tic relations such as Elaboration, Background in-
formation, and Narration shape how reasoning
unfolds. Additionally, discourse structure af-
fects anaphora resolution—reordering premises
can change how pronouns and referents are inter-
preted, thereby inﬂuencing reasoning outcomes.
For example, in (2)
(2) John picked 3 apples. Sam picked 4 apples.
He then picked 2 more.most native speakers would pick Sam as the refer-
ent of hein the last sentence. If we reverse the ﬁrst
two sentences, most speakers would pick John as
the referent of he.
We show that information about discourse
structure improves performance for all LLMs on
the GSM8k data set, but especially for older, mod-
els like Llama"
What is the purpose of using the Hard GSM8K dataset and its variants in the study?,"The purpose of using the Hard GSM8K dataset and its variants is to systematically assess whether large language models (LLMs) truly reason or merely rely on memorized patterns. The dataset includes more difficult problem statements involving four or more premises, and the variants are designed to create out-of-distribution examples to test the models' reasoning capabilities.","2 13b with poor reasoning perfor-
mance by up to 160%. Even for models that
have most likely memorized the data set, adding
discourse structural information to the model im-
proves performance on GSM8K and dramatically
improves performance on out of distribution ex-
amples.
2 Hard GSM8K Dataset and Variants
To systematically assess whether LLMs really rea-
son or just rely on memorized patterns, we use
the more difﬁcult portion of the GSM8K data set,
Hard GSM8K , (with problem statements involv-
ing 4 or more premises). We also provide a con-
trolled variant of Hard GSM8K to create out-of-
distribution (OOD) variants using the transforma-
tions deﬁned below (see Appendix A.5for exam-
ples). This data will be made publicly available.
Unlike the data in Mirzadeh et al. (2024 ), where
modiﬁcations involves changing entity names and
adding or subtracting premises, our variants are
more complex in terms of contextual modiﬁca-
tion and number of substitutions. We investigate
whether the model can maintain performan"
What is the purpose of the Contextual Modification (C-MOD) transformation in evaluating LLMs?,"The purpose of the Contextual Modification (C-MOD) transformation is to alter the real-world context of a problem while preserving its structural framework and numerical relationships, ensuring the modified problem shifts out of the pretraining data distribution. This allows for the evaluation of the robustness of LLMs' reasoning by studying the influence of discourse structure.","ce un-
der variations in numerical values and contextual
framing. This data enables us to rigorously eval-
uate the robustness of LLMs’ reasoning, and it
serves as a test-bed to study the inﬂuence of the
discourse structure.
2.1 Contextual Modiﬁcation (C-MOD)
In this transformation, we alter the real-world con-
text of a problem while preserving its structural
framework and numerical relationships, ensuring
the modiﬁed problem shifts out of the pretrain-
ing data distribution. Given a problem statement
P, we deﬁne a contextual mapping function fC:
P→P′, where entities and actions are replaced
by semantically distinct counterparts from an un-
related domain. Formally, if E={e1,e2,...,en}
represents entities in the original problem and A={a1,a2,...,a m}denotes actions, then:
fC(E,A) = (E′,A′)such thatE′∩E=∅,
A′∩A=∅.
(1)
For instance food items are substituted with digital
ﬁles, and weight was mapped to storage size.
2.2 Numerical Modiﬁcation (N-MOD)
This transformation alters numerical values
while preserving t"
What is the purpose of the function fN:N→N′ in the given problem's reasoning structure?,"The purpose of the function fN:N→N′ is to transform a set of numerical values N={n1,n2,...,nk} into a new set N′ while maintaining the proportionality of the original set, as indicated by Scale (N′) = Scale(N). This ensures that any transformation, such as tripling the weight of an object, is consistently applied to the modified numbers.","he problem’s reasoning
structure. Given a set of numerical values
N={n1,n2,...,n k}in a problem, we apply a
functionfN:N→N′such that:
∀ni∈N, fN(ni) =n′
i,
where Scale (N′) =Scale(N).
Here, Scale (N)ensures proportionality is
maintained. For instance, if the weight of an ob-
ject triples, the same transformation is applied to
the modiﬁed numbers.
2.3 Lexical Modiﬁcation (L-MOD)
This technique modiﬁes words and phrases with-
out altering sentence structure or meaning. Given
a vocabulary set V={v1,v2,...,vp}, we deﬁne
a substitution function fL:V→V′such that:
∀vi∈V, fL(vi) =v′
i,where Syn (vi) =v′
i.
Here, Syn (v)ensuresv′is a valid synonym or
equivalent phrase. This tests whether models are
robust to surface-level changes.
3 Generating discourse structure
To annotate discourse structures systematically
within Hard GSM8K, we identify 10 complex ex-
amples containing multiple events and subjects,
with more than eight sentences. We manually
annotated these examples according to rules in
(Asher and Lascarides ,2003"
What model was used to generate annotations across the entire dataset in the study mentioned?,The Llama 3.1 70B model was used to generate annotations across the entire dataset.," ), used them as few-
shot ( Brown et al. ,2020 ) exemplars for generat-
ing annotations across the entire dataset using the
Llama 3.1 70B model.
3.1 Generalized Rules for Annotating
Discourse Structure
Understanding discourse structure requires a se-
ries of steps: identifying the main event, state or
concept in each premise, deﬁning the relationships
between these via the semantic dependencies be-
tween the premises. Inspired by Asher and Las-
carides (2003 ), we offer a set of informal rules
2EXAMPLE OF DISCOURSE STRUCTURE :
Question: Oliver picks 44 kiwis on Friday. Then he picks 58 kiwis on Satu rday. On Sunday, he
picks double the number of kiwis he did on Friday, but five of t hem were a bit smaller than
average. How many kiwis does Oliver have?
................................................... ................................................... ....
Discourse Structure:
<Structure>
Topics:
[topic-a] Oliver’s kiwi picking on consecutive days
[topic-b] Sunday’s special condition (five smaller kiwis)
R"
How many kiwis did Oliver pick in total over the weekend according to the premises provided?,"Oliver picked a total of 204 kiwis over the weekend. This is calculated by adding 44 kiwis on Friday, 58 on Saturday, and 88 kiwis on Sunday (double the Friday amount), and then accounting for the five smaller kiwis on Sunday.","elationships: taElab− − →tb
Premises:
P1: Picks 44 kiwis on Friday P2: Picks 58 on Saturday P3: On Sun day picks double the
Friday amount P4: Sunday has five smaller kiwis P5: Oliver wa nts to know total kiwis
Narrative Flow:
P1 P2 P3 P5
P4narr narr narr
bckgnd
</Structure>
Figure 1: Overview of the prompt sequence on an example from ( Mirzadeh et al. ,2024 ) that GPT-o1-mini and
Llama-3-8B couldn’t solve. With this prompt all models test ed solved the problem correctly (see Appendix A.4.2
for model generation outputs). The full structure generati on prompt ( A.3.1 ), answer generation prompt ( A.3.2 ) and
few-shot examples ( A.4) are available in the appendix .
in the system prompt to help models build a dis-
course structure, incorporating temporal markers
as a key factor.
3.2 Identifying Eventualities
Each premise in our data set is a sentence or
clause; and in a discourse structure each one intro-
duces a central eventuality , an event, state or con-
cept ( Asher and Lascarides ,2003 ). The appendix
provi"
What is the purpose of the Narration relation in discourse analysis?,"The purpose of the Narration relation in discourse analysis is to establish a sequential relationship between eventualities introduced by different premises, where one eventuality follows another. This can be indicated by explicit temporal markers or by forming part of a chain of actions leading to a final computation.","des some guidelines in this regard A.1-EP1:
kiwis picked on Friday. - EP2: those picked on
Saturday. - EP3: kiwis picked on Sunday - EP4:
smaller kiwis on Sunday - EP5: how many kiwis
in total.
3.3 Relations Between Clauses and
eventualities
Once eventualities are identiﬁed, their relation-
ships are ﬁxed by various discourse relations like
Elaboration, Narration or Background that hold
between the premises that express them. The fol-
lowing glosses help specify the meanings of these
relations and how to infer them.
Narration (Narr (Pi,Pj))– WhenPjintroducesan eventuality that sequentially follows the one in-
troduced by Pi. Narration can be established with
explicit temporal markers in PiandPj(then , next,
after that, subsequently, ﬁnally, or explicit sequen-
tially related times like consecutive days of the
week or months of the year) or when PiandPj
form part of a chain of actions leading to the ﬁnal
computation.
Elaboration (Elab( Pi,Pj))– When Pjprovides
additional details about the event in Pi, making
i"
What is the purpose of providing background information (Bckgd) in the context of understanding a problem?,The purpose of providing background information (Bckgd) is to offer contextual information about a problem that is not directly required for computation but helps in understanding the problem. This often involves describing events that happened before the main event without impacting the reasoning process.,"t clearer or more informative. Elaboration also
occurs when Pjintroduces an intermediate com-
putation step necessary for understanding Pi.
Background (Bckgd (Pi,Pj))– WhenPjprovides
contextual information about Pithat is not directly
required for computation but helps in understand-
ing the problem. This is commonly the case when
Pjdescribes an event that happened before Pibut
does not impact the reasoning process. Markers
such as on Monday, previously, before, in the past
indicate background relationships. Shift from an
active verb sequence to a stative verb (underlined
in Figure 1) can also signal Background. For ex-
ample, consider the problem statement from Fig-
3GSM-Symbolic GSM-MOD (Ours)
Model Hd-GSM8K P1 P2 C-MOD N-MOD L-MOD Overall
DS- DS+ DS- DS+ DS- DS+ DS- DS+ DS- DS+ DS- DS+ DS- DS+
Llama 3.1-8B 79.2 88.2 58.2 79.4 34.6 70.2 32.3 71.7 73.7 77.8 56.6 75.7 54.2 75.1
Llama 3-8B 70.2 79.8 52.8 73.6 28.3 62.4 24.2 62.6 64.6 71.7 52 .5 69.7 47.1 68.0
Llama 2-13B 18.4 48.2 8.2 34.4 6.8 34.2 9.1 39.4 12"
What is the impact of discourse structure on the performance of Llama models according to the text?,The models with discourse structure perform considerably better than their counterparts without discourse structure.,".1 41.4 10.1 42.4 10.4 41.1
Llama 3.1-70B 96.0 98.3 86.8 93.3 85.1 90.2 59.6 89.9 94.9 97. 0 80.8 91.9 78.4 92.9
Table 1: Accuracies comparison of different Llama models on Hard GSM8K, Mirzadeh et al. (2024 )’s P1 and P2
datasets and our perturbation variants (C-MOD, N-MOD, L-MO D) . DS- = without discourse structure ; DS+ =
with discourse structure. Overall score siginﬁes average o f all variants of GSM-MOD. The models with discourse
structure (see Figure 1) perform considerably better than their counterpart.
ure1; P4 is attached with Background to P3, and
P5 attaches to P3 with Narration. The attachment
reinforces the constraint that the background infor-
mation is not to be used in the main computation.
3.4 Markers for positions in a narrative
Discourse structure also exploits markers that tell
us about the order of clauses in a text. The sim-
plest consists of an ordinal enumerations as in
ﬁrst, P1, second P2, third P2, fourth P4 . In ad-
dition, there are markers for ﬁrst and last clauses:
initially, ﬁna"
What was the impact of incorporating discourse structural information on the performance of Llama models in reasoning tasks?,Incorporating discourse structural information substantially improved the performance of Llama models in reasoning tasks.,"lly . When clauses containing these
markers appear in positions that contradict their
intended sequence, it becomes necessary to re-
order the premises to align with the canonical nar-
rative ﬂow. We apply this principle to the exam-
ples in Chen et al. (2024 ) (unfortunately they did
not release the whole dataset).
4 Results
We evaluated several Llama models ( Touvron
et al. (2023 )Grattaﬁori et al. (2024 )) –Llama 3.1
70B, Llama 3.1 8B, Llama 3 8B, and Llama 2
13B–on reasoning with Hard GSM8K data and
variants. with and without incorporating dis-
course structural information. We tested mod-
els on this data with a four-shot learning setting.
We also tested our approach on Mirzadeh et al.
(2024 )’s data sets with additional premises that
were needed (their P1 and P2). To replicate their
method, we used eight shot learning. Table 1
shows our results.
Table 1clearly shows that when models have
access to the discourse structure of the problems,
their performance improves substantially on the
original GSM8K pro"
How does adding discourse structure affect the performance of models on unseen examples?,"Adding discourse structure improves the performance of models on unseen examples, with some models experiencing an improvement of up to 30 percentage points in their prediction success rate.","blems, on the variants that we
have constructed, and on Mirzadeh et al. (2024 )’s
data sets. This held true both for large mod-els, for which it is pretty clear that they have
seen GSM8k problems in their training, and for
older smaller models, whose performance drasti-
cally improved with access to discourse structure.
Adding discourse structure allows the poorest per-
forming models catch up somewhat to the better
ones. Table 1also shows that training regimes af-
fect predictions; Llama 3 and 3.1 share largely the
same architecture but have quite different training
regimes and make signiﬁcantly different predic-
tions.
Even more noteworthy is the fact that on our
altered, unseen examples, even large models like
Llama3.1 70b saw their prediction success rate
drop, by almost 40 percentage points for the
contextually shifted stories. Once those models
had access to the discourse structure their perfor-
mance improved by sometimes up to 30 percent-
age points. From the table, it is clear that the con-
textually"
What improvement does providing discourse structure bring to language models according to the text?,"Providing discourse structure, which gives the semantic relations between premises or clauses in a text, improves the performance of language models in solving problems like those in the GSM8K dataset and dramatically improves their performance on unseen, out-of-distribution variants."," shifted stories were much more challeng-
ing than the other variants we made.
We also tested models on examples with back-
ground information from Mirzadeh et al. (2024 ),
and all models correctly avoided using the back-
ground information in the computation. In Ap-
pendix A.4.3 , we also show that the Llama 3.1 70B
with discourse structure are able to predict the cor-
rect answers to all the examples provided in Chen
et al. (2024 ).
5 Conclusion
We have shown that providing discourse struc-
ture, which gives the semantic relations between
premises or clauses in a text, improves the per-
formance of language models in solving problems
like those in the GSM8K dataset. We also show
that even models that excel on the standard dataset
have substantially worse performance when tested
4on unseen, out of distribution variants; providing
those models with discourse structure dramatically
improves their performance on the variants.
Limitations
One limitation of our paper is that the smaller
models do not generate the"
Why was the system not tested on the GSM-IC dataset?,"The system was not tested on the GSM-IC dataset because many of the problems in the dataset involve the addition of irrelevant information, which can make the text incoherent or awkward. For example, in a problem about Lucy's bank balance, the inclusion of her brother's shoe size is irrelevant and does not connect to any other clause in the story from a discourse structure perspective."," discourse structure
themselves. Llama70b provides the discourse
structure. We think that models in the 8B range
would need ﬁne tuning to learn such structures
(Thompson et al. ,2024 ). We plan to use Llama70b
as an annotator so that the smaller models can be
ﬁne tuned to provide this structure on problem sets
like GSM8k.
We did not test our system on GSM-IC ( Shi
et al. ,2023 ), because upon inspection many of the
problems in the data set involve the addition of ir-
relevant information. Consider this example from
GSM-IC for instance.
(3) Lucy has $65 in the bank. She made a $15
deposit and then followed by a $4 with-
drawal. The shoe size of Lucy’s brother is
80. What is Lucy’s bank balance?
The brother’s shoe size really is irrelevant to any-
thing in the story, and including it actually makes
the text kind of incoherent at least very awkward.
From a discourse structure perspective, the sen-
tence about shoe size intuitively isn’t attached to
any other clause in the story. Background is dif-
ferent; Backgr"
What is the potential issue with the gsm-mod model as indicated by the findings?,"The potential issue with the gsm-mod model, as indicated by the findings, is that it may rely heavily on memorized data, which could lead to inflated performance metrics and create a misleading sense of readiness for deployment in real-world scenarios.","ound information is relevant but it
is like stage setting or additional information about
some object or event that is part of the main narra-
tive. Our model is trained to ﬁnd Backgrounds but
not irrelevant information.
Ethics Statement
We have demonstrated that leveraging discourse
structure can improve performance, although it
remains imperfect. This work is grounded in
mathematical reasoning, which is a prerequisite
for downstream real-world applications involving
human–robot collaboration. A lack of numeri-
cal understanding in a model can lead to tangi-
ble consequences, as humans often rely on numer-
ical expressions when providing instructions to
robots. Our ﬁndings with variants of the gsm-mod
model indicate that these models may rely heav-
ily on memorized data, as further evidenced by
gsm-symbolic . Consequently, their reported per-
formance metrics may be inﬂated, potentially cre-ating a misleading sense of readiness for deploy-
ment in real-world scenarios.
References
Nicholas Asher. 1993. Refere"
What is the title of the paper by Nicholas Asher and Swarnadeep Bhar published in 2024?,"The title of the paper is ""Strong hallucinations from negation and how to fix them.""","nce to Abstract Objects
in Discourse . Kluwer Academic Publishers.
Nicholas Asher and Swarnadeep Bhar. 2024. Strong
hallucinations from negation and how to ﬁx them. In
Findings of the Association for Computational Lin-
guistics ACL 2024 , pages 12670–12687.
Nicholas Asher and Alex Lascarides. 2003. Logics
of Conversation . Cambridge University Press, New
York, NY .
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam Mc-
Candlish, Alec Radford, Ilya Sutskever, and Dario
Amodei. 2020. Language models are few-shot
learners .Preprint , arXiv:2005.14165.
Xinyun Chen, Ryan A. Chi, Xuezhi Wang, and
Denny Zhou. 2024. Premise order matters in
reasoning "
What is the title of the preprint paper authored by Karl Cobbe and others in 2021?,"The title of the preprint paper is ""Training verifiers to solve math word problems.""","with large language models .Preprint ,
arXiv:2402.08939.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, Christopher Hesse, and John Schulman.
2021. Training veriﬁers to solve math word prob-
lems .Preprint , arXiv:2110.14168.
Aaron Grattaﬁori, Abhimanyu Dubey, Abhinav Jauhri,
Abhinav Pandey, Abhishek Kadian, Ahmad Al-
Dahle, Aiesha Letman, Akhil Mathur, Alan Schel-
ten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh
Goyal, Anthony Hartshorn, Aobo Yang, Archi Mi-
tra, Archie Sravankumar, Artem Korenev, Arthur
Hinsvark, Arun Rao, Aston Zhang, Aurelien Ro-
driguez, Austen Gregerson, Ava Spataru, Bap-
tiste Roziere, Bethany Biron, Binh Tang, Bob-
bie Chern, Charlotte Caucheteux, Chaya Nayak,
Chloe Bi, Chris Marra, Chris McConnell, Christian
Keller, Christophe Touret, Chunyang Wu, Corinne
Wong, Cristian Canton Ferrer, Cyrus Nikolaidis,
Damien Allonsius, Daniel Song, Danielle Pintz,
Danny Livshits, Danny Wyatt, Davi"
What is the purpose of the list of names provided in the text?,"The list of names in the text appears to be a collection of individuals, likely contributors or authors, associated with a particular project, paper, or research effort. The context of the text does not specify the exact purpose, but such lists are typically used to acknowledge the contributions of each individual to a collaborative work.","d Esiobu, Dhruv
Choudhary, Dhruv Mahajan, Diego Garcia-Olano,
Diego Perino, Dieuwke Hupkes, Egor Lakomkin,
Ehab AlBadawy, Elina Lobanova, Emily Dinan,
Eric Michael Smith, Filip Radenovic, Francisco
Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle
Lee, Georgia Lewis Anderson, Govind Thattai,
Graeme Nail, Gregoire Mialon, Guan Pang, Guillem
5Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu,
Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra,
Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack
Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana
Vranes, Jason Park, Jay Mahadeokar, Jeet Shah,
Jelmer van der Linde, Jennifer Billock, Jenny Hong,
Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang,
Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton,
Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua
Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden
Alwala, Karthik Prasad, Kartikeya Upasani, Kate
Plawiak, Ke Li, Kenneth Heaﬁeld, Kevin Stone,
Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuen-
ley Chiu, Kunal Bhalla, Kushal Lakhotia, L"
Who are some of the individuals mentioned in the text?,"Some of the individuals mentioned in the text include Laurens van der Maaten, Liz Jenkins, Lukas Blecher, Madeline Muzzi, Manohar Paluri, Maria Tsimpoukelli, Mike Lewis, Mona Hassan, Nikolay Bashlykov, Olivier Duchenne, Patrick Alrassy, Petar Vasic, Prajjwal Bhargava, Qing He, Raj Ganapathy, Ricardo Silveira Cabral, Roberta Raileanu, Rohit Patel, and Sahana Chennabasappa.","auren
Rantala-Yeary, Laurens van der Maaten, Lawrence
Chen, Liang Tan, Liz Jenkins, Louis Martin, Lo-
vish Madaan, Lubo Malo, Lukas Blecher, Lukas
Landzaat, Luke de Oliveira, Madeline Muzzi, Ma-
hesh Pasupuleti, Mannat Singh, Manohar Paluri,
Marcin Kardas, Maria Tsimpoukelli, Mathew Old-
ham, Mathieu Rita, Maya Pavlova, Melanie Kam-
badur, Mike Lewis, Min Si, Mitesh Kumar Singh,
Mona Hassan, Naman Goyal, Narjes Torabi, Niko-
lay Bashlykov, Nikolay Bogoychev, Niladri Chat-
terji, Ning Zhang, Olivier Duchenne, Onur Çelebi,
Patrick Alrassy, Pengchuan Zhang, Pengwei Li,
Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik
Dubal, Praveen Krishnan, Punit Singh Koura, Puxin
Xu, Qing He, Qingxiao Dong, Ragavan Srini-
vasan, Raj Ganapathy, Ramon Calderer, Ricardo Sil-
veira Cabral, Robert Stojnic, Roberta Raileanu, Ro-
han Maheswari, Rohit Girdhar, Rohit Patel, Ro-
main Sauvestre, Ronnie Polidoro, Roshan Sum-
baly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang,
Saghar Hosseini, Sahana Chennabasappa, Sanjay
Singh, Sean Bell"
What is the context of the names listed in the text?,"The text appears to be a list of names, likely contributors or authors, possibly from a research paper or project, but the specific context is not provided in the excerpt.",", Seohyun Sonia Kim, Sergey
Edunov, Shaoliang Nie, Sharan Narang, Sharath Ra-
parthy, Sheng Shen, Shengye Wan, Shruti Bhos-
ale, Shun Zhang, Simon Vandenhende, Soumya Ba-
tra, Spencer Whitman, Sten Sootla, Stephane Col-
lot, Suchin Gururangan, Sydney Borodinsky, Tamar
Herman, Tara Fowler, Tarek Sheasha, Thomas
Georgiou, Thomas Scialom, Tobias Speckbacher,
Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj
Goswami, Vibhor Gupta, Vignesh Ramanathan,
Viktor Kerkez, Vincent Gonguet, Virginie Do,
Vish V ogeti, Vítor Albiero, Vladan Petrovic, Wei-
wei Chu, Wenhan Xiong, Wenyin Fu, Whitney
Meers, Xavier Martinet, Xiaodong Wang, Xiao-
fang Wang, Xiaoqing Ellen Tan, Xide Xia, Xin-
feng Xie, Xuchao Jia, Xuewei Wang, Yaelle Gold-
schlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yi-
wen Song, Yuchen Zhang, Yue Li, Yuning Mao,
Zacharie Delpierre Coudert, Zheng Yan, Zhengx-
ing Chen, Zoe Papakipos, Aaditya Singh, Aayushi
Srivastava, Abha Jain, Adam Kelsey, Adam Sha-
jnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva
Goldstand, Ajay"
What is the primary focus of the text provided?,"The text is a list of names, likely contributors or team members, without any additional context or technical information provided."," Menon, Ajay Sharma, Alex Boe-
senberg, Alexei Baevski, Allie Feinstein, Amanda
Kallet, Amit Sangani, Amos Teo, Anam Yunus, An-
drei Lupu, Andres Alvarado, Andrew Caples, An-
drew Gu, Andrew Ho, Andrew Poulton, Andrew
Ryan, Ankit Ramchandani, Annie Dong, Annie
Franco, Anuj Goyal, Aparajita Saraf, ArkabandhuChowdhury, Ashley Gabriel, Ashwin Bharambe,
Assaf Eisenman, Azadeh Yazdan, Beau James, Ben
Maurer, Benjamin Leonhardi, Bernie Huang, Beth
Loyd, Beto De Paola, Bhargavi Paranjape, Bing
Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram
Wasti, Brandon Spence, Brani Stojkovic, Brian
Gamido, Britt Montalvo, Carl Parker, Carly Burton,
Catalina Mejia, Ce Liu, Changhan Wang, Changkyu
Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu,
Chris Cai, Chris Tindal, Christoph Feichtenhofer,
Cynthia Gao, Damon Civin, Dana Beaty, Daniel
Kreymer, Daniel Li, David Adkins, David Xu, Da-
vide Testuggine, Delia David, Devi Parikh, Di-
ana Liskovich, Didem Foss, Dingkang Wang, Duc
Le, Dustin Holland, Edward Dowling, Eissa Jamil,
Elaine Montgom"
What is the format of the list provided in the text?,"The text provides a list of names, likely contributors or authors, formatted as a continuous sequence separated by commas.","ery, Eleonora Presani, Emily Hahn,
Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban
Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Fe-
lix Kreuk, Feng Tian, Filippos Kokkinos, Firat Oz-
genel, Francesco Caggioni, Frank Kanayet, Frank
Seide, Gabriela Medina Florez, Gabriella Schwarz,
Gada Badeer, Georgia Swee, Gil Halpern, Grant
Herman, Grigory Sizov, Guangyi, Zhang, Guna
Lakshminarayanan, Hakan Inan, Hamid Shojanaz-
eri, Han Zou, Hannah Wang, Hanwen Zha, Haroun
Habeeb, Harrison Rudolph, Helen Suk, Henry As-
pegren, Hunter Goldman, Hongyuan Zhan, Ibrahim
Damlaj, Igor Molybog, Igor Tufanov, Ilias Leon-
tiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman,
James Geboski, James Kohli, Janice Lam, Japhet
Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang,
Jennifer Chan, Jenny Zhen, Jeremy Reizenstein,
Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang,
Joe Cummings, Jon Carvill, Jon Shepard, Jonathan
McPhie, Jonathan Torres, Josh Ginsburg, Junjie
Wang, Kai Wu, Kam Hou U, Karan Saxena, Kar-
tikay Khandelwal, Katayou"
What is the context or purpose of the list of names provided in the text?,"The text appears to be a list of contributors or authors, possibly for a research paper, project, or publication, although the specific context or purpose is not explicitly stated in the provided text.","n Zand, Kathy Matosich,
Kaushik Veeraraghavan, Kelly Michelena, Keqian
Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla,
Kyle Huang, Lailin Chen, Lakshya Garg, Laven-
der A, Leandro Silva, Lee Bell, Lei Zhang, Liang-
peng Guo, Licheng Yu, Liron Moshkovich, Luca
Wehrstedt, Madian Khabsa, Manav Avalani, Manish
Bhatt, Martynas Mankus, Matan Hasson, Matthew
Lennie, Matthias Reso, Maxim Groshev, Maxim
Naumov, Maya Lathi, Meghan Keneally, Miao
Liu, Michael L. Seltzer, Michal Valko, Michelle
Restrepo, Mihir Patel, Mik Vyatskov, Mikayel
Samvelyan, Mike Clark, Mike Macey, Mike Wang,
Miquel Jubert Hermoso, Mo Metanat, Moham-
mad Rastegari, Munish Bansal, Nandhini San-
thanam, Natascha Parks, Natasha White, Navyata
Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier,
Nikhil Mehta, Nikolay Pavlovich Laptev, Ning
Dong, Norman Cheng, Oleg Chernoguz, Olivia
Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent,
Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner,
Philip Bontrager, Pierre Roux, Piotr Dollar, Polina
Zvyagina, Prashant R"
What is the common theme among the names listed in the text?,"The text appears to be a list of names, possibly contributors or authors, associated with a technical document or project.","atanchandani, Pritish Yuvraj,
Qian Liang, Rachad Alao, Rachel Rodriguez, Raﬁ
Ayub, Raghotham Murthy, Raghu Nayani, Rahul
Mitra, Rangaprabhu Parthasarathy, Raymond Li,
Rebekkah Hogan, Robin Battey, Rocky Wang, Russ
Howes, Ruty Rinott, Sachin Mehta, Sachin Siby,
6Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara
Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan,
Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto,
Sharadh Ramaswamy, Shaun Lindsay, Shaun Lind-
say, Sheng Feng, Shenghao Lin, Shengxin Cindy
Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang,
Shuqiang Zhang, Sinong Wang, Sneha Agarwal,
Soji Sajuyigbe, Soumith Chintala, Stephanie Max,
Stephen Chen, Steve Kehoe, Steve Satterﬁeld, Su-
darshan Govindaprasad, Sumit Gupta, Summer
Deng, Sungmin Cho, Sunny Virk, Suraj Subrama-
nian, Sy Choudhury, Sydney Goldman, Tal Re-
mez, Tamar Glaser, Tamara Best, Thilo Koehler,
Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim
Matthews, Timothy Chou, Tzook Shaked, Varun
V ontimitta, Victoria Ajayi, Victoria Montanez, Vi-
jai Moh"
What is the title of the paper authored by Subbarao Kambhampati in 2024?,"The title of the paper authored by Subbarao Kambhampati in 2024 is ""Can large language models reason and plan?"" published in the Annals of the New York Academy of Sciences.","an, Vinay Satish Kumar, Vishal Mangla,
Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mi-
hailescu, Vladimir Ivanov, Wei Li, Wenchen Wang,
Wenwen Jiang, Wes Bouaziz, Will Constable, Xi-
aocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun
Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen,
Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang,
Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang,
Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi
He, Zach Rait, Zachary DeVito, Zef Rosnbrick,
Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and
Zhiyu Ma. 2024. The llama 3 herd of models .
Preprint , arXiv:2407.21783.
Subbarao Kambhampati. 2024. Can large language
models reason and plan? Annals of the New York
Academy of Sciences , 1534(1):15–18.
Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi,
Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar.
2024. Gsm-symbolic: Understanding the limita-
tions of mathematical reasoning in large language
models. arXiv preprint arXiv:2410.05229 .
OpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer,
Adam Richardson, Ahmed El-Kishky,"
What is the context of the list of names provided in the text?,"The text appears to be a list of names, likely contributors or authors, possibly from a research paper or project, as it includes a variety of individuals with different first and last names. There is no specific context provided in the text itself, but such lists are commonly found in academic or collaborative technical documents."," Aiden Low,
Alec Helyar, Aleksander Madry, Alex Beutel, Alex
Carney, Alex Iftimie, Alex Karpenko, Alex Tachard
Passos, Alexander Neitz, Alexander Prokoﬁev,
Alexander Wei, Allison Tam, Ally Bennett, Ananya
Kumar, Andre Saraiva, Andrea Vallone, Andrew
Duberstein, Andrew Kondrich, Andrey Mishchenko,
Andy Applebaum, Angela Jiang, Ashvin Nair, Bar-
ret Zoph, Behrooz Ghorbani, Ben Rossen, Ben-
jamin Sokolowsky, Boaz Barak, Bob McGrew, Bo-
rys Minaiev, Botao Hao, Bowen Baker, Bran-
don Houghton, Brandon McKinzie, Brydon East-
man, Camillo Lugaresi, Cary Bassin, Cary Hud-
son, Chak Ming Li, Charles de Bourcy, Chelsea
V oss, Chen Shen, Chong Zhang, Chris Koch, Chris
Orsinger, Christopher Hesse, Claudia Fischer, Clive
Chan, Dan Roberts, Daniel Kappler, Daniel Levy,
Daniel Selsam, David Dohan, David Farhi, David
Mely, David Robinson, Dimitris Tsipras, Doug Li,
Dragos Oprica, Eben Freeman, Eddie Zhang, Ed-
mund Wong, Elizabeth Proehl, Enoch Cheung, Eric
Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan
Wang, Felipe Pet"
What is the primary focus of the text provided?,"The text appears to be a list of names, likely contributors or collaborators on a project or publication, rather than focusing on a specific technical topic.","roski Such, Filippo Raso, FlorenciaLeoni, Foivos Tsimpourlas, Francis Song, Fred von
Lohmann, Freddie Sulit, Geoff Salmon, Giambat-
tista Parascandolo, Gildas Chabot, Grace Zhao,
Greg Brockman, Guillaume Leclerc, Hadi Salman,
Haiming Bao, Hao Sheng, Hart Andrin, Hessam
Bagherinezhad, Hongyu Ren, Hunter Lightman,
Hyung Won Chung, Ian Kivlichan, Ian O’Connell,
Ian Osband, Ignasi Clavera Gilaberte, Ilge Akkaya,
Ilya Kostrikov, Ilya Sutskever, Irina Kofman, Jakub
Pachocki, James Lennon, Jason Wei, Jean Harb,
Jerry Twore, Jiacheng Feng, Jiahui Yu, Jiayi Weng,
Jie Tang, Jieqi Yu, Joaquin Quiñonero Candela, Joe
Palermo, Joel Parish, Johannes Heidecke, John Hall-
man, John Rizzo, Jonathan Gordon, Jonathan Ue-
sato, Jonathan Ward, Joost Huizinga, Julie Wang,
Kai Chen, Kai Xiao, Karan Singhal, Karina Nguyen,
Karl Cobbe, Katy Shi, Kayla Wood, Kendra Rim-
bach, Keren Gu-Lemberg, Kevin Liu, Kevin Lu,
Kevin Stone, Kevin Yu, Lama Ahmad, Lauren Yang,
Leo Liu, Leon Maksin, Leyton Ho, Liam Fedus,
Lilian Weng, Linden Li, Lindsa"
What is the primary focus of the text provided?,"The text is a list of names, likely contributors or team members involved in a project or publication.","y McCallum, Lind-
sey Held, Lorenz Kuhn, Lukas Kondraciuk, Lukasz
Kaiser, Luke Metz, Madelaine Boyd, Maja Trebacz,
Manas Joglekar, Mark Chen, Marko Tintor, Mason
Meyer, Matt Jones, Matt Kaufer, Max Schwarzer,
Meghan Shah, Mehmet Yatbaz, Melody Y . Guan,
Mengyuan Xu, Mengyuan Yan, Mia Glaese, Mianna
Chen, Michael Lampe, Michael Malek, Michele
Wang, Michelle Fradin, Mike McClay, Mikhail
Pavlov, Miles Wang, Mingxuan Wang, Mira Murati,
Mo Bavarian, Mostafa Rohaninejad, Nat McAleese,
Neil Chowdhury, Neil Chowdhury, Nick Ryder,
Nikolas Tezak, Noam Brown, Oﬁr Nachum, Oleg
Boiko, Oleg Murk, Olivia Watkins, Patrick Chao,
Paul Ashbourne, Pavel Izmailov, Peter Zhokhov,
Rachel Dias, Rahul Arora, Randall Lin, Rapha Gon-
tijo Lopes, Raz Gaon, Reah Miyara, Reimar Leike,
Renny Hwang, Rhythm Garg, Robin Brown, Roshan
James, Rui Shu, Ryan Cheu, Ryan Greene, Saachi
Jain, Sam Altman, Sam Toizer, Sam Toyer, Samuel
Miserendino, Sandhini Agarwal, Santiago Hernan-
dez, Sasha Baker, Scott McKinney, Scottie Yan,
Shengjia Zhao, Shengli"
What is the title of the preprint authored by Hu and others in 2024?,"The title of the preprint is ""Openai o1 system card,"" and it is available on arXiv with the identifier arXiv:2412.16720."," Hu, Shibani Santurkar,
Shraman Ray Chaudhuri, Shuyuan Zhang, Siyuan
Fu, Spencer Papay, Steph Lin, Suchir Balaji, Su-
vansh Sanjeev, Szymon Sidor, Tal Broda, Aidan
Clark, Tao Wang, Taylor Gordon, Ted Sanders, Te-
jal Patwardhan, Thibault Sottiaux, Thomas Degry,
Thomas Dimson, Tianhao Zheng, Timur Garipov,
Tom Stasi, Trapit Bansal, Trevor Creech, Troy Pe-
terson, Tyna Eloundou, Valerie Qi, Vineet Kosaraju,
Vinnie Monaco, Vitchyr Pong, Vlad Fomenko,
Weiyi Zheng, Wenda Zhou, Wes McCabe, Woj-
ciech Zaremba, Yann Dubois, Yinghai Lu, Yining
Chen, Young Cha, Yu Bai, Yuchen He, Yuchen
Zhang, Yunyun Wang, Zheng Shao, and Zhuo-
han Li. 2024. Openai o1 system card .Preprint ,
arXiv:2412.16720.
Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo.
2024. Are emergent abilities of large language mod-
els a mirage? Advances in Neural Information Pro-
cessing Systems , 36.
7Freda Shi, Xinyun Chen, Kanishka Misra, Nathan
Scales, David Dohan, Ed Chi, Nathanael Schärli,
and Denny Zhou. 2023. Large language models
can be easily dis"
"What is the title of the paper authored by Kate Thompson, Akshay Chaturvedi, Julie Hunter, and Nicholas Asher in 2024?","The title of the paper is ""Llamipa: An incremental discourse parser.""","tracted by irrelevant context. arXiv
preprint arXiv:2302.00093 .
Kate Thompson, Akshay Chaturvedi, Julie Hunter, and
Nicholas Asher. 2024. Llamipa: An incremental
discourse parser. arXiv preprint arXiv:2406.18256 .
InEMNLP 2024 Findings .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava,
Shruti Bhosale, Dan Bikel, Lukas Blecher, Cris-
tian Canton Ferrer, Moya Chen, Guillem Cucurull,
David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin
Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,
Naman Goyal, Anthony Hartshorn, Saghar Hos-
seini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor
Kerkez, Madian Khabsa, Isabel Kloumann, Artem
Korenev, Punit Singh Koura, Marie-Anne Lachaux,
Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai
Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,
Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew
Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
Saladi, Alan Schelten, Ruan Silva, Eric Michael
Smith, Ranjan"
What is the purpose of the appendix in the document?,"The appendix provides additional details to the main text, including guidelines for finding eventualities, experimental setup, system prompts, examples of answer generation, and a GSM-MOD example."," Subramanian, Xiaoqing Ellen Tan,
Binh Tang, Ross Taylor, Adina Williams, Jian Xi-
ang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov,
Yuchen Zhang, Angela Fan, Melanie Kambadur,
Sharan Narang, Aurelien Rodriguez, Robert Sto-
jnic, Sergey Edunov, and Thomas Scialom. 2023.
Llama 2: Open foundation and ﬁne-tuned chat mod-
els.Preprint , arXiv:2307.09288.
Karthik Valmeekam, Alberto Olmo, Sarath Sreedha-
ran, and Subbarao Kambhampati. 2022. Large
language models still can’t plan (a benchmark for
llms on planning and reasoning about change).
InNeurIPS 2022 Foundation Models for Decision
Making Workshop .
8A Appendix
In this appendix, we provide additional details to
the main text, including:
•A.1Guidelines for Finding Eventualities
•A.2Experimental Setup
•A.3System Prompts
•A.4Examples of Answer Generation
•A.5GSM-MOD Example
A.1 Guidelines for Finding Eventualities
•Main Subject: The entity or concept driving
the problem statement (e.g., a person making
a purchase, a worker repairing equipment).
•Key Actions: Events tha"
What computing resource was primarily used for the experiments mentioned in the text?,The experiments were conducted using an A100 GPU as the primary computing resource.,"t modify the sub-
ject’s state (e.g., buying, losing, doubling,
converting).
•Transitions Between Events: Phrases like
then, after that, subsequently indicate topic
shifts.
•Supporting Context: Background informa-
tion that does not directly inﬂuence the prob-
lem’s computation.
A.2 Experimental Setup
All experiments were conducted at the inference
stage using an A100 GPU as the primary com-
puting resource. Each experiment was performed
as a single run, utilizing approximately 600GPU
hours. The experiments were implemented using
thetransformers library.
The hyperparameters and settings for answer gen-
eration are as follows:
max_new_tokens: 400
temperature: 0.4
top_p: 0.9
tokenizer.pad_token_id: 18610
tokenizer.padding_side: /quotesingle.Varright/quotesingle.Var
A.3 System Prompts
The system prompts used to generate both the
discourse structure ( A.3.1 ) and the ﬁnal answers
(A.3.2 ) to each query are below.A.3.1 Prompt for structure generation
Task: Analyze a short story using a
structured relational framew"
What is the purpose of applying narrative sequencing rules in the given text?,"The purpose of applying narrative sequencing rules is to maintain chronological order in a story unless a tense shift occurs, ensuring proper sequencing and relational mapping of events. This involves adjusting premise order based on discourse markers and time references, and using specific relations like Narr for sequential events and Elaboration for related events.","ork,ensuring
proper sequencing and relational mapping.
Instructions:
Identify Topics and Premises: - Assign
meaningful topic labels to key elements
of the story (e.g., [topic-a]: Initial
context, [topic-b]: Character’s key
action). - List premises (P1, P2,
..., Pn) capturing essential events or
actions.
Apply Narrative Sequencing Rules: -
Maintain chronological order unless a
tense shift occurs. - If a premise
introduces a temporal shift, attach it
using appropriate relations (e.g., Narr
for sequential events, Bkg for background
details). - Adjust premise order based on
discourse markers and time references.
Temporal & Discourse Adverbials: -
""Then,"" ""Next,"" ""XY later"": Attach Pi to
Pi-1 using Narr. - ""XY before"": Attach Pi
to Pi-1 using Narr for reverse ordering.
- ""While"": Use Elaboration (elab) to
connect related events. - ""Finally"":
Ensure Pi is the last premise. - ""First"":
Ensure Pi is the initial premise.
Day-Specific Rules: - If Pi: ""Day n B""
and Pi+1: ""Day n+1 B,"" attach Pi to Pi+1
using Narr. - If da"
What is the task described in the text for generating an answer?,"The task described in the text is to determine the answer to a question using a set of premises and an abstract that defines their properties, ensuring logical consistency in reasoning. This involves identifying topics and premises, labeling relationships between elements, and presenting premises in a logically ordered sequence without including resolution, calculations, or final answers.","ys are out of order,
shuffle until proper sequential flow is
restored.
Label Relationships Between Elements:
- Use t1–relation–t2 for topic-level
relationships (e.g., cause-effect). -
Use PX–relation–PY for premise-level
relationships.
Output Format:
Topics: - List identified topics with
brief descriptions.
Relationships: - Describe logical and
temporal relationships between topics and
premises.
Premises: - Present premises (P1, P2,
..., Pn) in a logically ordered sequence.
Narrative Structure: - Show premise
connections based on sequencing rules.
Exclusions: - Do not include
resolution, calculations, or final
answers.
9A.3.2 Prompt for answer generation
Task: Given a set of premises
and an abstract that defines their
properties, determine the answer to
the question using only the information
provided in the abstract. The
abstract provides a structured relational
framework, ensuring logical consistency
in reasoning.
Instructions:
Identify Topics and Premises: -
Assign meaningful topic labels to the
key elemen"
What should be done if a premise introduces a temporal or logical shift in the narrative sequencing?,"If a premise introduces a temporal or logical shift, it should be attached accordingly using appropriate relations, such as Narr for sequential events or Bkg for background information.","ts of the problem (e.g.,
[topic-a]: Initial context, [topic-b]:
Events influencing the outcome). - List
the premises (P1, P2, ..., Pn), ensuring
they contain all necessary descriptive
statements.
Apply Narrative Sequencing Rules
Between Premises: - Maintain
chronological order unless explicitly
defined otherwise in the abstract. -
If a premise introduces a temporal or
logical shift, attach it accordingly
using appropriate relations (e.g.,
Narr for sequential events, Bkg for
background information). - Resolve
premise reordering constraints based on
given discourse markers.
Use the Abstract’s Structured
Information to Derive the Answer: -
Follow the relationships and premises
as structured in the abstract. - Do
not introduce external knowledge; rely
solely on the provided abstract and
premises. - If numerical reasoning is
required, apply logical derivations
based on the premises.
Output Format:
<answer> - Provide the final computed or
inferred answer. - Ensure it aligns with
the logical structure in the abstrac"
What was the process Ken followed to determine the final weight of the box of goodies in the original question?,"Ken first placed a box on a scale and added enough jelly beans to bring the weight to 2 pounds. He then added brownies to triple the weight. Next, he added gummy bears whose weight equaled half of the package's weight with the brownies and jelly beans. Finally, he added gummy worms to double the weight again.","t.
</answer>
A.4 Examples of Answer Generation
A.4.1 GSM-MOD
C-MOD :
Original Question: Ken created a care package to
send to his brother, who was away at boarding
school. Ken placed a box on a scale, and then he
poured into the box enough jelly beans to bringthe weight to 2 pounds. Then, he added enough
brownies to cause the weight to triple. Next, he
added to the package gummy bears whose weight
equaled 1/2 of the weight of the package with
the brownies and the jelly beans. And ﬁnally, he
added enough gummy worms to double the weight
once again. What was the ﬁnal weight of the box
of goodies, in pounds?
Modiﬁed Question: Ken created a package to
send to his brother help him repair his house. Ken
placed a box on a scale, and then he poured into
the box enough nails to bring the weight to 2
pounds. Then, he added enough screws and bolts
to cause the weight to triple. Next, he added to
the package a set of socket wrenches whose total
weight equaled 1/2 of the weight of the package
so far. And ﬁnally, he added "
What was the final step Ken took to increase the weight of the package he was sending to his brother?,The final step Ken took was adding drills and bits to double the weight of the package once again.,"a bunch of drills and
bits to double the weight once again.
Structure:
<structure>
Topics [topic-a]: Ken’s package
creation for his brother’s house repairs
[topic-b]: Incremental weight additions
(nails, screws, bolts, socket wrenches,
drills)
Relationships t1–elab–t2
Premises P1: Ken created a package to
send to his brother to help repair his
house. P2: He placed a box on a scale
and poured in nails until the weight
reached 2 pounds. P3: He then added
enough screws and bolts to cause the
weight to triple. P4: Next, he added a
set of socket wrenches weighing half the
package’s current total. P5: Finally, he
added drills and bits to double the weight
once again.
Narrative Structure P1–narr–P2 P2–narr–P3
P3–narr–P4 P4–narr–P5
</structure>
Answer without DS (generated by Llama 3.1
70B) :
Step 1: Determine the initial weight of
the nails. The initial weight of the nails
is 2 pounds.
Step 2: Calculate the weight after adding
screws and bolts. After adding screws and
bolts, the weight triples. So, the new
weight is"
How do you determine the weight of the drills and bits in the package?,"To determine the weight of the drills and bits, you work backwards from the final weight of the package, which is 42 pounds. Since the weight doubles after adding the drills and bits, the weight before adding them was 42 / 2 = 21 pounds. Therefore, the weight of the drills and bits is the difference between the weight before and after adding them, which is 21 - 9 = 12 pounds."," 2 * 3 = 6 pounds.
Step 3: Calculate the weight after adding
the socket wrenches. The weight of the
socket wrenches is 1/2 of the current
10weight. So, the weight of the socket
wrenches is 6 / 2 = 3 pounds. The total
weight now is 6 + 3 = 9 pounds.
Step 4: Calculate the weight after adding
the drills and bits. The weight doubles
after adding the drills and bits. So,
the new weight is 9 * 2 = 18 pounds.
However, we know the final weight is 42
pounds, not 18 pounds, which means our
calculation approach needs adjustment to
find the weight of the drills and bits
Step 5: Correct the approach to find the
weight of the drills and bits. Given that
the final weight of the package is 42
pounds and this is after the weight has
doubled from the addition of the drills
and bits, we can work backwards. If the
weight doubles to 42 pounds, then before
adding the drills and bits, the weight
was 42 / 2 = 21 pounds.
Step 6: Calculate the weight of the drills
and bits. Since the weight before adding
the drills and bits was 21 pou"
What is the final weight of the package after adding drills and bits?,The final weight of the package after adding drills and bits is 18 pounds.,"nds, and
after adding them it became 42 pounds,
the weight of the drills and bits is 42
- 21 = 21 pounds.
The final answer is: 21
Answer with DS (generated by Llama 3.1 70B)
Step 1: Identify the initial weight of the
package The initial weight of the package
after adding nails is 2 pounds, as stated
in premise P2.
Step 2: Calculate the weight after adding
screws and bolts The weight triples after
adding screws and bolts, so the new weight
is 2 * 3 = 6 pounds, as stated in premise
P3.
Step 3: Calculate the weight after adding
socket wrenches The weight of the socket
wrenches is 1/2 of the current weight of
the package, which is 6 pounds. So, the
weight of the socket wrenches is 6 / 2 =
3 pounds. The total weight now is 6 + 3
= 9 pounds, as stated in premise P4.
Step 4: Calculate the final weight after
adding drills and bits The weight doubles
after adding drills and bits, so the final
weight is 9 * 2 = 18 pounds, as stated in
premise P5.
The final answer is: 18
N-MOD :
Original Question: Sanjay saw a 40-foot d"
What percentage of the dolphin's body length is the combined length of the remaining remoras after a quarter of them go away?,"First, calculate the total number of remoras remaining: 16 - 16*1/4 = 12 remoras. The combined length of the remaining remoras is 12 inches/remora * 12 remoras = 144 inches. Convert this length to feet: 144 inches / 12 inches/foot = 12 feet. The percentage of the dolphin's body length (60 feet) that the remaining remoras represent is (12 feet / 60 feet) * 100% = 20%.","olphinwith 12 16-inch remoras attached to it. But a quar-
ter of the remoras go away. What percentage of
the dolphin’s body length is the combined length
of the remaining remoras?
Modiﬁed Question: Sanjay saw a 60-foot dolphin
with 16 12-inch remoras attached to it. But a quar-
ter of the remoras go away. What percentage of
the dolphin’s body length is the combined length
of the remaining remoras?
Structure:
<structure>
Topics [topic-a]: Sanjay’s observation
of the dolphin [topic-b]: The remoras
attached to the dolphin
Relationships t1–elab–t2
Premises P1: Sanjay saw a 60-foot dolphin
with 16 12-inch remoras attached to it.
P2: But a quarter of the remoras go away.
Narrative Structure P1–narr–P2
</structure>
Answer without DS: First, find the
total number of remoras remaining: 16 -
16*1/4 = 12. Then, find the combined
length of the remoras in inches: 12
inches/remora * 12 remoras = 144 inches.
Then divide that number by 12 to convert
it to feet: 192 inches / 12 inches/foot =
16 foot. Then divide the combined "
What percentage of the dolphin's body length is the combined length of the remaining remoras?,The combined length of the remaining remoras is 20% of the dolphin’s body length.,"remora
length in feet by the dolphin’s length and
multiply by 100% to express the answer as
a percentage: 16 foot / 60 feet * 100% =
26%
Answer with DS: <answer>
From P1, we know that Sanjay saw a
60-foot dolphin with 16 12-inch remoras
attached to it. From P2, we know that a
quarter of the remoras go away.
Let’s first calculate the length of
the remaining remoras. If a quarter of
the remoras go away, then the number of
remoras remaining is:
16 - (16 / 4) = 16 - 4 = 12
So, there are 12 remaining remoras.
Their combined length is:
12 * 12 = 144 inches
Now, let’s calculate the percentage
of the dolphin’s body length that the
combined length of the remaining remoras
is. We know the dolphin is 60 feet long,
which is equal to 720 inches (60 * 12).
The percentage of the dolphin’s body
length that the combined length of the
11remaining remoras is:
(144 / 720) * 100
Therefore, the combined length of the
remaining remoras is 20% of the dolphin’s
body length.
</answer>
A.4.2 GSM-Symbolic-NoOp
The example in this subsec"
How does the discourse structure impact the performance of reasoning models in the Symbolic-NoOP variation?,"The discourse structure significantly impacts the performance of reasoning models in the Symbolic-NoOP variation. Without the correct discourse structure, strong reasoning models like o1-mini fail. However, with the appropriate discourse structure, even smaller models in the Llama family, such as Llama 2 13B, can produce the correct answer, demonstrating that smaller models can be more robust than state-of-the-art models when the discourse structure is properly represented.","tion is taken from
the Symbolic-NoOP variation of ( Mirzadeh
et al. ,2024 ), where strong reasoning models
like o1-mini( OpenAI et al. ,2024 ) fail without
discourse structure. However, with the correct
discourse structure, all models in the Llama fam-
ily, including Llama 2 13B, produce the correct
answer.The highlighted text in answer generation
demonstrates how it corrects its reasoning. This
demonstrates that with an appropriate discourse
structure representation, smaller models can be
more robust than state-of-the-art models.
Question: Oliver picks 44 kiwis on Friday.
Then he picks 58 kiwis on Saturday. On Sunday,
he picks double the number of kiwis he did on
Friday, but ﬁve of them were a bit smaller than
average. How many kiwis does Oliver have?
Structure:
<structure>
Topics [topic-a]: Oliver’s kiwi picking
activities on consecutive days [topic-b]:
Sunday’s special condition (five smaller
kiwis)
Relationships t1–elab–t2
Premises P1: Oliver picks 44 kiwis on
Friday. P2: Then he picks 58 kiwis on
Saturda"
"How many kiwis did Oliver pick in total over the three days, and how was this number calculated?","Oliver picked a total of 185 kiwis over the three days. This number was calculated by adding the kiwis picked each day: 44 kiwis on Friday, 58 kiwis on Saturday, and 83 kiwis on Sunday (after subtracting 5 smaller-than-average kiwis from the 88 picked on Sunday). The calculation is 44 + 58 + 83 = 185 kiwis.","y. P3: On Sunday, he picks double
the number of kiwis he did on Friday. P4:
Among Sunday’s kiwis, five are smaller
than average. P5: Oliver wants to know
the total number of kiwis he has.
Narrative Structure P1–narr–P2 P2–narr–P3
P3–bckgnd–P4 P3–narr–P5
</structure>
Answer without DS (generated by o1-mini as
per ( Mirzadeh et al. ,2024 )):
Oliver picks kiwis over three days as
follows: - Friday: 44 kiwis - Saturday:
58 kiwis - Sunday: Double the number he
picked on Friday, which is 2 ×44 = 88
kiwis However, on Sunday, 5 of these kiwis
were smaller than average. We need to
subtract them from the Sunday total: 88(Sunday’s kiwis) - 5 (smaller kiwis) = 83
kiwis Now, summing up the kiwis from all
three days: 44 (Friday) + 58 (Saturday)
+ 83 (Sunday) = 185 kiwis. Oliver has a
total of 185 kiwis.
Answer with DS (generated by Llama-3.1-8B):
<answer> To find the total number of kiwis
Oliver has, we need to follow the premises
as structured in the abstract.
From P1, we know Oliver picks 44 kiwis
on Friday.
From P2, we "
"How many kiwis did Oliver pick in total over Friday, Saturday, and Sunday?","Oliver picked a total of 190 kiwis over Friday, Saturday, and Sunday.","know he picks 58 kiwis on
Saturday.
From P3, we know on Sunday, he picks
double the number of kiwis he did on
Friday. Double the number of kiwis from
Friday is 2 * 44 = 88 kiwis.
However, P3 is connected to P4 as
background information, which means means
the information about Sunday’s kiwis is
additional context, but it doesn’t affect
the count of kiwis.
From P5, we know Oliver wants to know
the total number of kiwis he has.
To find the total number of kiwis, we
add the kiwis picked on Friday, Saturday,
and Sunday. Total kiwis = 44 (Friday) +
58 (Saturday) + 88 (Sunday) = 190 kiwis.
The five smaller kiwis mentioned in P4
do not affect the total count, as they are
a subset of the kiwis picked on Sunday.
Therefore, Oliver has 190 kiwis.
</answer>
A.4.3 R-GSM
The examples of this subsection is taken from
the R-GSM examples mentioned in ( Chen et al. ,
2024 ).We see that GPT4-turbo fails to understand
the semantics of the question and evaluate the or-
der of sequences whereas Llama 70B with dis-
course structure "
How many 5 dollar bills does Thomas have after converting all his money and losing some bills on the way home?,Thomas has 240 five dollar bills.,"get it.
Question: Thomas withdraws $1000 in 20 dollar
bills from the bank account. After getting home,
he uses half of the remaining bills to pay for a bill.
Thomas then triples his money. He then converts
all his bills to 5 dollar bills. He lost 10 bills while
getting home. How many 5 dollar bills does he
have?
Correct Answer: 240
Structure:
<structure>
12Topics [topic-a]: Thomas’s initial
withdrawal [topic-b]: Subsequent money
actions
Relationships t1–elab–t2
Premises P1: Thomas withdraws $1000 in 20
dollar bills from the bank account. P2:
He lost 10 bills while getting home. P3:
After getting home, he uses half of the
remaining bills to pay for a bill. P4:
Thomas then triples his money. P5: He
then converts all his bills to 5 dollar
bills.
Narrative Structure P1–elab–P2 P2–narr–P3
P3–narr–P4 P4–narr–P5
</structure>
Answer without DS: (generated by GPT-4-
Turbo):
1. Thomas withdraws $1000 in $20 bills
from the bank. - $1000 / $20 = 50 bills
2. He uses half of the remaining bills
to pay for a bill. - Half of"
How many $5 bills does Thomas have after all the transactions and losses?,Thomas has 290 $5 bills after all the transactions and losses.," 50 bills is
25 bills - So, he uses 25 bills and has
25 bills left
3. Thomas then triples his money. - He
had 25 bills of $20 each - 25 * $20 = $500
before tripling - Tripling $500 gives him
$500 * 3 = $1500
4. He then converts all his bills to $5
bills. - $1500 / $5 = 300 bills
5. He lost 10 bills while getting home.
- 300 bills - 10 bills = 290 bills
Final Answer: Thomas has 290 $5 bills.
Answer with DS:
<answer> To determine the number of 5
dollar bills Thomas has, we will follow
the premises in the given order.
1. Thomas withdraws $1000 in 20 dollar
bills from the bank account (P1). Since
each bill is $20, the number of bills is
$1000 / $20 = 50 bills.
2. He lost 10 bills while getting home
(P2). The remaining number of bills is 50
- 10 = 40 bills.
3. After getting home, he uses half
of the remaining bills to pay for a bill
(P3). Half of the remaining bills is 1/2
* 40 = 20 bills. He uses 20 bills to pay
for a bill, leaving him with 40 - 20 = 20
bills. Since each bill is $20, the total
amount of money he "
How many 5 dollar bills does Thomas have after converting all his money into 5 dollar bills?,Thomas has 240 5 dollar bills after converting all his money into 5 dollar bills.,"has is 20 * $20 = $400.
4. Thomas then triples his money (P4).
He triples the $400, resulting in 3 * $400= $1200.
5. He then converts all his bills to
5 dollar bills (P5). Since he now has
$1200 and each bill is $5, the number of
5 dollar bills is $1200 / $5 = 240 bills.
Therefore, Thomas has 240 5 dollar
bills. </answer>
Question: Larry loves taking care of animals. He
has 3 cats. He has 3 times as many dogs as cats.
He has 2 fewer rabbits than dogs. He also has a
collection of gerbils that’s 1/3 the number of ﬁsh
he has. He has a ﬁsh tank with three times the
number of ﬁsh as rabbits. How many pets does
Larry have?
Correct Answer: 47
Structure:
<structure>
Topics [topic-a]: Larry’s interest in
animals [topic-b]: Inventory of Larry’s
pets
Relationships t1–elab–t2
Premises P1: Larry loves taking care of
animals. P2: He has 3 cats. P3: He has 3
times as many dogs as cats. P4: He has 2
fewer rabbits than dogs. P5: He also has
a collection of gerbils that’s 1/3 the
number of fish he has. P6: He has a fish
tank w"
"How many total pets does Larry have, and how is this number calculated?","Larry has a total of 42 pets. This number is calculated by adding the number of each type of pet he owns: 3 cats, 9 dogs (3 times the number of cats), 7 rabbits (2 fewer than the number of dogs), 2 gerbils (1/3 the number of fish), and 21 fish (3 times the number of rabbits). The sum is 3 + 9 + 7 + 2 + 21 = 42 pets.","ith three times the number of fish
as rabbits.
Narrative Structure P1–narr–P2 P2–narr–P3
P3–narr–P4 P4–narr–P5 P5–narr–P6
</structure>
Answer without DS (generated by GPT-4-
Turbo):
1. Larry has 3 cats.
2. He has 3 times as many dogs as cats.
3 * 3 = 9 dogs
3. He has 2 fewer rabbits than dogs. 9
- 2 = 7 rabbits
4. He has 1/3 the number of gerbils as
fish. 7 / 3 = 2 gerbils
5. He has 3 times the number of fish as
rabbits. 3 * 7 = 21 fish
Total pets: 3 (cats) + 9 (dogs) + 7
(rabbits) + 2 (gerbils) + 21 (fish) = 42
pets
Final Answer: 42.
Answer with DS:
<answer> To determine the total number
of pets Larry has, we need to follow the
premises in the given order.
From P2, we know Larry has 3 cats.
13From P3, we know he has 3 times as many
dogs as cats. 3 * 3 = 9 dogs.
From P4, we know he has 2 fewer rabbits
than dogs. 9 - 2 = 7 rabbits.
From P6, we know he has a fish tank
with three times the number of fish as
rabbits. 3 * 7 = 21 fish.
From P5, we know he has a collection of
gerbils that’s 1/3 the number of fish he"
What is the total number of pets Larry has?,Larry has a total of 47 pets.,"
has. 1/3 * 21 = 7 gerbils.
Now, we have the number of each type of
pet: - Cats: 3 - Dogs: 9 - Rabbits: 7 -
Fish: 21 - Gerbils: 7
To find the total number of pets, we
add the number of each type of pet: 3 +
9 + 7 + 21 + 7 = 47
Therefore, Larry has 47 pets.
</answer>
Question: Marcus is trying to decide whether he
really needs to do his homework. If the normal
teacher comes in, there’s a 40% chance she’ll give
everyone an extension. There’s a 50% chance
that tomorrow he’ll have a substitute teacher who
won’t collect the homework. Even if the whole
class doesn’t get an extension, there’s a 20%
chance Marcus can convince the teacher his dog
ate his assignment and get a personal extension.
What is the percentage chance that Marcus will
actually have to turn in his homework tomorrow?
Correct Answer: 24%
Structure:
<structure>
Topics [topic-a]: Marcus’s homework
dilemma [topic-b]: Probabilities of
extensions or no extension
Relationships t1–elab–t2
Premises P1: Marcus is trying to decide
whether he really needs to "
What is the probability that Marcus will have to turn in his homework if the normal teacher comes in and does not give an extension?,The probability that Marcus will have to turn in his homework if the normal teacher comes in and does not give an extension is 48%. This is calculated by multiplying the 60% chance that the normal teacher does not give an extension by the 80% chance that Marcus does not get a personal extension.,"do his
homework. P2: If the normal teacher
comes in, there’s a 40% chance she’ll
give everyone an extension. P3: There’s
a 50% chance that tomorrow he’ll have a
substitute teacher who won’t collect the
homework. P4: Even if the whole class
doesn’t get an extension, there’s a 20%
chance Marcus can convince the teacher his
dog ate his assignment and get a personal
extension.
Narrative Structure P1–narr–P2 P2–narr–P3
P3–narr–P4
</structure>
Answer without DS:1. The normal teacher comes in and does
not give an extension. - 60% chance (since
there’s a 40% chance she will give one).
- Marcus has a 20% chance of getting a
personal extension. - 80% chance he will
not get the extension and must turn in
his homework. - Probability: 60% * 80% =
48%
2. The normal teacher gives an
extension (40% chance). - Marcus does
not turn in his homework.
3. A substitute teacher comes in (50%
chance). - Marcus does not turn in his
homework.
4. Total probability of Marcus having
to turn in his homework:
- Scenario 1 contributes 48%
pr"
How is the 48% chance that Marcus must turn in his homework calculated?,"The 48% chance is calculated by considering all scenarios where Marcus won't get an extension. First, the probability of the normal teacher coming in and not giving an extension is calculated as 30%. Then, the probability of Marcus not getting a personal extension from the normal teacher is calculated as 24%. Adding these probabilities gives the final chance that Marcus must turn in his homework, which is 48%.","obability. - Other scenarios do
not contribute.
Final Answer: 48% chance that Marcus
must turn in his homework.
Answer with DS:
<answer> To find the percentage chance
that Marcus will actually have to turn
in his homework tomorrow, we need to
consider all scenarios where he won’t get
an extension.
First, we calculate the probability
of the normal teacher coming in and
not giving an extension: Probability of
normal teacher coming in = 100% - 50%
(substitute teacher) = 50% Probability of
normal teacher not giving an extension =
100% - 40% (giving an extension) = 60% So,
the probability of normal teacher coming
in and not giving an extension = 50% *
60% = 30%
Now, we calculate the probability of
Marcus not getting a personal extension
from the normal teacher: Probability of
not getting a personal extension = 100%
- 20% (getting a personal extension) =
80% So, the probability of normal teacher
coming in, not giving an extension, and
Marcus not getting a personal extension
= 30% * 80% = 24%
Next, we calculate the "
What is the probability that Marcus will have to turn in his homework tomorrow?,The probability that Marcus will have to turn in his homework tomorrow is 24%.,"probability of
the substitute teacher coming in and
collecting the homework: Probability
of substitute teacher coming in =
50% Probability of substitute teacher
collecting the homework = 100% (since it’s
14not stated that the substitute teacher
won’t collect homework in all cases,
we assume the worst-case scenario for
Marcus)
However, since the problem states that
the substitute teacher won’t collect the
homework, this scenario is actually in
Marcus’s favor, and we don’t need to
consider it as a case where he has to
turn in his homework.
Now, we add the probability of the
normal teacher coming in, not giving
an extension, and Marcus not getting
a personal extension to get the total
probability of Marcus having to turn
in his homework: Probability of Marcus
having to turn in his homework =
24% (normal teacher, no extension, no
personal extension)
Therefore, the percentage chance that
Marcus will actually have to turn in his
homework tomorrow is 24%. </answer>
Question: A bumper car rink has 12 red cars.
They h"
"How many yellow cars does the bumper car rink have if they have 75 cars in total, 12 red cars, 10 green cars, and 3 times the number of blue cars as green cars?",The bumper car rink has 23 yellow cars.,"ave 2 fewer green cars than they have red
cars. The rink also has yellow cars. They have 3
times the number of blue cars as they have green
cars. If the rink has 75 cars in total how many yel-
low cars do they have?
Correct Answer: 23
Structure:
<structure>
Topics [topic-a]: Bumper car rink
inventory [topic-b]: Color counts of cars
Relationships t1–elab–t2
Premises P1: A bumper car rink has 12
red cars. P2: They have 2 fewer green
cars than they have red cars. P3: The
rink also has yellow cars. P4: They have
3 times the number of blue cars as they
have green cars. P5: The rink has 75 cars
in total.
Narrative Structure P1–narr–P2 P2–narr–P3
P3–narr–P4 P4–narr–P5
</structure>
Answer without DS:
1. There are 12 red cars.
2. There are 2 fewer green cars than
red cars. 12 - 2 = 10 green cars
3. The number of yellow cars is 3 times
the number of blue cars, but the number
of blue cars is not given. Instead, we
rely on the total number of cars given.4. Total number of cars = 75 Non-yellow
cars = Red cars + Green cars"
How many yellow cars does the rink have according to the problem's solution?,The rink has 23 yellow cars.," 12 + 10 = 22
non-yellow cars
5. Yellow cars = Total cars - Non-yellow
cars 75 - 22 = 53 yellow cars
Final Answer: 53 yellow cars.
Answer with DS:
<answer>
To find out how many yellow cars
the rink has, we need to follow the
relationships and premises as structured
in the abstract.
From P1, we know the rink has 12 red
cars.
From P2, we know they have 2 fewer green
cars than they have red cars. 12 - 2 = 10
green cars.
From P4, we know they have 3 times the
number of blue cars as they have green
cars. 3 * 10 = 30 blue cars.
Now, we know the rink has: - 12 red
cars - 10 green cars - 30 blue cars
Total cars accounted for so far: 12 +
10 + 30 = 52 cars.
From P5, we know the rink has 75 cars
in total.
Since we’ve accounted for 52 cars, the
remaining cars must be yellow. 75 - 52 =
23 yellow cars.
Therefore, the rink has 23 yellow cars.
</answer>
15A.5 GSM-MOD Example
ASTORY AND ITS VARIANTS
Original Story
Kenny wants to make sure he does more jumping jacks every week than he did the week before. He recorded that las"
How many jumping jacks does Kenny need to do on Saturday to beat last week's total of 324 jumping jacks?,"Kenny needs to do 61 jumping jacks on Saturday to beat last week's total of 324 jumping jacks. (He has done 34 + 20 + 0 + 123 + 64 + 23 = 264 jumping jacks so far, so he needs 324 - 264 = 60 to match last week, and 61 to beat it.)","t week he
did 324 total jumping jacks. On Saturday of this week, he look s at his records and sees that on Sunday he did 34. On Monday
he did 20. On Tuesday he skipped a day. On Wednesday he did 123. On Thursday he did 64. On Friday he did 23. How many does
he have to do on Saturday to make sure he beats last week’s number?
................................................... ................................................... ....................................
Variation: C-MOD
Zeta wants to write more lines of code this sprint than she did in the previous sprint . Last sprint, she
wrote 324 lines in total . This sprint, so far: Day 1: 34 lines Day 2: 20 lines Day 3: 0 lines Day 4: 123 lines
Day 5: 64 lines Day 6: 23 lines How many lines must Zeta write on Day 7 to exceed 324?
................................................... ................................................... ....................................
Variation: N-MOD
Kenny wants to make sure he does more jumping jacks this week t han he did th"
How many star jumps does Kenny need to perform on Saturday to surpass last week's total of 324 star jumps?,Kenny needs to perform 61 star jumps on Saturday to surpass last week's total of 324 star jumps.,"e week before. He recorded that last week he
did648 total jumping jacks. On Saturday of this week, he looks at his records and sees that on Sunday he did 68 , on
Monday 40 , on Tuesday he skipped a day ( 0 ), on Wednesday 246 , on Thursday 128 , and on Friday 46 . How many does he
have to do on Saturday to ensure he beats last week’s number of 648 ?
................................................... ................................................... ....................................
Variation: L-MOD
Kennyintends to ensure he performs more star jumps every week than he completed the previous week. He documented
that last week he completed 324 overall star jumps . On Saturday of this week, he examines his logs and notices
that on Sunday he accomplished 34. On Monday he accomplished 20. On Tuesday he omitted a day. On Wednesday he
accomplished 123. On Thursday he performed 64. On Friday he accomplished 23. How many does he have to perform on
Saturday to ensure he surpasses last week’s total ?
Figure 2: Comp"
"Where can the figure ""narrrative.png"" be accessed from?","The figure ""narrrative.png"" can be accessed from the URL: http://arxiv.org/ps/2503.04685v1.","arison of an original story and its variants.
16This figure ""narrrative.png"" is available in ""png""
 format from:
http://arxiv.org/ps/2503.04685v1"
What did the study find about the ability of large language models (LLMs) to perform Bayesian reasoning using in-context learning (ICL)?,"The study found that large language models (LLMs) can broadly follow Bayesian posterior updates when given sufficient demonstrations of biased coin flips via in-context learning (ICL). However, deviations from Bayesian reasoning are primarily due to miscalibrated priors rather than flawed updates. Additionally, in-context evidence tends to outweigh explicit bias instructions, and attention magnitude has a negligible effect on Bayesian inference.","Enough Coin Flips Can Make LLMs Act Bayesian
Ritwik Gupta*Rodolfo Corona*Jiaxin Ge*Eric Wang
Dan Klein Trevor Darrell David M. Chan
University of California, Berkeley
Abstract
Large language models (LLMs) exhibit the
ability to generalize given few-shot examples
in their input prompt, an emergent capability
known as in-context learning (ICL). We
investigate whether LLMs utilize ICL to
perform structured reasoning in ways that are
consistent with a Bayesian framework or rely on
pattern matching. Using a controlled setting of
biased coin flips, we find that: (1) LLMs often
possess biased priors, causing initial divergence
in zero-shot settings, (2) in-context evidence
outweighs explicit bias instructions, (3) LLMs
broadly follow Bayesian posterior updates, with
deviations primarily due to miscalibrated priors
rather than flawed updates, and (4) attention
magnitude has negligible effect on Bayesian
inference. With sufficient demonstrations of
biased coin flips via ICL, LLMs update their
priors in a Bayesian mann"
What is in-context learning (ICL) and how is it used in large language models?,"In-context learning (ICL) is a technique used in large language models where demonstrations of a specific task are provided to the model as part of its input prompt. This allows the model to adapt its predictions based on few-shot examples without requiring explicit weight updates. Despite its empirical success, the underlying mechanisms of ICL are not fully understood, particularly whether the adaptation aligns with statistical principles like Bayesian inference.","er.
1 Introduction
Large language models (LLMs) designed for
next-token prediction have gained significant pop-
ularity, largely because of their ability to generalize
beyond language prediction and perform a wide
range of novel tasks without requiring explicit
weight updates (Brown et al., 2020). Methods
to induce emergence in controlled ways include
techniques such as chain-of-thought prompting
(Wei et al., 2022), prompt chaining (Wu et al., 2022),
and in-context learning (ICL). ICL, particularly,
provides demonstrations of a specific task to the
model as part of its input prompt.
Despite significant empirical success, the under-
lying mechanisms of ICL remain poorly understood.
While it is clear that models can adapt their predic-
tions in response to few-shot examples, it is less
clear whether this adaptation aligns with statistical
principles such as Bayesian inference. Do these
models simply replicate memorized patterns from
their training data, or do they systematically update
*Denotes co-first authors"
How do large language models (LLMs) adapt their posterior probabilities when using in-context learning (ICL) in a biased coin-flipping experiment?,"Large language models (LLMs) adapt their posterior probabilities by updating their parameter estimates in a way that broadly aligns with Bayesian behavior. Despite often having poorly calibrated priors, LLMs adjust their beliefs consistently with Bayesian reasoning when presented with new evidence in the prompt, as demonstrated in the biased coin-flipping experiment.","hip.
0 50 100
ICL Examples0.000.250.500.751.00Pmodel
Model parameter estimate (Llama3.1-8B)
Current parameter ()
Bayesian Posterior Mean (=0.88)
Figure 1: When we ask large language models (LLMs)
to model sequences with in-context learning (ICL), how
do they adapt their posterior probabilities given the
provided examples? This figure explores how model
probabilities change as we add new ICL examples in a
biased coin-flipping experiment. The X-axis represents
steps in the trajectory, while the Y-axis shows the
predicted parameter of a Bernoulli distribution. Our
results reveal that, while LLMs often have poorly
calibrated priors, their updated parameter estimates
broadly align with Bayesian behavior.
their beliefs in a way that is consistent with Bayesian
reasoning when presented with new evidence in the
prompt? In this work, we investigate these questions
using a controlled setting of biased coin flips.
A prominent explanation for ICL’s behavior is
that it reflects some form of Bayesian learning.
Prior studie"
What is the challenge in determining how closely large language models adhere to Bayesian updates in certain tasks?,"The challenge is that many tasks, such as question-answering or language modeling, have unknown true posterior distributions, making it difficult to determine how closely a model's inferences adhere to normative Bayesian updates.","s have suggested that, in certain
scenarios, large language models can approximate
Bayesian updating by maintaining an implicit prior
distribution over latent structures and refining that
prior using contextual information (Xie et al., 2021;
Hahn and Goyal, 2023; Akyürek et al., 2022; Zhang
et al., 2023; Panwar et al., 2023). However, many of
these works rely on tasks (e.g., question-answering
or language modeling) where the true posterior
distribution is unknown, making it difficult to
determine how closely a model’s inferences adhere
1arXiv:2503.04722v1  [cs.CL]  6 Mar 2025to normative Bayesian updates. Other research
has pursued more controlled theoretical settings
with known posteriors, but with strong assumptions
about model architectures or data domains. As
a result, the extent to which pre-trained LLMs
truly follow Bayesian update rules, and whether
their test-time behavior aligns with canonical
probabilistic reasoning, remains an open question.
We reduce the complexity of typical ICL analyses
by focus"
What are some key findings about pre-trained LLMs when evaluating their Bayesian reasoning with biased coin flips?,"The key findings are: (1) Language models often exhibit biased priors for stochastic phenomena, leading to significant initial divergence in zero-shot scenarios; (2) They tend to disregard explicit bias instructions and rely more heavily on in-context examples; (3) Their predictions are consistent with Bayesian updates once new evidence is presented.","ing on a stochastic phenomenon: biased
coin flips. This setting allows us to compute all
relevant Bayesian quantities and thus precisely
evaluate whether pre-trained LLMs update their
priors in a Bayesian manner. By examining how
models estimate coin biases and incorporate se-
quential evidence, we can directly assess the degree
to which they converge on normative probabilistic
reasoning. In addition, this streamlined setup lets us
explore the impact of factors like attention, model
scale, and instruction tuning without introducing
the distributional complexities of more elaborate
language tasks.
In this work we find several results: (1) language
models often exhibit biased priors for stochastic
phenomena, leading to significant initial divergence
when modeling zero-shot scenarios; (2) they tend
to disregard explicit bias instructions and rely more
heavily on in-context examples; (3) their predictions
are consistent with Bayesian updates once new
evidence is presented, with most deviations from
the true poste"
What do the results imply about the performance of LLMs in Bayesian modeling?,"The results imply that LLMs implicitly perform Bayesian modeling in simple cases, and that poor priors may cause reduced performance in more complex environments rather than failures of updates due to in-context learning.","rior arising from miscalibrated priors
rather than faulty updates; and (4) attention magni-
tude has minimal influence on the updating process.
Taken together, these results imply that LLMs
implicitly perform Bayesian modeling in simple
cases, and that poor priors may cause reduced
performance in more complex environments rather
than failures of updates due to in-context learning.
2 Background & Related Work
Representing probabilities in language models.
As LLMs have proliferated across a wide set of
applications, many have examined whether LLMs
can properly represent the concept of probability.
Much of this examination has been done through
the lens of model calibration and alignment. Zhu
and Griffiths (2024) show that LLMs are biased
judges of probability much in the same fashion as
human probability judgments. Gu et al. (2024) asks
whether LLMs can play dice and finds that whileLLMs know what probability is, they struggle to
accurately sample from distributions. They attempt
to solve this through tool use,"
What is the main challenge with using in-context learning (ICL) to align language models with target distributions?,"The main challenge with using in-context learning (ICL) to align language models with target distributions is the models' sensitivity to the positioning of tokens in their prompts, which complicates the effective use of ICL as an alignment technique."," but find that this is
not a guaranteed solution to the problem. Meister
et al. (2024) evaluates how well LLMs can align
to human groups’ distributions over a diverse set
of opinions. They find that LLMs are good at
describing biased distributions but are incapable
of simulating these distributions.
In this work, we explore the ability of LLMs
to simulate biased probability distributions and
explore the mechanism of in-context learning as
a natural method by which LLMs can align their
priors to requested distributions.
In-context learning. Brown et al. (2020) intro-
duces in-context learning (ICL) as a mechanism for
few-shot generalization in language models. Al-
though ICL usage has surged, users rarely employ it
as a method to align models with target distributions.
Further, issues with models’ sensitivity to the posi-
tioning of tokens in their prompts have complicated
the effective use of ICL as an alignment technique.
Lu et al. (2022) demonstrates that the positioning of
information within an ICL prompt "
"What is the ""lost-in-the-middle"" effect as discussed by Liu et al. (2023)?","The ""lost-in-the-middle"" effect, as discussed by Liu et al. (2023), refers to the phenomenon where information in the middle of a prompt is down-weighted, affecting model performance.","affects model per-
formance and devises a permutation-based approach
to overcome this bias. Liu et al. (2023) extends this
analysis to highlight a persistent “lost-in-the-middle”
effect, in which the information in the middle of a
prompt is down-weighted.
Our work shows that in-context rollouts of a
probability distribution correlate well with the
mean of a Bayesian posterior, and we further show
that LLMs have a time-varying discount factor over
the ICL prompt.
Bayesian updating in language models. Many
authors have explored the mechanism by which ICL
emerges in language models. Xie et al. (2021) finds
that ICL can be viewed as a language model im-
plicitly performing Bayesian inference—i.e., ICL
emerges via modeling long-range coherence during
pretraining. Jiang (2023) shows that emergent
capabilities of LLMs, such as ICL, are Bayesian
inference on the sparse joint distribution of
languages. Wang et al. (2024) react to the ordering
sensitivity of ICL prompts and pose ICL as a natural
side effect of LLMs fun"
What do Zhang et al. (2023) propose about In-Context Learning (ICL)?,Zhang et al. (2023) propose that In-Context Learning (ICL) is an implicit form of Bayesian model averaging.,"ctioning as latent variable
models. Finally, Zhang et al. (2023) posit that ICL
is an implicit form of Bayesian model averaging.
A complementary perspective comes Zhao et al.
2(2021a). They demonstrate that a model’s outputs
in few-shot prompts can be systematically skewed
by inherent biases or the arrangement of examples.
They show that adjusting the model’s decision
boundary or distribution (via contextual calibration)
can substantially mitigate these biases.
Our own findings, that LLMs can often apply
Bayesian-like updates despite relying on miscali-
brated priors, resonate with this need for calibration,
underscoring the importance of correcting initial
biases when using LLMs in downstream tasks. We
confirm the ordering sensitivity of ICL prompts
and further show empirically that ICL has several
implicit Bayesian modeling behaviors. Finally,
we demonstrate that it is unlikely that attention
magnitude is a key component of the formalization.
3 Preliminaries
Bayesian systems: General Bayesian systems
are ex"
What is the likelihood function for observing k heads in n independent coin flips when modeling the probability of obtaining heads as a Bayesian process?,The likelihood function for observing k heads in n independent coin flips is given by \( p(D|\theta) = \theta^k (1-\theta)^{n-k} \).,"pected to update their beliefs in a manner
consistent with Bayes’ rule. Given some evidence,
D, a prior distribution p(θ)and a likelihood p(D|θ),
the posterior distribution is obtained via:
p(θ|D)=p(D|θ)p(θ)
p(D)(1)
where p(D)is the marginal likelihood (or evidence)
ensuring the posterior is properly normalized.
While prior work (Falck et al., 2024) has explored
additional assumptions (such as exchangeability),
here we aim to explore the fundamental update
process in a restricted environment.
Modeling coin-flips as Bayesian processes: In
our setup, we model a biased coin by treating the
probability of obtaining heads, denoted by θ, as
a random variable with a binomial distribution.
Suppose we perform nindependent coin flips and
observe kheads and n−ktails. The likelihood of
the observed data is given by:
p(D|θ)=θk(1−θ)n−k(2)
A common choice for the prior distribution of
θis the Beta distribution due to its conjugacy with
the binomial likelihood:
p(θ)=θα−1(1−θ)β−1
B(α,β)(3)
where B(α,β)is the Beta function. By"
What is the posterior distribution for θ when using a uniform prior in Bayesian inference for coin bias?,"When using a uniform prior, the posterior distribution for θ is proportional to θ^k(1−θ)^(n−k), which is a Beta distribution with parameters α+k and β+n−k. Specifically, with a uniform prior (α=1, β=1), the posterior distribution becomes p(θ|D)∝θ^k(1−θ)^(n−k)."," applying
Bayes’ theorem, the posterior distribution is thusproportional to the product of the likelihood and the
prior:
p(θ|D)∝p(D|θ)p(θ) (4)
∝θk(1−θ)n−k·θα−1(1−θ)β−1(5)
=θα+k−1(1−θ)β+n−k−1(6)
And the posterior distribution for θis also a Beta
distribution:
θ|D∼Beta(α+k,β+n−k). (7)
It is often useful to consider the case where we
have no strong prior beliefs about the coin’s bias,
leading us to adopt a uniform prior for θ. The
uniform prior over the interval [0,1]is a special case
of the Beta distribution with parameters α=1and
β=1, i.e.,p(θ)=Beta(θ;1,1)=1 . When using the
uniform prior, the posterior distribution becomes:
p(θ|D)∝θk(1−θ)n−k, (8)
This Bayesian framework allows us to update our
beliefs about the coin’s bias as more coin-flip data
is collected, providing both a point estimate and a
measure of uncertainty for θ.
Experimental design: We focus on open-source
language models and extract stochastic represen-
tations directly from the underlying learned model
distributions. Consider a sequence of tok"
How is the probability of an outcome computed in a next-token prediction-based language model given a prompt?,"The probability of an outcome is computed by multiplying the probabilities of each token in the outcome sequence, given the preceding tokens and the prompt. This is expressed as \( p_M(o|prompt) = \prod_{i=1}^{|o|} p_M(o_i|o_{1:i-1}, prompt) \), where \(|o|\) denotes the number of tokens in the outcome, and \(o_{1:i-1}\) represents the subsequence of tokens preceding the \(i\)th token in the outcome.","ens
x={x1,x2,...,x n} (9)
drawn from a vocabulary V(with|V|elements). A
large next-token prediction-based language model,
M, approximates a probability distribution over
the next token:
pM(xi+1|x1:i) (10)
where x1:i={x1,x2,...,x i}.
To evaluate stochastic processes, we define a
fixed set of possible outcomes Ω ={o1,o2,...,o k},
where each outcome o∈Ωis a sequence of tokens
corresponding to a specific string value (e.g.,
when modeling a coin flip, the outcomes “heads”
and “tails” might correspond to token sequences
[_heads] and[_tails] , respectively). For each
outcome o, we compute the probability given a
prompt—analogous to updating our beliefs in a
Bayesian framework—as follows:
pM(o|prompt )=|o|Y
i=1pM(oi|o1:i−1,prompt )
(11)
3where |o|denotes the number of tokens in o
ando1:i−1represents the subsequence of tokens
preceding the ith token in o.
Because these outcomes are a subset of all
possible token sequences that Mcould generate,
we renormalize the distribution over the support
Ω. We denote the renormali"
What metric is used in the experiments to measure the discrepancy between the true posterior distribution and the normalized model distribution?,The total variation distance (TVD) is used to measure the discrepancy between the true posterior distribution \( p^*(o) \) and the normalized model distribution \( \hat{p}_M(o) \).,"zed model distribution
asˆpM(o)foro∈Ω(see subsection C.2 for further
details on the renormalization process).
In our experiments, we measure the total
variation distance (TVD) between the true posterior
distribution p∗(o)and the normalized model
distribution ˆpM(o)over the support Ω:
δ(p∗,ˆpM)=1
2X
o∈Ω|p∗(o)−ˆpM(o)| (12)
This distance metric quantifies the discrepancy
between the two distributions—zero indicating
perfect alignment and higher values indicating
greater divergence.
We would like to clearly state that we are not
claiming that LLMs themselves are explicitly
Bayesian, rather, we ask the question: do model
predictive distributions have Bayesian behavior? In
this paper we treat models themselves as point-wise
estimators of distributional parameters (in our case,
we use them to estimate the parameters of a binomial
distribution), and ask if those point-wise estimates
align with reasonable Bayesian frameworks.
We evaluate several models, including Gemma-2
(Team et al., 2024), Phi-2/Phi-3.5 (mini) (Abdi"
What is the purpose of querying language models with different prompt variants in the study?,"The purpose of querying language models with different prompt variants is to extract and understand the models' inherent priors over the likelihood of heads in a coin flip, and to explore how these priors can be updated through explicit prompting.","n
et al., 2024), Llama-3.1 (8B) (Dubey et al., 2024),
Mistral 7B (Jiang et al., 2023), and OLMoE
(7B) (Muennighoff et al., 2024), along with their
instruction-tuned variants. For scaling experiments,
we leverage the Pythia Scaling Suite (Biderman
et al., 2023) For more details regarding these
models, please refer to Appendix D.
4 Understanding the LLM Prior
Due to data-intensive pre-training, language models
inherently encode a prior over θ(the likelihood
of heads in the coin-flip). We are interested in
understanding these priors and understanding how
to update the priors via explicit prompting.
To extract a prior over heads and tails, we query
the models for a coin flip through 50different
prompt variants (e.g. “I flipped a coin and
it landed on”) , and compute the normalized
0.0 0.2 0.4 0.6 0.8 1.0
OLMoE-1B-7B-0924Gemma-2-2BGemma-2B-InstructLlama3.1-8BLlama3.1-8B-InstructPhi-3.5-mini-InstructPhi-2Mistral-7B-InstructMistral-7BModel Priors Over the Probability of Heads ()
Figure 2: Model priors: All language "
"How do language models initially handle the tokenization of ""tails"" in the context of coin flips, and what is the impact on their priors?","Language models evaluated show a bias towards heads, and in some cases, they do not see sufficient data to assign a full token to ""tails."" Instead, ""tails"" is encoded as a pair of tokens, which contributes to the models beginning with a heads-biased prior, divergent from an unbiased estimate of coin priors. This is discussed in more detail in Appendix C.","models evaluated
present a bias towards heads.
logit value ascribed to heads (discussed in detail in
Appendix C). As shown in Figure 2, all language
models evaluated begin with fundamental priors
forθthat are heads-biased, and in some cases,
significantly so. This observation is reflected in
the tokenization structure itself; in some cases,
models do not see sufficient data to assign a full
token to [_tails] and instead encode this in a
pair of tokens (which we handle when computing
probability, see Appendix C). Thus, models begin
divergent from an unbiased estimate of coin priors.
Effect of explicit biasing via prompting. Next,
we explore if we can encourage models to update
their priors by providing an explicit value for θin
the prompt. We define a set of biasing statements,
i.e. describing unfair coins, of the form “When I
flip coins, they land on heads X% of the
time.” , and run a set of trials, evaluating the TVD
between models’ probabilities over outcomes and
the expected distribution for the biased θ.
"
How do instruct models perform compared to non-instruct LLMs when given an explicit bias in the input prompt?,"Instruct models perform slightly better than non-instruct LLMs when given an explicit bias in the input prompt. While both types of models exhibit a bias toward heads, instruct models show improved performance at the extremes of bias values, with Total Variation Distance (TVD) values dropping for 0% and 100% heads biases.","Results from this experiment are presented in
Figure 3. Given an explicit bias in the input prompt,
non-instruct LLMs fail to converge to the expected
biased distribution with their token probabilities fol-
lowing their originally computed prior—generally
showing a tendency to ascribe ≈60%-80% prob-
ability to heads, independent of explicit context.
Instruct models performed slightly better, though
they still exhibited a bias toward heads. Addition-
ally, instruct models showed improved performance
at the extremes of bias values, with TVD values
dropping for 0% and 100% heads biases (matching
observations from Zhao et al. (2021b)).
Effect of model size on priors. Scaling the lan-
guage model size has shown effectiveness in many
40 0.2 0.4 0.6 0.8 1
0.00.20.40.60.81.0TVD
Coin Flip (Non-Instruct)
Mistral-7B
OLMoE-1B-7B-0924
Gemma-2-2B
Phi-2
Llama3.1-8B
0 0.2 0.4 0.6 0.8 1
0.00.20.40.60.81.0TVD
Coin Flip (Instruct)
Llama3.1-8B-Instruct
Gemma-2B-Instruct
Mistral-7B-Instruct
Phi-3.5-mini-InstructFigure 3: Biased c"
How do instruct-based models perform in the biased coin flip experiment compared to non-instruct models?,"Instruct-based models perform significantly better than non-instruct models in the biased coin flip experiment. They pay better attention to biasing information and perform well when modeling extreme bias, such as always generating heads or tails, whereas non-instruct models tend to ignore biasing instructions and almost always generate a biased distribution of approximately 70% heads.","oins: Plots of mean total variation distance (TVD, ↓) against bias ( θ) for non-instruct (left) and
instruct (right) models when aggregated across prompts (N=50) for the biased coin flip experiment. Shaded areas
show one standard deviation. While non-instruct models both (1) ignore biasing instructions in the prompts and
(2) almost always generate a biased distribution ( ≈70% heads), instruct-based models pay better attention to biasing
information, and perform significantly better when modeling extreme bias (always generating heads/tails).
tasks. Therefore, we explore whether scaling also
boosts performance on modeling expected biased
distribution. We use Pythia Scaling Suite (Biderman
et al., 2023) that covers model size ranging from
70M to 12B and test on different biased θ. Results
from this experiment are presented in Figure 4. For
a given bias, scaling the model size does not substan-
tially change the language models’ priors or improve
the performance of modeling expected distributions.
However, the re"
How does the relative ordering among different biases change with model size according to the text?,The relative ordering among different biases shifts as the model size increases.,"lative ordering among different bi-
ases does shift as the model size increases.
70M160M 410M1B1.4B 2.8B 6.9B 12B
Model Size0.00.20.40.60.81.0TVD
TVD Across Model Sizes
=0.0
=0.1
=0.2
=0.3
=0.4
=0.5
=0.6
=0.7
=0.8
=0.9
=1.0
Figure 4: Biased coins and parameter scaling: Mean
total variation distance (TVD, ↓) vs. model size for
different bias percentages. We use the models from the
Pythia Scaling Suite. As the size of the model increases,
the performance does not change for a certain bias. The
relative ordering among different biases does shift as the
model size increases
5 Does In-Context Learning
Improve Parameter Estimates?
We are interested in understanding if and how
LLMs incorporate in-context evidence into their
posteriors. Specifically, rather than explicitlydescribing the underlying distribution as before,
we implicitly specify it by providing the LLM
with a sequence of samples from that distribution
in its prompt (e.g., “I flipped a coin and it
landed on heads, then on tails, then on
tails, then on ta"
