{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZ6pwyhFup9t",
        "outputId": "689ba9cf-7301-43da-bbfa-ddb23085e4b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyPDF2 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (3.0.1)\n",
            "Requirement already satisfied: dotenv in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (0.9.9)\n",
            "Requirement already satisfied: feedparser in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (6.0.11)\n",
            "Requirement already satisfied: ragas in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (0.2.14)\n",
            "Requirement already satisfied: datasets in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (3.3.2)\n",
            "Requirement already satisfied: python-dotenv in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from dotenv) (1.0.1)\n",
            "Requirement already satisfied: sgmllib3k in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from feedparser) (1.0.0)\n",
            "Requirement already satisfied: numpy in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from ragas) (2.2.3)\n",
            "Requirement already satisfied: tiktoken in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from ragas) (0.9.0)\n",
            "Requirement already satisfied: langchain in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from ragas) (0.3.20)\n",
            "Requirement already satisfied: langchain-core in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from ragas) (0.3.43)\n",
            "Requirement already satisfied: langchain-community in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from ragas) (0.3.19)\n",
            "Requirement already satisfied: langchain_openai in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from ragas) (0.3.8)\n",
            "Requirement already satisfied: nest-asyncio in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from ragas) (1.6.0)\n",
            "Requirement already satisfied: appdirs in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from ragas) (1.4.4)\n",
            "Requirement already satisfied: pydantic>=2 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from ragas) (2.10.6)\n",
            "Requirement already satisfied: openai>1 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from ragas) (1.65.4)\n",
            "Requirement already satisfied: diskcache>=5.6.3 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from ragas) (5.6.3)\n",
            "Requirement already satisfied: filelock in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from datasets) (19.0.1)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from datasets) (0.29.2)\n",
            "Requirement already satisfied: packaging in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from aiohttp->datasets) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from openai>1->ragas) (4.8.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from openai>1->ragas) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from openai>1->ragas) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from openai>1->ragas) (0.8.2)\n",
            "Requirement already satisfied: sniffio in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from openai>1->ragas) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from pydantic>=2->ragas) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from pydantic>=2->ragas) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: colorama in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from langchain->ragas) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from langchain->ragas) (0.3.13)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from langchain->ragas) (2.0.38)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from langchain-core->ragas) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from langchain-core->ragas) (1.33)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from langchain-community->ragas) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from langchain-community->ragas) (2.8.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from langchain-community->ragas) (0.4.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from tiktoken->ragas) (2024.11.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (0.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai>1->ragas) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core->ragas) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain->ragas) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain->ragas) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain->ragas) (0.23.0)\n",
            "Requirement already satisfied: six>=1.5 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain->ragas) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\gayanuka amarasuriya\\documents\\competitions\\intellihack_5.0\\code\\intellihack_sidemen_3\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (1.0.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install PyPDF2 dotenv feedparser ragas datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "x90UYI_QufcI"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Gayanuka Amarasuriya\\Documents\\Competitions\\IntelliHack_5.0\\Code\\Intellihack_Sidemen_3\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "import markdown\n",
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"PyPDF2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PEsyAJJ7u8IF"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvT-tEu353eH"
      },
      "source": [
        "#### Download additional research papers from the internet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdwuTWZLvOTc",
        "outputId": "ece4ec15-3997-46b2-a535-780bc04f3a00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Title: Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and\n",
            "  Reasoning-Driven Code Intelligence in LLMs\n",
            "PDF: http://arxiv.org/pdf/2502.19411v1\n",
            "\n",
            "Title: Rewarding Graph Reasoning Process makes LLMs more Generalized Reasoners\n",
            "PDF: http://arxiv.org/pdf/2503.00845v1\n",
            "\n",
            "Title: Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic\n",
            "  Corpus\n",
            "PDF: http://arxiv.org/pdf/2411.12498v2\n",
            "\n",
            "Title: Reasoning-as-Logic-Units: Scaling Test-Time Reasoning in Large Language\n",
            "  Models Through Logic Unit Alignment\n",
            "PDF: http://arxiv.org/pdf/2502.07803v1\n",
            "\n",
            "Title: Logical Reasoning in Large Language Models: A Survey\n",
            "PDF: http://arxiv.org/pdf/2502.09100v1\n",
            "\n",
            "Title: Leveraging LLM Reasoning Enhances Personalized Recommender Systems\n",
            "PDF: http://arxiv.org/pdf/2408.00802v1\n",
            "\n",
            "Title: Reinforcing Thinking through Reasoning-Enhanced Reward Models\n",
            "PDF: http://arxiv.org/pdf/2501.01457v1\n",
            "\n",
            "Title: ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning\n",
            "PDF: http://arxiv.org/pdf/2502.01100v1\n",
            "\n",
            "Title: Disentangling Logic: The Role of Context in Large Language Model\n",
            "  Reasoning Capabilities\n",
            "PDF: http://arxiv.org/pdf/2406.02787v1\n",
            "\n",
            "Title: Advancing Reasoning in Large Language Models: Promising Methods and\n",
            "  Approaches\n",
            "PDF: http://arxiv.org/pdf/2502.03671v1\n",
            "\n",
            "Title: Enhancing Logical Reasoning in Large Language Models to Facilitate Legal\n",
            "  Applications\n",
            "PDF: http://arxiv.org/pdf/2311.13095v1\n",
            "\n",
            "Title: RL-STaR: Theoretical Analysis of Reinforcement Learning Frameworks for\n",
            "  Self-Taught Reasoner\n",
            "PDF: http://arxiv.org/pdf/2410.23912v1\n",
            "\n",
            "Title: Automatic Curriculum Expert Iteration for Reliable LLM Reasoning\n",
            "PDF: http://arxiv.org/pdf/2410.07627v1\n",
            "\n",
            "Title: SocraSynth: Multi-LLM Reasoning with Conditional Statistics\n",
            "PDF: http://arxiv.org/pdf/2402.06634v1\n",
            "\n",
            "Title: DeLTa: A Decoding Strategy based on Logit Trajectory Prediction Improves\n",
            "  Factuality and Reasoning Ability\n",
            "PDF: http://arxiv.org/pdf/2503.02343v1\n",
            "\n",
            "Title: HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs\n",
            "PDF: http://arxiv.org/pdf/2412.18925v1\n",
            "\n",
            "Title: Chain-of-Thought Hub: A Continuous Effort to Measure Large Language\n",
            "  Models' Reasoning Performance\n",
            "PDF: http://arxiv.org/pdf/2305.17306v1\n",
            "\n",
            "Title: Training Language Models to Reason Efficiently\n",
            "PDF: http://arxiv.org/pdf/2502.04463v2\n",
            "\n",
            "Title: Policy Guided Tree Search for Enhanced LLM Reasoning\n",
            "PDF: http://arxiv.org/pdf/2502.06813v1\n",
            "\n",
            "Title: Make LLMs better zero-shot reasoners: Structure-orientated autonomous\n",
            "  reasoning\n",
            "PDF: http://arxiv.org/pdf/2410.19000v1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import feedparser\n",
        "import urllib.parse\n",
        "\n",
        "# Function to fetch AI research papers from arXiv\n",
        "def fetch_arxiv_papers(query=\"large language models\", max_results=20):\n",
        "    base_url = \"http://export.arxiv.org/api/query?\"\n",
        "    encoded_query = urllib.parse.quote(query)  # Encode the query to handle spaces\n",
        "    search_query = f\"search_query=ti:{encoded_query}+OR+abs:{encoded_query}+OR+cat:cs.LG&start=0&max_results={max_results}&sortBy=relevance&sortOrder=descending\"\n",
        "\n",
        "    response = feedparser.parse(base_url + search_query)\n",
        "\n",
        "    papers = []\n",
        "    for entry in response.entries:\n",
        "        papers.append({\n",
        "            \"title\": entry.title,\n",
        "            \"summary\": entry.summary,\n",
        "            \"pdf_url\": entry.link.replace(\"abs\", \"pdf\")\n",
        "        })\n",
        "\n",
        "    return papers\n",
        "\n",
        "# Fetch and print latest AI papers\n",
        "ai_papers = fetch_arxiv_papers(\"Reasoning Capability in LLMs OR Reinforcement Learning OR Logical Reasoning\", max_results=20)\n",
        "for paper in ai_papers:\n",
        "    print(f\"Title: {paper['title']}\\nPDF: {paper['pdf_url']}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MwiTIr6fvX7A"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "DATA_FOLDER = \"data/dataset/\"\n",
        "OUTPUT_FOLDER = \"data/processed_data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOLf-Mltv8fa",
        "outputId": "14d8f1d4-2480-464a-9f46-63aa1e3e0a4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded: data/dataset/Code to Think_ Think to Code_ A Survey on Code-Enhanced Reasoning and_  Reasoning-Driven Code Intelligence in LLMs.pdf\n",
            "Downloaded: data/dataset/Rewarding Graph Reasoning Process makes LLMs more Generalized Reasoners.pdf\n",
            "Downloaded: data/dataset/Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic_  Corpus.pdf\n",
            "Downloaded: data/dataset/Reasoning-as-Logic-Units_ Scaling Test-Time Reasoning in Large Language_  Models Through Logic Unit Alignment.pdf\n",
            "Downloaded: data/dataset/Logical Reasoning in Large Language Models_ A Survey.pdf\n",
            "Downloaded: data/dataset/Leveraging LLM Reasoning Enhances Personalized Recommender Systems.pdf\n",
            "Downloaded: data/dataset/Reinforcing Thinking through Reasoning-Enhanced Reward Models.pdf\n",
            "Downloaded: data/dataset/ZebraLogic_ On the Scaling Limits of LLMs for Logical Reasoning.pdf\n",
            "Downloaded: data/dataset/Disentangling Logic_ The Role of Context in Large Language Model_  Reasoning Capabilities.pdf\n",
            "Downloaded: data/dataset/Advancing Reasoning in Large Language Models_ Promising Methods and_  Approaches.pdf\n",
            "Downloaded: data/dataset/Enhancing Logical Reasoning in Large Language Models to Facilitate Legal_  Applications.pdf\n",
            "Downloaded: data/dataset/RL-STaR_ Theoretical Analysis of Reinforcement Learning Frameworks for_  Self-Taught Reasoner.pdf\n",
            "Downloaded: data/dataset/Automatic Curriculum Expert Iteration for Reliable LLM Reasoning.pdf\n",
            "Downloaded: data/dataset/SocraSynth_ Multi-LLM Reasoning with Conditional Statistics.pdf\n",
            "Downloaded: data/dataset/DeLTa_ A Decoding Strategy based on Logit Trajectory Prediction Improves_  Factuality and Reasoning Ability.pdf\n",
            "Downloaded: data/dataset/HuatuoGPT-o1_ Towards Medical Complex Reasoning with LLMs.pdf\n",
            "Downloaded: data/dataset/Chain-of-Thought Hub_ A Continuous Effort to Measure Large Language_  Models_ Reasoning Performance.pdf\n",
            "Downloaded: data/dataset/Training Language Models to Reason Efficiently.pdf\n",
            "Downloaded: data/dataset/Policy Guided Tree Search for Enhanced LLM Reasoning.pdf\n",
            "Downloaded: data/dataset/Make LLMs better zero-shot reasoners_ Structure-orientated autonomous_  reasoning.pdf\n"
          ]
        }
      ],
      "source": [
        "def sanitize_filename(filename):\n",
        "    return \"\".join(c if c.isalnum() or c in (' ', '_', '-') else '_' for c in filename)\n",
        "\n",
        "def download_papers(papers, save_folder):\n",
        "    if not os.path.exists(save_folder):\n",
        "        os.makedirs(save_folder)\n",
        "\n",
        "    for paper in papers:\n",
        "        pdf_url = paper[\"pdf_url\"]\n",
        "        response = requests.get(pdf_url)\n",
        "        sanitized_title = sanitize_filename(paper['title'])\n",
        "        pdf_filename = os.path.join(save_folder, f\"{sanitized_title}.pdf\")\n",
        "\n",
        "        with open(pdf_filename, \"wb\") as file:\n",
        "            file.write(response.content)\n",
        "\n",
        "        print(f\"Downloaded: {pdf_filename}\")\n",
        "\n",
        "download_papers(ai_papers, DATA_FOLDER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTLwm53x6C2V"
      },
      "source": [
        "#### Process Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pvPgNLX8wBXO"
      },
      "outputs": [],
      "source": [
        "def extract_pdf_text(pdf_path):\n",
        "    text = \"\"\n",
        "    with open(pdf_path, \"rb\") as f:\n",
        "        reader = PyPDF2.PdfReader(f)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "def extract_md_text(md_path):\n",
        "    with open(md_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        md_text = f.read()\n",
        "    return markdown.markdown(md_text)  # Convert MD to plain text if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgetPkz7wHtE",
        "outputId": "c117925a-6232-4e19-d465-66e549774094"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# Process all files\n",
        "all_text = []\n",
        "\n",
        "for file in os.listdir(DATA_FOLDER):\n",
        "    file_path = os.path.join(DATA_FOLDER, file)\n",
        "    if file.endswith(\".pdf\"):\n",
        "        all_text.append(extract_pdf_text(file_path))\n",
        "    elif file.endswith(\".md\"):\n",
        "        all_text.append(extract_md_text(file_path))\n",
        "\n",
        "# Split into chunks (adjust chunk_size as needed)\n",
        "chunk_size = 4000\n",
        "overlap = 500\n",
        "chunks = [text[i:i+chunk_size] for text in all_text for i in range(0, len(text), chunk_size - overlap)]\n",
        "\n",
        "random.shuffle(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BLRZRG2wW1K",
        "outputId": "82fd71ad-89eb-490e-b1aa-6eafaaa4edc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())  # Should output True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EV2ajvF36HQL"
      },
      "source": [
        "#### Generate Q&A Pairs **Prompt 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_RTqktSwbJY",
        "outputId": "e036127b-5b54-44c2-8dfc-0297bd2282d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated QA pair 1\n",
            "Generated QA pair 2\n",
            "Generated QA pair 3\n",
            "Generated QA pair 4\n",
            "Generated QA pair 5\n",
            "Generated QA pair 6\n",
            "Generated QA pair 7\n",
            "Generated QA pair 8\n",
            "Generated QA pair 9\n",
            "Generated QA pair 10\n",
            "Generated QA pair 11\n",
            "Generated QA pair 12\n",
            "Generated QA pair 13\n",
            "Generated QA pair 14\n",
            "Generated QA pair 15\n",
            "Generated QA pair 16\n",
            "Generated QA pair 17\n",
            "Generated QA pair 18\n",
            "Generated QA pair 19\n",
            "Generated QA pair 20\n",
            "Generated QA pair 21\n",
            "Generated QA pair 22\n",
            "Generated QA pair 23\n",
            "Generated QA pair 24\n",
            "Generated QA pair 25\n",
            "Generated QA pair 26\n",
            "Generated QA pair 27\n",
            "Generated QA pair 28\n",
            "Generated QA pair 29\n",
            "Generated QA pair 30\n",
            "Generated QA pair 31\n",
            "Generated QA pair 32\n",
            "Generated QA pair 33\n",
            "Generated QA pair 34\n",
            "Generated QA pair 35\n",
            "Generated QA pair 36\n",
            "Generated QA pair 37\n",
            "Generated QA pair 38\n",
            "Generated QA pair 39\n",
            "Generated QA pair 40\n",
            "Generated QA pair 41\n",
            "Generated QA pair 42\n",
            "Generated QA pair 43\n",
            "Generated QA pair 44\n",
            "Generated QA pair 45\n",
            "Generated QA pair 46\n",
            "Generated QA pair 47\n",
            "Generated QA pair 48\n",
            "Generated QA pair 49\n",
            "Generated QA pair 50\n",
            "Generated QA pair 51\n",
            "Generated QA pair 52\n",
            "Generated QA pair 53\n",
            "Generated QA pair 54\n",
            "Generated QA pair 55\n",
            "Generated QA pair 56\n",
            "Generated QA pair 57\n",
            "Generated QA pair 58\n",
            "Generated QA pair 59\n",
            "Generated QA pair 60\n",
            "Generated QA pair 61\n",
            "Generated QA pair 62\n",
            "Generated QA pair 63\n",
            "Generated QA pair 64\n",
            "Generated QA pair 65\n",
            "Generated QA pair 66\n",
            "Generated QA pair 67\n",
            "Generated QA pair 68\n",
            "Generated QA pair 69\n",
            "Generated QA pair 70\n",
            "Generated QA pair 71\n",
            "Generated QA pair 72\n",
            "Generated QA pair 73\n",
            "Generated QA pair 74\n",
            "Generated QA pair 75\n",
            "Generated QA pair 76\n",
            "Generated QA pair 77\n",
            "Generated QA pair 78\n",
            "Generated QA pair 79\n",
            "Generated QA pair 80\n",
            "Generated QA pair 81\n",
            "Generated QA pair 82\n",
            "Generated QA pair 83\n",
            "Generated QA pair 84\n",
            "Generated QA pair 85\n",
            "Generated QA pair 86\n",
            "Generated QA pair 87\n",
            "Generated QA pair 88\n",
            "Generated QA pair 89\n",
            "Generated QA pair 90\n",
            "Generated QA pair 91\n",
            "Generated QA pair 92\n",
            "Generated QA pair 93\n",
            "Generated QA pair 94\n",
            "Generated QA pair 95\n",
            "Generated QA pair 96\n",
            "Generated QA pair 97\n",
            "Generated QA pair 98\n",
            "Generated QA pair 99\n",
            "Generated QA pair 100\n",
            "Generated QA pair 101\n",
            "Generated QA pair 102\n",
            "Generated QA pair 103\n",
            "Generated QA pair 104\n",
            "Generated QA pair 105\n",
            "Generated QA pair 106\n",
            "Generated QA pair 107\n",
            "Generated QA pair 108\n",
            "Generated QA pair 109\n",
            "Generated QA pair 110\n",
            "Generated QA pair 111\n",
            "Generated QA pair 112\n",
            "Generated QA pair 113\n",
            "Generated QA pair 114\n",
            "Generated QA pair 115\n",
            "Generated QA pair 116\n",
            "Generated QA pair 117\n",
            "Generated QA pair 118\n",
            "Generated QA pair 119\n",
            "Generated QA pair 120\n",
            "Generated QA pair 121\n",
            "Generated QA pair 122\n",
            "Generated QA pair 123\n",
            "Generated QA pair 124\n",
            "Generated QA pair 125\n",
            "Generated QA pair 126\n",
            "Generated QA pair 127\n",
            "Generated QA pair 128\n",
            "Generated QA pair 129\n",
            "Generated QA pair 130\n",
            "Generated QA pair 131\n",
            "Generated QA pair 132\n",
            "Generated QA pair 133\n",
            "Generated QA pair 134\n",
            "Generated QA pair 135\n",
            "Generated QA pair 136\n",
            "Generated QA pair 137\n",
            "Generated QA pair 138\n",
            "Generated QA pair 139\n",
            "Generated QA pair 140\n",
            "Generated QA pair 141\n",
            "Generated QA pair 142\n",
            "Generated QA pair 143\n",
            "Generated QA pair 144\n",
            "Generated QA pair 145\n",
            "Generated QA pair 146\n",
            "Generated QA pair 147\n",
            "Generated QA pair 148\n",
            "Generated QA pair 149\n",
            "Generated QA pair 150\n",
            "Generated QA pair 151\n",
            "Generated QA pair 152\n",
            "Generated QA pair 153\n",
            "Generated QA pair 154\n",
            "Generated QA pair 155\n",
            "Generated QA pair 156\n",
            "Generated QA pair 157\n",
            "Generated QA pair 158\n",
            "Generated QA pair 159\n",
            "Generated QA pair 160\n",
            "Generated QA pair 161\n",
            "Generated QA pair 162\n",
            "Generated QA pair 163\n",
            "Generated QA pair 164\n",
            "Generated QA pair 165\n",
            "Generated QA pair 166\n",
            "Generated QA pair 167\n",
            "Generated QA pair 168\n",
            "Generated QA pair 169\n",
            "Generated QA pair 170\n",
            "Generated QA pair 171\n",
            "Generated QA pair 172\n",
            "Generated QA pair 173\n",
            "Generated QA pair 174\n",
            "Generated QA pair 175\n",
            "Generated QA pair 176\n",
            "Generated QA pair 177\n",
            "Generated QA pair 178\n",
            "Generated QA pair 179\n",
            "Generated QA pair 180\n",
            "Generated QA pair 181\n",
            "Generated QA pair 182\n",
            "Generated QA pair 183\n",
            "Generated QA pair 184\n",
            "Generated QA pair 185\n",
            "Generated QA pair 186\n",
            "Generated QA pair 187\n",
            "Generated QA pair 188\n",
            "Generated QA pair 189\n",
            "Generated QA pair 190\n",
            "Generated QA pair 191\n",
            "Generated QA pair 192\n",
            "Generated QA pair 193\n",
            "Generated QA pair 194\n",
            "Generated QA pair 195\n",
            "Generated QA pair 196\n",
            "Generated QA pair 197\n",
            "Generated QA pair 198\n",
            "Generated QA pair 199\n",
            "Generated QA pair 200\n",
            "Generated QA pair 201\n",
            "Generated QA pair 202\n",
            "Generated QA pair 203\n",
            "Generated QA pair 204\n",
            "Generated QA pair 205\n",
            "Generated QA pair 206\n",
            "Generated QA pair 207\n",
            "Generated QA pair 208\n",
            "Generated QA pair 209\n",
            "Generated QA pair 210\n",
            "Generated QA pair 211\n",
            "Generated QA pair 212\n",
            "Generated QA pair 213\n",
            "Generated QA pair 214\n",
            "Generated QA pair 215\n",
            "Generated QA pair 216\n",
            "Generated QA pair 217\n",
            "Generated QA pair 218\n",
            "Generated QA pair 219\n",
            "Generated QA pair 220\n",
            "Generated QA pair 221\n",
            "Generated QA pair 222\n",
            "Generated QA pair 223\n",
            "Generated QA pair 224\n",
            "Generated QA pair 225\n",
            "Generated QA pair 226\n",
            "Generated QA pair 227\n",
            "Generated QA pair 228\n",
            "Generated QA pair 229\n",
            "Generated QA pair 230\n",
            "Generated QA pair 231\n",
            "Generated QA pair 232\n",
            "Generated QA pair 233\n",
            "Generated QA pair 234\n",
            "Generated QA pair 235\n",
            "Generated QA pair 236\n",
            "Generated QA pair 237\n",
            "Generated QA pair 238\n",
            "Generated QA pair 239\n",
            "Generated QA pair 240\n",
            "Generated QA pair 241\n",
            "Generated QA pair 242\n",
            "Generated QA pair 243\n",
            "Generated QA pair 244\n",
            "Generated QA pair 245\n",
            "Generated QA pair 246\n",
            "Generated QA pair 247\n",
            "Generated QA pair 248\n",
            "Generated QA pair 249\n",
            "Generated QA pair 250\n",
            "Generated QA pair 251\n",
            "Generated QA pair 252\n",
            "Generated QA pair 253\n",
            "Generated QA pair 254\n",
            "Generated QA pair 255\n",
            "Generated QA pair 256\n",
            "Generated QA pair 257\n",
            "Generated QA pair 258\n",
            "Generated QA pair 259\n",
            "Generated QA pair 260\n",
            "Generated QA pair 261\n",
            "Generated QA pair 262\n",
            "Generated QA pair 263\n",
            "Generated QA pair 264\n",
            "Generated QA pair 265\n",
            "Generated QA pair 266\n",
            "Generated QA pair 267\n",
            "Generated QA pair 268\n",
            "Generated QA pair 269\n",
            "Generated QA pair 270\n",
            "Generated QA pair 271\n",
            "Generated QA pair 272\n",
            "Generated QA pair 273\n",
            "Generated QA pair 274\n",
            "Generated QA pair 275\n",
            "Generated QA pair 276\n",
            "Generated QA pair 277\n",
            "Generated QA pair 278\n",
            "Generated QA pair 279\n",
            "Generated QA pair 280\n",
            "Generated QA pair 281\n",
            "Generated QA pair 282\n",
            "Generated QA pair 283\n",
            "Generated QA pair 284\n",
            "Generated QA pair 285\n",
            "Generated QA pair 286\n",
            "Generated QA pair 287\n",
            "Generated QA pair 288\n",
            "Generated QA pair 289\n",
            "Generated QA pair 290\n",
            "Generated QA pair 291\n",
            "Generated QA pair 292\n",
            "Generated QA pair 293\n",
            "Generated QA pair 294\n",
            "Generated QA pair 295\n",
            "Generated QA pair 296\n",
            "Generated QA pair 297\n",
            "Generated QA pair 298\n",
            "Generated QA pair 299\n",
            "Generated QA pair 300\n",
            "Generated QA pair 301\n",
            "Generated QA pair 302\n",
            "Generated QA pair 303\n",
            "Generated QA pair 304\n",
            "Generated QA pair 305\n",
            "Generated QA pair 306\n",
            "Generated QA pair 307\n",
            "Generated QA pair 308\n",
            "Generated QA pair 309\n",
            "Generated QA pair 310\n",
            "Generated QA pair 311\n",
            "Generated QA pair 312\n",
            "Generated QA pair 313\n",
            "Generated QA pair 314\n",
            "Generated QA pair 315\n",
            "Generated QA pair 316\n",
            "Generated QA pair 317\n",
            "Generated QA pair 318\n",
            "Generated QA pair 319\n",
            "Generated QA pair 320\n",
            "Generated QA pair 321\n",
            "Generated QA pair 322\n",
            "Generated QA pair 323\n",
            "Generated QA pair 324\n",
            "Generated QA pair 325\n",
            "Generated QA pair 326\n",
            "Generated QA pair 327\n",
            "Generated QA pair 328\n",
            "Generated QA pair 329\n",
            "Generated QA pair 330\n",
            "Generated QA pair 331\n",
            "Generated QA pair 332\n",
            "Generated QA pair 333\n",
            "Generated QA pair 334\n",
            "Generated QA pair 335\n",
            "Generated QA pair 336\n",
            "Generated QA pair 337\n",
            "Generated QA pair 338\n",
            "Generated QA pair 339\n",
            "Generated QA pair 340\n",
            "Generated QA pair 341\n",
            "Generated QA pair 342\n",
            "Generated QA pair 343\n",
            "Generated QA pair 344\n",
            "Generated QA pair 345\n",
            "Generated QA pair 346\n",
            "Generated QA pair 347\n",
            "Generated QA pair 348\n",
            "Generated QA pair 349\n",
            "Generated QA pair 350\n",
            "Generated QA pair 351\n",
            "Generated QA pair 352\n",
            "Generated QA pair 353\n",
            "Generated QA pair 354\n",
            "Generated QA pair 355\n",
            "Generated QA pair 356\n",
            "Generated QA pair 357\n",
            "Generated QA pair 358\n",
            "Generated QA pair 359\n",
            "Generated QA pair 360\n",
            "Generated QA pair 361\n",
            "Generated QA pair 362\n",
            "Generated QA pair 363\n",
            "Generated QA pair 364\n",
            "Generated QA pair 365\n",
            "Generated QA pair 366\n",
            "Generated QA pair 367\n",
            "Generated QA pair 368\n",
            "Generated QA pair 369\n",
            "Generated QA pair 370\n",
            "Generated QA pair 371\n",
            "Generated QA pair 372\n",
            "Generated QA pair 373\n",
            "Generated QA pair 374\n",
            "Generated QA pair 375\n",
            "Generated QA pair 376\n",
            "Generated QA pair 377\n",
            "Generated QA pair 378\n",
            "Generated QA pair 379\n",
            "Generated QA pair 380\n",
            "Generated QA pair 381\n",
            "Generated QA pair 382\n",
            "Generated QA pair 383\n",
            "Generated QA pair 384\n",
            "Generated QA pair 385\n",
            "Generated QA pair 386\n",
            "Generated QA pair 387\n",
            "Generated QA pair 388\n",
            "Generated QA pair 389\n",
            "Generated QA pair 390\n",
            "Generated QA pair 391\n",
            "Generated QA pair 392\n",
            "Generated QA pair 393\n",
            "Generated QA pair 394\n",
            "Generated QA pair 395\n",
            "Generated QA pair 396\n",
            "Generated QA pair 397\n",
            "Generated QA pair 398\n",
            "Generated QA pair 399\n",
            "Generated QA pair 400\n",
            "Generated QA pair 401\n",
            "Generated QA pair 402\n",
            "Generated QA pair 403\n",
            "Generated QA pair 404\n",
            "Generated QA pair 405\n",
            "Generated QA pair 406\n",
            "Generated QA pair 407\n",
            "Generated QA pair 408\n",
            "Generated QA pair 409\n",
            "Generated QA pair 410\n",
            "Generated QA pair 411\n",
            "Generated QA pair 412\n",
            "Generated QA pair 413\n",
            "Generated QA pair 414\n",
            "Generated QA pair 415\n",
            "Generated QA pair 416\n",
            "Generated QA pair 417\n",
            "Generated QA pair 418\n",
            "Generated QA pair 419\n",
            "Generated QA pair 420\n",
            "Generated QA pair 421\n",
            "Generated QA pair 422\n",
            "Generated QA pair 423\n",
            "Generated QA pair 424\n",
            "Generated QA pair 425\n",
            "Generated QA pair 426\n",
            "Generated QA pair 427\n",
            "Generated QA pair 428\n",
            "Generated QA pair 429\n",
            "Generated QA pair 430\n",
            "Generated QA pair 431\n",
            "Generated QA pair 432\n",
            "Generated QA pair 433\n",
            "Generated QA pair 434\n",
            "Generated QA pair 435\n",
            "Generated QA pair 436\n",
            "Generated QA pair 437\n",
            "Generated QA pair 438\n",
            "Generated QA pair 439\n",
            "Generated QA pair 440\n",
            "Generated QA pair 441\n",
            "Generated QA pair 442\n",
            "Generated QA pair 443\n",
            "Generated QA pair 444\n",
            "Generated QA pair 445\n",
            "Generated QA pair 446\n",
            "Generated QA pair 447\n",
            "Generated QA pair 448\n",
            "Generated QA pair 449\n",
            "Generated QA pair 450\n",
            "Generated QA pair 451\n",
            "Generated QA pair 452\n",
            "Generated QA pair 453\n",
            "Generated QA pair 454\n",
            "Generated QA pair 455\n",
            "Generated QA pair 456\n",
            "Generated QA pair 457\n",
            "Generated QA pair 458\n",
            "Generated QA pair 459\n",
            "Generated QA pair 460\n",
            "Generated QA pair 461\n",
            "Generated QA pair 462\n",
            "Generated QA pair 463\n",
            "Generated QA pair 464\n",
            "Generated QA pair 465\n",
            "Generated QA pair 466\n",
            "Generated QA pair 467\n",
            "Generated QA pair 468\n",
            "Generated QA pair 469\n",
            "Generated QA pair 470\n",
            "Generated QA pair 471\n",
            "Generated QA pair 472\n",
            "Generated QA pair 473\n",
            "Generated QA pair 474\n",
            "Generated QA pair 475\n",
            "Generated QA pair 476\n",
            "Generated QA pair 477\n",
            "Generated QA pair 478\n",
            "Generated QA pair 479\n",
            "Generated QA pair 480\n",
            "Generated QA pair 481\n",
            "Generated QA pair 482\n",
            "Generated QA pair 483\n",
            "Generated QA pair 484\n",
            "Generated QA pair 485\n",
            "Generated QA pair 486\n",
            "Generated QA pair 487\n",
            "Generated QA pair 488\n",
            "Generated QA pair 489\n",
            "Generated QA pair 490\n",
            "Generated QA pair 491\n",
            "Generated QA pair 492\n",
            "Generated QA pair 493\n",
            "Generated QA pair 494\n",
            "Generated QA pair 495\n",
            "Generated QA pair 496\n",
            "Generated QA pair 497\n",
            "Generated QA pair 498\n",
            "Generated QA pair 499\n",
            "Generated QA pair 500\n",
            "Generated QA pair 501\n",
            "Generated QA pair 502\n",
            "Generated QA pair 503\n",
            "Generated QA pair 504\n",
            "Generated QA pair 505\n",
            "Generated QA pair 506\n",
            "Generated QA pair 507\n",
            "Generated QA pair 508\n",
            "Generated QA pair 509\n",
            "Generated QA pair 510\n",
            "Generated QA pair 511\n",
            "Generated QA pair 512\n",
            "Generated QA pair 513\n",
            "Generated QA pair 514\n",
            "Generated QA pair 515\n",
            "Generated QA pair 516\n",
            "Generated QA pair 517\n",
            "Generated QA pair 518\n",
            "Generated QA pair 519\n",
            "Generated QA pair 520\n",
            "Generated QA pair 521\n",
            "Generated QA pair 522\n",
            "Generated QA pair 523\n",
            "Generated QA pair 524\n",
            "Generated QA pair 525\n",
            "Generated QA pair 526\n",
            "Generated QA pair 527\n",
            "Generated QA pair 528\n",
            "Generated QA pair 529\n",
            "Generated QA pair 530\n",
            "Generated QA pair 531\n",
            "Generated QA pair 532\n",
            "Generated QA pair 533\n",
            "Generated QA pair 534\n",
            "Generated QA pair 535\n",
            "Generated QA pair 536\n",
            "Generated QA pair 537\n",
            "Generated QA pair 538\n",
            "Generated QA pair 539\n",
            "Generated QA pair 540\n",
            "Generated QA pair 541\n",
            "Generated QA pair 542\n",
            "Generated QA pair 543\n",
            "Generated QA pair 544\n",
            "Generated QA pair 545\n",
            "Generated QA pair 546\n",
            "Generated QA pair 547\n",
            "Generated QA pair 548\n",
            "Generated QA pair 549\n",
            "Generated QA pair 550\n",
            "Generated QA pair 551\n",
            "Generated QA pair 552\n",
            "Generated QA pair 553\n",
            "Generated QA pair 554\n",
            "Generated QA pair 555\n",
            "Generated QA pair 556\n",
            "Generated QA pair 557\n",
            "Generated QA pair 558\n",
            "Generated QA pair 559\n",
            "Generated QA pair 560\n",
            "Generated QA pair 561\n",
            "Generated QA pair 562\n",
            "Generated QA pair 563\n",
            "Generated QA pair 564\n",
            "Generated QA pair 565\n",
            "Generated QA pair 566\n",
            "Generated QA pair 567\n",
            "Generated QA pair 568\n",
            "Generated QA pair 569\n",
            "Generated QA pair 570\n",
            "Generated QA pair 571\n",
            "Generated QA pair 572\n",
            "Generated QA pair 573\n",
            "Generated QA pair 574\n",
            "Generated QA pair 575\n",
            "Generated QA pair 576\n",
            "Generated QA pair 577\n",
            "Generated QA pair 578\n",
            "Generated QA pair 579\n",
            "Generated QA pair 580\n",
            "Generated QA pair 581\n",
            "Generated QA pair 582\n",
            "Generated QA pair 583\n",
            "Generated QA pair 584\n",
            "Generated QA pair 585\n",
            "Generated QA pair 586\n",
            "Generated QA pair 587\n",
            "Generated QA pair 588\n",
            "Generated QA pair 589\n",
            "Generated QA pair 590\n",
            "Generated QA pair 591\n",
            "Generated QA pair 592\n",
            "Generated QA pair 593\n",
            "Generated QA pair 594\n",
            "Generated QA pair 595\n",
            "Generated QA pair 596\n",
            "Generated QA pair 597\n",
            "Generated QA pair 598\n",
            "Generated QA pair 599\n",
            "Generated QA pair 600\n",
            "Generated QA pair 601\n",
            "Generated QA pair 602\n",
            "Generated QA pair 603\n",
            "Generated QA pair 604\n",
            "Generated QA pair 605\n",
            "Generated QA pair 606\n",
            "Generated QA pair 607\n",
            "Generated QA pair 608\n",
            "Generated QA pair 609\n",
            "Generated QA pair 610\n",
            "Generated QA pair 611\n",
            "Generated QA pair 612\n",
            "Generated QA pair 613\n",
            "Generated QA pair 614\n",
            "Generated QA pair 615\n",
            "Generated QA pair 616\n",
            "Generated QA pair 617\n",
            "Generated QA pair 618\n",
            "Generated QA pair 619\n",
            "Generated QA pair 620\n",
            "Generated QA pair 621\n",
            "Generated QA pair 622\n",
            "Generated QA pair 623\n",
            "Generated QA pair 624\n",
            "Generated QA pair 625\n",
            "Generated QA pair 626\n",
            "Generated QA pair 627\n",
            "Generated QA pair 628\n",
            "Generated QA pair 629\n",
            "Generated QA pair 630\n",
            "Generated QA pair 631\n",
            "Generated QA pair 632\n",
            "Generated QA pair 633\n",
            "Generated QA pair 634\n",
            "Generated QA pair 635\n",
            "Generated QA pair 636\n",
            "Generated QA pair 637\n",
            "Generated QA pair 638\n",
            "Generated QA pair 639\n",
            "Generated QA pair 640\n",
            "Generated QA pair 641\n",
            "Generated QA pair 642\n",
            "Generated QA pair 643\n",
            "Generated QA pair 644\n",
            "Generated QA pair 645\n",
            "Generated QA pair 646\n",
            "Generated QA pair 647\n",
            "Generated QA pair 648\n",
            "Generated QA pair 649\n",
            "Generated QA pair 650\n",
            "Generated QA pair 651\n",
            "Generated QA pair 652\n",
            "Generated QA pair 653\n",
            "Generated QA pair 654\n",
            "Generated QA pair 655\n",
            "Generated QA pair 656\n",
            "Generated QA pair 657\n",
            "Generated QA pair 658\n",
            "Generated QA pair 659\n",
            "Generated QA pair 660\n",
            "Generated QA pair 661\n",
            "Generated QA pair 662\n",
            "Generated QA pair 663\n",
            "Generated QA pair 664\n",
            "Generated QA pair 665\n",
            "Generated QA pair 666\n",
            "Generated QA pair 667\n",
            "Generated QA pair 668\n",
            "Generated QA pair 669\n",
            "Generated QA pair 670\n",
            "Generated QA pair 671\n",
            "Generated QA pair 672\n",
            "Generated QA pair 673\n",
            "Generated QA pair 674\n",
            "Generated QA pair 675\n",
            "Generated QA pair 676\n",
            "Generated QA pair 677\n",
            "Generated QA pair 678\n",
            "Generated QA pair 679\n",
            "Generated QA pair 680\n",
            "Generated QA pair 681\n",
            "Generated QA pair 682\n",
            "Generated QA pair 683\n",
            "Generated QA pair 684\n",
            "Generated QA pair 685\n",
            "Generated QA pair 686\n",
            "Generated QA pair 687\n",
            "Generated QA pair 688\n",
            "Generated QA pair 689\n",
            "Generated QA pair 690\n",
            "Generated QA pair 691\n",
            "Generated QA pair 692\n",
            "Generated QA pair 693\n",
            "Generated QA pair 694\n",
            "Generated QA pair 695\n",
            "Generated QA pair 696\n",
            "Generated QA pair 697\n",
            "Generated QA pair 698\n",
            "Generated QA pair 699\n",
            "Generated QA pair 700\n",
            "Generated QA pair 701\n",
            "Generated QA pair 702\n",
            "Generated QA pair 703\n",
            "Generated QA pair 704\n",
            "Generated QA pair 705\n",
            "Generated QA pair 706\n",
            "Generated QA pair 707\n",
            "Generated QA pair 708\n",
            "Generated QA pair 709\n",
            "Generated QA pair 710\n",
            "Generated QA pair 711\n",
            "Generated QA pair 712\n",
            "Generated QA pair 713\n",
            "Generated QA pair 714\n",
            "Generated QA pair 715\n",
            "Generated QA pair 716\n",
            "Generated QA pair 717\n",
            "Generated QA pair 718\n",
            "Generated QA pair 719\n",
            "Generated QA pair 720\n",
            "Generated QA pair 721\n",
            "Generated QA pair 722\n",
            "Generated QA pair 723\n",
            "Generated QA pair 724\n",
            "Generated QA pair 725\n",
            "Generated QA pair 726\n",
            "Generated QA pair 727\n",
            "Generated QA pair 728\n",
            "Generated QA pair 729\n",
            "Generated QA pair 730\n",
            "Generated QA pair 731\n",
            "Generated QA pair 732\n",
            "Generated QA pair 733\n",
            "Generated QA pair 734\n",
            "Generated QA pair 735\n",
            "Generated QA pair 736\n",
            "Generated QA pair 737\n",
            "Generated QA pair 738\n",
            "Generated QA pair 739\n",
            "Generated QA pair 740\n",
            "Generated QA pair 741\n",
            "Generated QA pair 742\n",
            "Generated QA pair 743\n",
            "Generated QA pair 744\n",
            "Generated QA pair 745\n",
            "Generated QA pair 746\n",
            "Generated QA pair 747\n",
            "Generated QA pair 748\n",
            "Generated QA pair 749\n",
            "Generated QA pair 750\n",
            "Generated QA pair 751\n",
            "Generated QA pair 752\n",
            "Generated QA pair 753\n",
            "Generated QA pair 754\n",
            "Generated QA pair 755\n",
            "Generated QA pair 756\n",
            "Generated QA pair 757\n",
            "Generated QA pair 758\n",
            "Generated QA pair 759\n",
            "Generated QA pair 760\n",
            "Generated QA pair 761\n",
            "Generated QA pair 762\n",
            "Generated QA pair 763\n",
            "Generated QA pair 764\n",
            "Generated QA pair 765\n",
            "Generated QA pair 766\n",
            "Generated QA pair 767\n",
            "Generated QA pair 768\n",
            "Generated QA pair 769\n",
            "Generated QA pair 770\n",
            "Generated QA pair 771\n",
            "Generated QA pair 772\n",
            "Generated QA pair 773\n",
            "Generated QA pair 774\n",
            "Generated QA pair 775\n",
            "Generated QA pair 776\n",
            "Generated QA pair 777\n",
            "Generated QA pair 778\n",
            "Generated QA pair 779\n",
            "Generated QA pair 780\n",
            "Generated QA pair 781\n",
            "Generated QA pair 782\n",
            "Generated QA pair 783\n",
            "Generated QA pair 784\n",
            "Generated QA pair 785\n",
            "Generated QA pair 786\n",
            "Generated QA pair 787\n",
            "Generated QA pair 788\n",
            "Generated QA pair 789\n",
            "Generated QA pair 790\n",
            "Generated QA pair 791\n",
            "Generated QA pair 792\n",
            "Generated QA pair 793\n",
            "Generated QA pair 794\n",
            "Generated QA pair 795\n",
            "Generated QA pair 796\n",
            "Generated QA pair 797\n",
            "Generated QA pair 798\n",
            "Generated QA pair 799\n",
            "Generated QA pair 800\n",
            "Generated QA pair 801\n",
            "Generated QA pair 802\n",
            "Generated QA pair 803\n",
            "Generated QA pair 804\n",
            "Generated QA pair 805\n",
            "Generated QA pair 806\n",
            "Generated QA pair 807\n",
            "Generated QA pair 808\n",
            "Generated QA pair 809\n",
            "Generated QA pair 810\n",
            "Generated QA pair 811\n",
            "Generated QA pair 812\n",
            "Generated QA pair 813\n",
            "Generated QA pair 814\n",
            "Generated QA pair 815\n",
            "Generated QA pair 816\n",
            "Generated QA pair 817\n",
            "Generated QA pair 818\n",
            "Generated QA pair 819\n",
            "Generated QA pair 820\n",
            "Generated QA pair 821\n",
            "Generated QA pair 822\n",
            "Generated QA pair 823\n",
            "Generated QA pair 824\n",
            "Generated QA pair 825\n",
            "Generated QA pair 826\n",
            "Generated QA pair 827\n",
            "Generated QA pair 828\n",
            "Generated QA pair 829\n",
            "Generated QA pair 830\n",
            "Generated QA pair 831\n",
            "Generated QA pair 832\n",
            "Generated QA pair 833\n",
            "Generated QA pair 834\n",
            "Generated QA pair 835\n",
            "Generated QA pair 836\n",
            "Generated QA pair 837\n",
            "Generated QA pair 838\n",
            "Generated QA pair 839\n",
            "Generated QA pair 840\n",
            "Generated QA pair 841\n",
            "Generated QA pair 842\n",
            "Generated QA pair 843\n",
            "Generated QA pair 844\n",
            "Generated QA pair 845\n",
            "Generated QA pair 846\n",
            "Generated QA pair 847\n",
            "Generated QA pair 848\n",
            "Generated QA pair 849\n",
            "Generated QA pair 850\n",
            "Generated QA pair 851\n",
            "Generated QA pair 852\n",
            "Generated QA pair 853\n",
            "Generated QA pair 854\n",
            "Generated QA pair 855\n",
            "Generated QA pair 856\n",
            "Generated QA pair 857\n",
            "Generated QA pair 858\n",
            "Generated QA pair 859\n",
            "Generated QA pair 860\n",
            "Generated QA pair 861\n",
            "Generated QA pair 862\n",
            "Generated QA pair 863\n",
            "Generated QA pair 864\n",
            "Generated QA pair 865\n",
            "Generated QA pair 866\n",
            "Generated QA pair 867\n",
            "Generated QA pair 868\n",
            "Generated QA pair 869\n",
            "Generated QA pair 870\n",
            "Generated QA pair 871\n",
            "Generated QA pair 872\n",
            "Generated QA pair 873\n",
            "Generated QA pair 874\n",
            "Generated QA pair 875\n",
            "Generated QA pair 876\n",
            "Generated QA pair 877\n",
            "Generated QA pair 878\n",
            "Generated QA pair 879\n",
            "Generated QA pair 880\n",
            "Generated QA pair 881\n",
            "Generated QA pair 882\n",
            "Generated QA pair 883\n",
            "Generated QA pair 884\n",
            "Generated QA pair 885\n",
            "Generated QA pair 886\n",
            "Generated QA pair 887\n",
            "Generated QA pair 888\n",
            "Generated QA pair 889\n",
            "Generated QA pair 890\n",
            "Generated QA pair 891\n",
            "Generated QA pair 892\n",
            "Generated QA pair 893\n",
            "Generated QA pair 894\n",
            "Generated QA pair 895\n",
            "Generated QA pair 896\n",
            "Generated QA pair 897\n",
            "Generated QA pair 898\n",
            "Generated QA pair 899\n",
            "Generated QA pair 900\n",
            "Generated QA pair 901\n",
            "Generated QA pair 902\n",
            "Generated QA pair 903\n",
            "Generated QA pair 904\n",
            "Generated QA pair 905\n",
            "Generated QA pair 906\n",
            "Generated QA pair 907\n",
            "Generated QA pair 908\n",
            "Generated QA pair 909\n",
            "Generated QA pair 910\n",
            "Generated QA pair 911\n",
            "Generated QA pair 912\n",
            "Generated QA pair 913\n",
            "Generated QA pair 914\n",
            "Generated QA pair 915\n",
            "Generated QA pair 916\n",
            "Generated 916 QA pairs\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "\n",
        "target_pairs = 1000  # Set desired number\n",
        "qa_pairs = []\n",
        "current_count = 0\n",
        "\n",
        "for chunk in chunks:\n",
        "    if current_count >= target_pairs:\n",
        "        break\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-4o\",  # or \"gpt-4\"\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Generate ONE question and answer from technical text. Use format: Q: [question]\\nA: [answer]\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Text: {chunk[:2000]}\"}\n",
        "        ],\n",
        "        temperature=0.1,  # Lower for factual accuracy\n",
        "        max_tokens=256,\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        full_response = response.choices[0].message.content\n",
        "        q, a = full_response.split(\"A: \")\n",
        "        q = q.replace(\"Q: \", \"\").strip()\n",
        "        qa_pairs.append({\"question\": q, \"answer\": a.strip(), \"context\": chunk})\n",
        "        current_count += 1\n",
        "        print (f\"Generated QA pair {current_count}\")\n",
        "    except:\n",
        "        continue  # Skip malformed responses\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(qa_pairs)\n",
        "print(f\"Generated {len(df)} QA pairs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c4d9-vN6TzY"
      },
      "source": [
        "#### Generate Q&A Pairs **Prompt 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "dshDy2Jqs5EN"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "\n",
        "target_pairs = 2000\n",
        "qa_pairs = []\n",
        "current_count = 0\n",
        "failed_attempts = 0\n",
        "max_retries = 3\n",
        "\n",
        "# Improved prompt template\n",
        "system_prompt = \"\"\"You are an AI research assistant creating high-quality question-answer pairs from technical AI literature. Follow these rules:\n",
        "\n",
        "1. Generate **at least 3-5** DEEP, TECHNICAL questions per chunk.\n",
        "2. Answers must be:\n",
        "   - Comprehensive but concise (1-3 sentences)\n",
        "   - Contain technical details from the text\n",
        "3. Question types should include:\n",
        "   - Conceptual understanding\n",
        "   - Experimental results analysis\n",
        "   - Technical comparisons\n",
        "\n",
        "🚨 STRICT OUTPUT FORMAT:\n",
        "Q1: [question]\n",
        "A1: [answer]\n",
        "Q2: [question]\n",
        "A2: [answer]\n",
        "...\n",
        "DO NOT include explanations or unnecessary text.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Generated QA pair 1389/2000\n",
            "✅ Generated QA pair 1390/2000\n",
            "✅ Generated QA pair 1391/2000\n",
            "✅ Generated QA pair 1392/2000\n",
            "✅ Generated QA pair 1393/2000\n",
            "✅ Generated QA pair 1394/2000\n",
            "✅ Generated QA pair 1395/2000\n",
            "✅ Generated QA pair 1396/2000\n",
            "✅ Generated QA pair 1397/2000\n",
            "✅ Generated QA pair 1398/2000\n",
            "✅ Generated QA pair 1399/2000\n",
            "✅ Generated QA pair 1400/2000\n",
            "✅ Generated QA pair 1401/2000\n",
            "✅ Generated QA pair 1402/2000\n",
            "✅ Generated QA pair 1403/2000\n",
            "✅ Generated QA pair 1404/2000\n",
            "✅ Generated QA pair 1405/2000\n",
            "✅ Generated QA pair 1406/2000\n",
            "✅ Generated QA pair 1407/2000\n",
            "✅ Generated QA pair 1408/2000\n",
            "✅ Generated QA pair 1409/2000\n",
            "✅ Generated QA pair 1410/2000\n",
            "✅ Generated QA pair 1411/2000\n",
            "✅ Generated QA pair 1412/2000\n",
            "✅ Generated QA pair 1413/2000\n",
            "✅ Generated QA pair 1414/2000\n",
            "✅ Generated QA pair 1415/2000\n",
            "✅ Generated QA pair 1416/2000\n",
            "✅ Generated QA pair 1417/2000\n",
            "✅ Generated QA pair 1418/2000\n",
            "✅ Generated QA pair 1419/2000\n",
            "✅ Generated QA pair 1420/2000\n",
            "✅ Generated QA pair 1421/2000\n",
            "✅ Generated QA pair 1422/2000\n",
            "✅ Generated QA pair 1423/2000\n",
            "✅ Generated QA pair 1424/2000\n",
            "✅ Generated QA pair 1425/2000\n",
            "✅ Generated QA pair 1426/2000\n",
            "✅ Generated QA pair 1427/2000\n",
            "✅ Generated QA pair 1428/2000\n",
            "✅ Generated QA pair 1429/2000\n",
            "✅ Generated QA pair 1430/2000\n",
            "✅ Generated QA pair 1431/2000\n",
            "✅ Generated QA pair 1432/2000\n",
            "✅ Generated QA pair 1433/2000\n",
            "✅ Generated QA pair 1434/2000\n",
            "✅ Generated QA pair 1435/2000\n",
            "✅ Generated QA pair 1436/2000\n",
            "✅ Generated QA pair 1437/2000\n",
            "✅ Generated QA pair 1438/2000\n",
            "✅ Generated QA pair 1439/2000\n",
            "✅ Generated QA pair 1440/2000\n",
            "✅ Generated QA pair 1441/2000\n",
            "✅ Generated QA pair 1442/2000\n",
            "✅ Generated QA pair 1443/2000\n",
            "✅ Generated QA pair 1444/2000\n",
            "✅ Generated QA pair 1445/2000\n",
            "✅ Generated QA pair 1446/2000\n",
            "✅ Generated QA pair 1447/2000\n",
            "✅ Generated QA pair 1448/2000\n",
            "✅ Generated QA pair 1449/2000\n",
            "✅ Generated QA pair 1450/2000\n",
            "✅ Generated QA pair 1451/2000\n",
            "✅ Generated QA pair 1452/2000\n",
            "✅ Generated QA pair 1453/2000\n",
            "✅ Generated QA pair 1454/2000\n",
            "✅ Generated QA pair 1455/2000\n",
            "✅ Generated QA pair 1456/2000\n",
            "✅ Generated QA pair 1457/2000\n",
            "✅ Generated QA pair 1458/2000\n",
            "✅ Generated QA pair 1459/2000\n",
            "✅ Generated QA pair 1460/2000\n",
            "✅ Generated QA pair 1461/2000\n",
            "✅ Generated QA pair 1462/2000\n",
            "✅ Generated QA pair 1463/2000\n",
            "✅ Generated QA pair 1464/2000\n",
            "✅ Generated QA pair 1465/2000\n",
            "✅ Generated QA pair 1466/2000\n",
            "✅ Generated QA pair 1467/2000\n",
            "✅ Generated QA pair 1468/2000\n",
            "✅ Generated QA pair 1469/2000\n",
            "✅ Generated QA pair 1470/2000\n",
            "✅ Generated QA pair 1471/2000\n",
            "✅ Generated QA pair 1472/2000\n",
            "✅ Generated QA pair 1473/2000\n",
            "✅ Generated QA pair 1474/2000\n",
            "✅ Generated QA pair 1475/2000\n",
            "✅ Generated QA pair 1476/2000\n",
            "✅ Generated QA pair 1477/2000\n",
            "✅ Generated QA pair 1478/2000\n",
            "✅ Generated QA pair 1479/2000\n",
            "✅ Generated QA pair 1480/2000\n",
            "✅ Generated QA pair 1481/2000\n",
            "✅ Generated QA pair 1482/2000\n",
            "✅ Generated QA pair 1483/2000\n",
            "✅ Generated QA pair 1484/2000\n",
            "✅ Generated QA pair 1485/2000\n",
            "✅ Generated QA pair 1486/2000\n",
            "✅ Generated QA pair 1487/2000\n",
            "✅ Generated QA pair 1488/2000\n",
            "✅ Generated QA pair 1489/2000\n",
            "✅ Generated QA pair 1490/2000\n",
            "✅ Generated QA pair 1491/2000\n",
            "✅ Generated QA pair 1492/2000\n",
            "✅ Generated QA pair 1493/2000\n",
            "✅ Generated QA pair 1494/2000\n",
            "✅ Generated QA pair 1495/2000\n",
            "✅ Generated QA pair 1496/2000\n",
            "✅ Generated QA pair 1497/2000\n",
            "✅ Generated QA pair 1498/2000\n",
            "✅ Generated QA pair 1499/2000\n",
            "✅ Generated QA pair 1500/2000\n",
            "✅ Generated QA pair 1501/2000\n",
            "✅ Generated QA pair 1502/2000\n",
            "✅ Generated QA pair 1503/2000\n",
            "✅ Generated QA pair 1504/2000\n",
            "✅ Generated QA pair 1505/2000\n",
            "✅ Generated QA pair 1506/2000\n",
            "✅ Generated QA pair 1507/2000\n",
            "✅ Generated QA pair 1508/2000\n",
            "✅ Generated QA pair 1509/2000\n",
            "✅ Generated QA pair 1510/2000\n",
            "✅ Generated QA pair 1511/2000\n",
            "✅ Generated QA pair 1512/2000\n",
            "✅ Generated QA pair 1513/2000\n",
            "✅ Generated QA pair 1514/2000\n",
            "✅ Generated QA pair 1515/2000\n",
            "✅ Generated QA pair 1516/2000\n",
            "✅ Generated QA pair 1517/2000\n",
            "✅ Generated QA pair 1518/2000\n",
            "✅ Generated QA pair 1519/2000\n",
            "✅ Generated QA pair 1520/2000\n",
            "✅ Generated QA pair 1521/2000\n",
            "✅ Generated QA pair 1522/2000\n",
            "✅ Generated QA pair 1523/2000\n",
            "✅ Generated QA pair 1524/2000\n",
            "✅ Generated QA pair 1525/2000\n",
            "✅ Generated QA pair 1526/2000\n",
            "✅ Generated QA pair 1527/2000\n",
            "✅ Generated QA pair 1528/2000\n",
            "✅ Generated QA pair 1529/2000\n",
            "✅ Generated QA pair 1530/2000\n",
            "✅ Generated QA pair 1531/2000\n",
            "✅ Generated QA pair 1532/2000\n",
            "✅ Generated QA pair 1533/2000\n",
            "✅ Generated QA pair 1534/2000\n",
            "✅ Generated QA pair 1535/2000\n",
            "✅ Generated QA pair 1536/2000\n",
            "✅ Generated QA pair 1537/2000\n",
            "✅ Generated QA pair 1538/2000\n",
            "✅ Generated QA pair 1539/2000\n",
            "✅ Generated QA pair 1540/2000\n",
            "✅ Generated QA pair 1541/2000\n",
            "✅ Generated QA pair 1542/2000\n",
            "✅ Generated QA pair 1543/2000\n",
            "✅ Generated QA pair 1544/2000\n",
            "✅ Generated QA pair 1545/2000\n",
            "✅ Generated QA pair 1546/2000\n",
            "✅ Generated QA pair 1547/2000\n",
            "✅ Generated QA pair 1548/2000\n",
            "✅ Generated QA pair 1549/2000\n",
            "✅ Generated QA pair 1550/2000\n",
            "✅ Generated QA pair 1551/2000\n",
            "✅ Generated QA pair 1552/2000\n",
            "✅ Generated QA pair 1553/2000\n",
            "✅ Generated QA pair 1554/2000\n",
            "✅ Generated QA pair 1555/2000\n",
            "✅ Generated QA pair 1556/2000\n",
            "✅ Generated QA pair 1557/2000\n",
            "✅ Generated QA pair 1558/2000\n",
            "✅ Generated QA pair 1559/2000\n",
            "✅ Generated QA pair 1560/2000\n",
            "✅ Generated QA pair 1561/2000\n",
            "✅ Generated QA pair 1562/2000\n",
            "✅ Generated QA pair 1563/2000\n",
            "✅ Generated QA pair 1564/2000\n",
            "✅ Generated QA pair 1565/2000\n",
            "✅ Generated QA pair 1566/2000\n",
            "✅ Generated QA pair 1567/2000\n",
            "✅ Generated QA pair 1568/2000\n",
            "✅ Generated QA pair 1569/2000\n",
            "✅ Generated QA pair 1570/2000\n",
            "✅ Generated QA pair 1571/2000\n",
            "✅ Generated QA pair 1572/2000\n",
            "✅ Generated QA pair 1573/2000\n",
            "✅ Generated QA pair 1574/2000\n",
            "✅ Generated QA pair 1575/2000\n",
            "✅ Generated QA pair 1576/2000\n",
            "✅ Generated QA pair 1577/2000\n",
            "✅ Generated QA pair 1578/2000\n",
            "✅ Generated QA pair 1579/2000\n",
            "✅ Generated QA pair 1580/2000\n",
            "✅ Generated QA pair 1581/2000\n",
            "✅ Generated QA pair 1582/2000\n",
            "✅ Generated QA pair 1583/2000\n",
            "✅ Generated QA pair 1584/2000\n",
            "✅ Generated QA pair 1585/2000\n",
            "✅ Generated QA pair 1586/2000\n",
            "✅ Generated QA pair 1587/2000\n",
            "✅ Generated QA pair 1588/2000\n",
            "✅ Generated QA pair 1589/2000\n",
            "✅ Generated QA pair 1590/2000\n",
            "✅ Generated QA pair 1591/2000\n",
            "✅ Generated QA pair 1592/2000\n",
            "✅ Generated QA pair 1593/2000\n",
            "✅ Generated QA pair 1594/2000\n",
            "✅ Generated QA pair 1595/2000\n",
            "✅ Generated QA pair 1596/2000\n",
            "✅ Generated QA pair 1597/2000\n",
            "✅ Generated QA pair 1598/2000\n",
            "✅ Generated QA pair 1599/2000\n",
            "✅ Generated QA pair 1600/2000\n",
            "✅ Generated QA pair 1601/2000\n",
            "✅ Generated QA pair 1602/2000\n",
            "✅ Generated QA pair 1603/2000\n",
            "✅ Generated QA pair 1604/2000\n",
            "✅ Generated QA pair 1605/2000\n",
            "✅ Generated QA pair 1606/2000\n",
            "✅ Generated QA pair 1607/2000\n",
            "✅ Generated QA pair 1608/2000\n",
            "✅ Generated QA pair 1609/2000\n",
            "✅ Generated QA pair 1610/2000\n",
            "✅ Generated QA pair 1611/2000\n",
            "✅ Generated QA pair 1612/2000\n",
            "✅ Generated QA pair 1613/2000\n",
            "✅ Generated QA pair 1614/2000\n",
            "✅ Generated QA pair 1615/2000\n",
            "✅ Generated QA pair 1616/2000\n",
            "✅ Generated QA pair 1617/2000\n",
            "✅ Generated QA pair 1618/2000\n",
            "✅ Generated QA pair 1619/2000\n",
            "✅ Generated QA pair 1620/2000\n",
            "✅ Generated QA pair 1621/2000\n",
            "✅ Generated QA pair 1622/2000\n",
            "✅ Generated QA pair 1623/2000\n",
            "✅ Generated QA pair 1624/2000\n",
            "✅ Generated QA pair 1625/2000\n",
            "✅ Generated QA pair 1626/2000\n",
            "✅ Generated QA pair 1627/2000\n",
            "✅ Generated QA pair 1628/2000\n",
            "✅ Generated QA pair 1629/2000\n",
            "✅ Generated QA pair 1630/2000\n",
            "✅ Generated QA pair 1631/2000\n",
            "✅ Generated QA pair 1632/2000\n",
            "✅ Generated QA pair 1633/2000\n",
            "✅ Generated QA pair 1634/2000\n",
            "✅ Generated QA pair 1635/2000\n",
            "✅ Generated QA pair 1636/2000\n",
            "✅ Generated QA pair 1637/2000\n",
            "✅ Generated QA pair 1638/2000\n",
            "✅ Generated QA pair 1639/2000\n",
            "✅ Generated QA pair 1640/2000\n",
            "✅ Generated QA pair 1641/2000\n",
            "✅ Generated QA pair 1642/2000\n",
            "✅ Generated QA pair 1643/2000\n",
            "✅ Generated QA pair 1644/2000\n",
            "✅ Generated QA pair 1645/2000\n",
            "✅ Generated QA pair 1646/2000\n",
            "✅ Generated QA pair 1647/2000\n",
            "✅ Generated QA pair 1648/2000\n",
            "✅ Generated QA pair 1649/2000\n",
            "✅ Generated QA pair 1650/2000\n",
            "✅ Generated QA pair 1651/2000\n",
            "✅ Generated QA pair 1652/2000\n",
            "✅ Generated QA pair 1653/2000\n",
            "✅ Generated QA pair 1654/2000\n",
            "✅ Generated QA pair 1655/2000\n",
            "✅ Generated QA pair 1656/2000\n",
            "✅ Generated QA pair 1657/2000\n",
            "✅ Generated QA pair 1658/2000\n",
            "✅ Generated QA pair 1659/2000\n",
            "✅ Generated QA pair 1660/2000\n",
            "✅ Generated QA pair 1661/2000\n",
            "✅ Generated QA pair 1662/2000\n",
            "✅ Generated QA pair 1663/2000\n",
            "✅ Generated QA pair 1664/2000\n",
            "✅ Generated QA pair 1665/2000\n",
            "✅ Generated QA pair 1666/2000\n",
            "✅ Generated QA pair 1667/2000\n",
            "✅ Generated QA pair 1668/2000\n",
            "✅ Generated QA pair 1669/2000\n",
            "✅ Generated QA pair 1670/2000\n",
            "✅ Generated QA pair 1671/2000\n",
            "✅ Generated QA pair 1672/2000\n",
            "✅ Generated QA pair 1673/2000\n",
            "✅ Generated QA pair 1674/2000\n",
            "✅ Generated QA pair 1675/2000\n",
            "✅ Generated QA pair 1676/2000\n",
            "✅ Generated QA pair 1677/2000\n",
            "✅ Generated QA pair 1678/2000\n",
            "✅ Generated QA pair 1679/2000\n",
            "✅ Generated QA pair 1680/2000\n",
            "✅ Generated QA pair 1681/2000\n",
            "✅ Generated QA pair 1682/2000\n",
            "✅ Generated QA pair 1683/2000\n",
            "✅ Generated QA pair 1684/2000\n",
            "✅ Generated QA pair 1685/2000\n",
            "✅ Generated QA pair 1686/2000\n",
            "✅ Generated QA pair 1687/2000\n",
            "✅ Generated QA pair 1688/2000\n",
            "✅ Generated QA pair 1689/2000\n",
            "✅ Generated QA pair 1690/2000\n",
            "✅ Generated QA pair 1691/2000\n",
            "✅ Generated QA pair 1692/2000\n",
            "✅ Generated QA pair 1693/2000\n",
            "✅ Generated QA pair 1694/2000\n",
            "✅ Generated QA pair 1695/2000\n",
            "✅ Generated QA pair 1696/2000\n",
            "✅ Generated QA pair 1697/2000\n",
            "✅ Generated QA pair 1698/2000\n",
            "✅ Generated QA pair 1699/2000\n",
            "✅ Generated QA pair 1700/2000\n",
            "✅ Generated QA pair 1701/2000\n",
            "✅ Generated QA pair 1702/2000\n",
            "✅ Generated QA pair 1703/2000\n",
            "✅ Generated QA pair 1704/2000\n",
            "✅ Generated QA pair 1705/2000\n",
            "✅ Generated QA pair 1706/2000\n",
            "✅ Generated QA pair 1707/2000\n",
            "✅ Generated QA pair 1708/2000\n",
            "✅ Generated QA pair 1709/2000\n",
            "✅ Generated QA pair 1710/2000\n",
            "✅ Generated QA pair 1711/2000\n",
            "✅ Generated QA pair 1712/2000\n",
            "✅ Generated QA pair 1713/2000\n",
            "✅ Generated QA pair 1714/2000\n",
            "✅ Generated QA pair 1715/2000\n",
            "✅ Generated QA pair 1716/2000\n",
            "✅ Generated QA pair 1717/2000\n",
            "✅ Generated QA pair 1718/2000\n",
            "✅ Generated QA pair 1719/2000\n",
            "✅ Generated QA pair 1720/2000\n",
            "✅ Generated QA pair 1721/2000\n",
            "✅ Generated QA pair 1722/2000\n",
            "✅ Generated QA pair 1723/2000\n",
            "✅ Generated QA pair 1724/2000\n",
            "✅ Generated QA pair 1725/2000\n",
            "✅ Generated QA pair 1726/2000\n",
            "✅ Generated QA pair 1727/2000\n",
            "✅ Generated QA pair 1728/2000\n",
            "✅ Generated QA pair 1729/2000\n",
            "✅ Generated QA pair 1730/2000\n",
            "✅ Generated QA pair 1731/2000\n",
            "✅ Generated QA pair 1732/2000\n",
            "✅ Generated QA pair 1733/2000\n",
            "✅ Generated QA pair 1734/2000\n",
            "✅ Generated QA pair 1735/2000\n",
            "✅ Generated QA pair 1736/2000\n",
            "✅ Generated QA pair 1737/2000\n",
            "✅ Generated QA pair 1738/2000\n",
            "✅ Generated QA pair 1739/2000\n",
            "✅ Generated QA pair 1740/2000\n",
            "✅ Generated QA pair 1741/2000\n",
            "✅ Generated QA pair 1742/2000\n",
            "✅ Generated QA pair 1743/2000\n",
            "✅ Generated QA pair 1744/2000\n",
            "✅ Generated QA pair 1745/2000\n",
            "✅ Generated QA pair 1746/2000\n",
            "✅ Generated QA pair 1747/2000\n",
            "✅ Generated QA pair 1748/2000\n",
            "✅ Generated QA pair 1749/2000\n",
            "✅ Generated QA pair 1750/2000\n",
            "✅ Generated QA pair 1751/2000\n",
            "✅ Generated QA pair 1752/2000\n",
            "✅ Generated QA pair 1753/2000\n",
            "✅ Generated QA pair 1754/2000\n",
            "✅ Generated QA pair 1755/2000\n",
            "✅ Generated QA pair 1756/2000\n",
            "✅ Generated QA pair 1757/2000\n",
            "✅ Generated QA pair 1758/2000\n",
            "✅ Generated QA pair 1759/2000\n",
            "✅ Generated QA pair 1760/2000\n",
            "✅ Generated QA pair 1761/2000\n",
            "✅ Generated QA pair 1762/2000\n",
            "✅ Generated QA pair 1763/2000\n",
            "✅ Generated QA pair 1764/2000\n",
            "✅ Generated QA pair 1765/2000\n",
            "✅ Generated QA pair 1766/2000\n",
            "✅ Generated QA pair 1767/2000\n",
            "✅ Generated QA pair 1768/2000\n",
            "✅ Generated QA pair 1769/2000\n",
            "✅ Generated QA pair 1770/2000\n",
            "✅ Generated QA pair 1771/2000\n",
            "✅ Generated QA pair 1772/2000\n",
            "✅ Generated QA pair 1773/2000\n",
            "✅ Generated QA pair 1774/2000\n",
            "✅ Generated QA pair 1775/2000\n",
            "✅ Generated QA pair 1776/2000\n",
            "✅ Generated QA pair 1777/2000\n",
            "✅ Generated QA pair 1778/2000\n",
            "✅ Generated QA pair 1779/2000\n",
            "✅ Generated QA pair 1780/2000\n",
            "✅ Generated QA pair 1781/2000\n",
            "✅ Generated QA pair 1782/2000\n",
            "✅ Generated QA pair 1783/2000\n",
            "✅ Generated QA pair 1784/2000\n",
            "✅ Generated QA pair 1785/2000\n",
            "✅ Generated QA pair 1786/2000\n",
            "✅ Generated QA pair 1787/2000\n",
            "✅ Generated QA pair 1788/2000\n",
            "✅ Generated QA pair 1789/2000\n",
            "✅ Generated QA pair 1790/2000\n",
            "✅ Generated QA pair 1791/2000\n",
            "✅ Generated QA pair 1792/2000\n",
            "✅ Generated QA pair 1793/2000\n",
            "✅ Generated QA pair 1794/2000\n",
            "✅ Generated QA pair 1795/2000\n",
            "✅ Generated QA pair 1796/2000\n",
            "✅ Generated QA pair 1797/2000\n",
            "✅ Generated QA pair 1798/2000\n",
            "✅ Generated QA pair 1799/2000\n",
            "✅ Generated QA pair 1800/2000\n",
            "✅ Generated QA pair 1801/2000\n",
            "✅ Generated QA pair 1802/2000\n",
            "✅ Generated QA pair 1803/2000\n",
            "✅ Generated QA pair 1804/2000\n",
            "✅ Generated QA pair 1805/2000\n",
            "✅ Generated QA pair 1806/2000\n",
            "✅ Generated QA pair 1807/2000\n",
            "✅ Generated QA pair 1808/2000\n",
            "✅ Generated QA pair 1809/2000\n",
            "✅ Generated QA pair 1810/2000\n",
            "✅ Generated QA pair 1811/2000\n",
            "✅ Generated QA pair 1812/2000\n",
            "✅ Generated QA pair 1813/2000\n",
            "✅ Generated QA pair 1814/2000\n",
            "✅ Generated QA pair 1815/2000\n",
            "✅ Generated QA pair 1816/2000\n",
            "✅ Generated QA pair 1817/2000\n",
            "✅ Generated QA pair 1818/2000\n",
            "✅ Generated QA pair 1819/2000\n",
            "✅ Generated QA pair 1820/2000\n",
            "✅ Generated QA pair 1821/2000\n",
            "✅ Generated QA pair 1822/2000\n",
            "✅ Generated QA pair 1823/2000\n",
            "✅ Generated QA pair 1824/2000\n",
            "✅ Generated QA pair 1825/2000\n",
            "✅ Generated QA pair 1826/2000\n",
            "✅ Generated QA pair 1827/2000\n",
            "✅ Generated QA pair 1828/2000\n",
            "✅ Generated QA pair 1829/2000\n",
            "✅ Generated QA pair 1830/2000\n",
            "✅ Generated QA pair 1831/2000\n",
            "✅ Generated QA pair 1832/2000\n",
            "✅ Generated QA pair 1833/2000\n",
            "✅ Generated QA pair 1834/2000\n",
            "✅ Generated QA pair 1835/2000\n",
            "✅ Generated QA pair 1836/2000\n",
            "✅ Generated QA pair 1837/2000\n",
            "✅ Generated QA pair 1838/2000\n",
            "✅ Generated QA pair 1839/2000\n",
            "✅ Generated QA pair 1840/2000\n",
            "✅ Generated QA pair 1841/2000\n",
            "✅ Generated QA pair 1842/2000\n",
            "✅ Generated QA pair 1843/2000\n",
            "✅ Generated QA pair 1844/2000\n",
            "✅ Generated QA pair 1845/2000\n",
            "✅ Generated QA pair 1846/2000\n",
            "✅ Generated QA pair 1847/2000\n",
            "✅ Generated QA pair 1848/2000\n",
            "✅ Generated QA pair 1849/2000\n",
            "✅ Generated QA pair 1850/2000\n",
            "✅ Generated QA pair 1851/2000\n",
            "✅ Generated QA pair 1852/2000\n",
            "✅ Generated QA pair 1853/2000\n",
            "✅ Generated QA pair 1854/2000\n",
            "✅ Generated QA pair 1855/2000\n",
            "✅ Generated QA pair 1856/2000\n",
            "✅ Generated QA pair 1857/2000\n",
            "✅ Generated QA pair 1858/2000\n",
            "✅ Generated QA pair 1859/2000\n",
            "✅ Generated QA pair 1860/2000\n",
            "✅ Generated QA pair 1861/2000\n",
            "✅ Generated QA pair 1862/2000\n",
            "✅ Generated QA pair 1863/2000\n",
            "✅ Generated QA pair 1864/2000\n",
            "✅ Generated QA pair 1865/2000\n",
            "✅ Generated QA pair 1866/2000\n",
            "✅ Generated QA pair 1867/2000\n",
            "✅ Generated QA pair 1868/2000\n",
            "✅ Generated QA pair 1869/2000\n",
            "✅ Generated QA pair 1870/2000\n",
            "✅ Generated QA pair 1871/2000\n",
            "✅ Generated QA pair 1872/2000\n",
            "✅ Generated QA pair 1873/2000\n",
            "✅ Generated QA pair 1874/2000\n",
            "✅ Generated QA pair 1875/2000\n",
            "✅ Generated QA pair 1876/2000\n",
            "✅ Generated QA pair 1877/2000\n",
            "✅ Generated QA pair 1878/2000\n",
            "✅ Generated QA pair 1879/2000\n",
            "✅ Generated QA pair 1880/2000\n",
            "✅ Generated QA pair 1881/2000\n",
            "✅ Generated QA pair 1882/2000\n",
            "✅ Generated QA pair 1883/2000\n",
            "✅ Generated QA pair 1884/2000\n",
            "✅ Generated QA pair 1885/2000\n",
            "✅ Generated QA pair 1886/2000\n",
            "✅ Generated QA pair 1887/2000\n",
            "✅ Generated QA pair 1888/2000\n",
            "✅ Generated QA pair 1889/2000\n",
            "✅ Generated QA pair 1890/2000\n",
            "✅ Generated QA pair 1891/2000\n",
            "✅ Generated QA pair 1892/2000\n",
            "✅ Generated QA pair 1893/2000\n",
            "✅ Generated QA pair 1894/2000\n",
            "✅ Generated QA pair 1895/2000\n",
            "✅ Generated QA pair 1896/2000\n",
            "✅ Generated QA pair 1897/2000\n",
            "✅ Generated QA pair 1898/2000\n",
            "✅ Generated QA pair 1899/2000\n",
            "✅ Generated QA pair 1900/2000\n",
            "✅ Generated QA pair 1901/2000\n",
            "✅ Generated QA pair 1902/2000\n",
            "✅ Generated QA pair 1903/2000\n",
            "✅ Generated QA pair 1904/2000\n",
            "✅ Generated QA pair 1905/2000\n",
            "✅ Generated QA pair 1906/2000\n",
            "✅ Generated QA pair 1907/2000\n",
            "✅ Generated QA pair 1908/2000\n",
            "✅ Generated QA pair 1909/2000\n",
            "✅ Generated QA pair 1910/2000\n",
            "✅ Generated QA pair 1911/2000\n",
            "✅ Generated QA pair 1912/2000\n",
            "✅ Generated QA pair 1913/2000\n",
            "✅ Generated QA pair 1914/2000\n",
            "✅ Generated QA pair 1915/2000\n",
            "✅ Generated QA pair 1916/2000\n",
            "✅ Generated QA pair 1917/2000\n",
            "✅ Generated QA pair 1918/2000\n",
            "✅ Generated QA pair 1919/2000\n",
            "✅ Generated QA pair 1920/2000\n",
            "✅ Generated QA pair 1921/2000\n",
            "✅ Generated QA pair 1922/2000\n",
            "✅ Generated QA pair 1923/2000\n",
            "✅ Generated QA pair 1924/2000\n",
            "✅ Generated QA pair 1925/2000\n",
            "✅ Generated QA pair 1926/2000\n",
            "✅ Generated QA pair 1927/2000\n",
            "✅ Generated QA pair 1928/2000\n",
            "✅ Generated QA pair 1929/2000\n",
            "✅ Generated QA pair 1930/2000\n",
            "✅ Generated QA pair 1931/2000\n",
            "✅ Generated QA pair 1932/2000\n",
            "✅ Generated QA pair 1933/2000\n",
            "✅ Generated QA pair 1934/2000\n",
            "✅ Generated QA pair 1935/2000\n",
            "✅ Generated QA pair 1936/2000\n",
            "✅ Generated QA pair 1937/2000\n",
            "✅ Generated QA pair 1938/2000\n",
            "✅ Generated QA pair 1939/2000\n",
            "✅ Generated QA pair 1940/2000\n",
            "✅ Generated QA pair 1941/2000\n",
            "✅ Generated QA pair 1942/2000\n",
            "✅ Generated QA pair 1943/2000\n",
            "✅ Generated QA pair 1944/2000\n",
            "✅ Generated QA pair 1945/2000\n",
            "✅ Generated QA pair 1946/2000\n",
            "✅ Generated QA pair 1947/2000\n",
            "✅ Generated QA pair 1948/2000\n",
            "✅ Generated QA pair 1949/2000\n",
            "✅ Generated QA pair 1950/2000\n",
            "✅ Generated QA pair 1951/2000\n",
            "✅ Generated QA pair 1952/2000\n",
            "✅ Generated QA pair 1953/2000\n",
            "✅ Generated QA pair 1954/2000\n",
            "✅ Generated QA pair 1955/2000\n",
            "✅ Generated QA pair 1956/2000\n",
            "✅ Generated QA pair 1957/2000\n",
            "✅ Generated QA pair 1958/2000\n",
            "✅ Generated QA pair 1959/2000\n",
            "✅ Generated QA pair 1960/2000\n",
            "✅ Generated QA pair 1961/2000\n",
            "✅ Generated QA pair 1962/2000\n",
            "✅ Generated QA pair 1963/2000\n",
            "✅ Generated QA pair 1964/2000\n",
            "✅ Generated QA pair 1965/2000\n",
            "✅ Generated QA pair 1966/2000\n",
            "✅ Generated QA pair 1967/2000\n",
            "✅ Generated QA pair 1968/2000\n",
            "✅ Generated QA pair 1969/2000\n",
            "✅ Generated QA pair 1970/2000\n",
            "✅ Generated QA pair 1971/2000\n",
            "✅ Generated QA pair 1972/2000\n",
            "✅ Generated QA pair 1973/2000\n",
            "✅ Generated QA pair 1974/2000\n",
            "✅ Generated QA pair 1975/2000\n",
            "✅ Generated QA pair 1976/2000\n",
            "✅ Generated QA pair 1977/2000\n",
            "✅ Generated QA pair 1978/2000\n",
            "✅ Generated QA pair 1979/2000\n",
            "✅ Generated QA pair 1980/2000\n",
            "✅ Generated QA pair 1981/2000\n",
            "✅ Generated QA pair 1982/2000\n",
            "✅ Generated QA pair 1983/2000\n",
            "✅ Generated QA pair 1984/2000\n",
            "✅ Generated QA pair 1985/2000\n",
            "✅ Generated QA pair 1986/2000\n",
            "✅ Generated QA pair 1987/2000\n",
            "✅ Generated QA pair 1988/2000\n",
            "✅ Generated QA pair 1989/2000\n",
            "✅ Generated QA pair 1990/2000\n",
            "✅ Generated QA pair 1991/2000\n",
            "✅ Generated QA pair 1992/2000\n",
            "✅ Generated QA pair 1993/2000\n",
            "✅ Generated QA pair 1994/2000\n",
            "✅ Generated QA pair 1995/2000\n",
            "✅ Generated QA pair 1996/2000\n",
            "✅ Generated QA pair 1997/2000\n",
            "✅ Generated QA pair 1998/2000\n",
            "✅ Generated QA pair 1999/2000\n",
            "✅ Generated QA pair 2000/2000\n",
            "✅ Generation complete. Final dataset: 2000 QA pairs\n"
          ]
        }
      ],
      "source": [
        "for chunk in chunks:\n",
        "    if current_count >= target_pairs:\n",
        "        break  # Stop if we have reached the target pairs\n",
        "\n",
        "    # Preprocess chunk to preserve technical content\n",
        "    chunk = chunk[:4000].replace(\"\\n\", \" \")  # Remove newlines but keep other formatting\n",
        "    chunk = re.sub(r'\\s+', ' ', chunk).strip()  # Collapse multiple spaces\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            # API call with GPT-4o for question generation\n",
        "            response = openai.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": f\"Text excerpt from AI research paper:\\n{chunk}\"}\n",
        "                ],\n",
        "                temperature=0.3,  # Controlled randomness for better phrasing\n",
        "                max_tokens=1500,  # Allow longer technical answers\n",
        "                top_p=0.95,\n",
        "                frequency_penalty=0.2,\n",
        "                presence_penalty=0.1\n",
        "            )\n",
        "\n",
        "            full_response = response.choices[0].message.content\n",
        "\n",
        "            # Extract multiple Q&A pairs using regex\n",
        "            matches = re.findall(r\"Q\\d*:\\s*(.*?)\\nA\\d*:\\s*(.*?)(?=\\nQ\\d*:|\\Z)\", full_response, re.DOTALL)\n",
        "\n",
        "            if matches:\n",
        "                for q, a in matches:\n",
        "                    qa_pairs.append({\n",
        "                        \"question\": q.strip(),\n",
        "                        \"answer\": a.strip(),\n",
        "                        \"context\": chunk[:500] + \"...\"  # Store only first 500 chars to reduce redundancy\n",
        "                    })\n",
        "                    current_count += 1\n",
        "\n",
        "                    print(f\"✅ Generated QA pair {current_count}/{target_pairs}\")\n",
        "\n",
        "                    # Stop processing this chunk if we reach target pairs\n",
        "                    if current_count >= target_pairs:\n",
        "                        break\n",
        "\n",
        "                failed_attempts = 0  # Reset failures after success\n",
        "                break  # Stop retrying if we succeeded\n",
        "\n",
        "            else:\n",
        "                raise ValueError(\"Invalid format detected\")\n",
        "\n",
        "        except openai.RateLimitError:\n",
        "            wait_time = 2 ** attempt  # Exponential backoff\n",
        "            print(f\"⚠️ Rate limit reached. Retrying in {wait_time} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "\n",
        "        except Exception as e:\n",
        "            failed_attempts += 1\n",
        "            print(f\"⚠️ Retry {attempt+1} for chunk. Error: {str(e)}\")\n",
        "\n",
        "            # If too many failures, skip the chunk and reset failure count\n",
        "            if failed_attempts > 20:\n",
        "                print(\"⚠️ Skipping this chunk after 20 failures.\")\n",
        "                failed_attempts = 0\n",
        "                break\n",
        "\n",
        "# Convert to DataFrame with metadata\n",
        "df = pd.DataFrame(qa_pairs)\n",
        "\n",
        "print(f\"✅ Generation complete. Final dataset: {len(df)} QA pairs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZaB8ZTI6XUg"
      },
      "source": [
        "#### Save Q&A Pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "shRJjXS0yhoI"
      },
      "outputs": [],
      "source": [
        "df.to_csv(os.path.join(OUTPUT_FOLDER, \"qa_pairs2.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                               question  \\\n",
            "0     What are the potential societal benefits of us...   \n",
            "1     How does the paper \"Program synthesis with lar...   \n",
            "2     What is the focus of the research by Chen et a...   \n",
            "3     Describe the approach taken by Chen et al. in ...   \n",
            "4     What is the significance of teaching large lan...   \n",
            "...                                                 ...   \n",
            "1995  What framework is SARA built on, and what is i...   \n",
            "1996  Which datasets are used to evaluate the method...   \n",
            "1997  How does SARA's performance compare to other b...   \n",
            "1998  What evaluation metrics are used for HotpotQA,...   \n",
            "1999  What problem does the proposed framework aim t...   \n",
            "\n",
            "                                                 answer  \\\n",
            "0     The potential societal benefits include creati...   \n",
            "1     The paper explores the use of large language m...   \n",
            "2     The research focuses on improving code generat...   \n",
            "3     The approach involves leveraging a divide-and-...   \n",
            "4     Teaching large language models to self-debug c...   \n",
            "...                                                 ...   \n",
            "1995  SARA is built on the open-source multi-agent f...   \n",
            "1996  The datasets used are HotpotQA for multi-hop q...   \n",
            "1997  SARA consistently outperforms all baseline met...   \n",
            "1998  For HotpotQA, a GPT-4 judge assesses answer co...   \n",
            "1999  The proposed framework addresses the \"when to ...   \n",
            "\n",
            "                                                context  \n",
            "0     orithmic domains, without any fine-tuning or r...  \n",
            "1     orithmic domains, without any fine-tuning or r...  \n",
            "2     orithmic domains, without any fine-tuning or r...  \n",
            "3     orithmic domains, without any fine-tuning or r...  \n",
            "4     orithmic domains, without any fine-tuning or r...  \n",
            "...                                                 ...  \n",
            "1995  nt and Retrieval Agent). Four representative L...  \n",
            "1996  nt and Retrieval Agent). Four representative L...  \n",
            "1997  nt and Retrieval Agent). Four representative L...  \n",
            "1998  nt and Retrieval Agent). Four representative L...  \n",
            "1999  d refine their outputs by leveraging their pre...  \n",
            "\n",
            "[2000 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68Vq1cPstUBc"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(os.path.join(OUTPUT_FOLDER, \"qa_pairs2.csv\"))\n",
        "\n",
        "# Add quality checks\n",
        "df = df[df['answer'].str.len() > 50]  # Remove short answers\n",
        "df = df.drop_duplicates(subset=['question'])  # Remove duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                               question  \\\n",
            "0     What are the potential societal benefits of us...   \n",
            "1     How does the paper \"Program synthesis with lar...   \n",
            "2     What is the focus of the research by Chen et a...   \n",
            "3     Describe the approach taken by Chen et al. in ...   \n",
            "4     What is the significance of teaching large lan...   \n",
            "...                                                 ...   \n",
            "1995  What framework is SARA built on, and what is i...   \n",
            "1996  Which datasets are used to evaluate the method...   \n",
            "1997  How does SARA's performance compare to other b...   \n",
            "1998  What evaluation metrics are used for HotpotQA,...   \n",
            "1999  What problem does the proposed framework aim t...   \n",
            "\n",
            "                                                 answer  \\\n",
            "0     The potential societal benefits include creati...   \n",
            "1     The paper explores the use of large language m...   \n",
            "2     The research focuses on improving code generat...   \n",
            "3     The approach involves leveraging a divide-and-...   \n",
            "4     Teaching large language models to self-debug c...   \n",
            "...                                                 ...   \n",
            "1995  SARA is built on the open-source multi-agent f...   \n",
            "1996  The datasets used are HotpotQA for multi-hop q...   \n",
            "1997  SARA consistently outperforms all baseline met...   \n",
            "1998  For HotpotQA, a GPT-4 judge assesses answer co...   \n",
            "1999  The proposed framework addresses the \"when to ...   \n",
            "\n",
            "                                                context  \n",
            "0     orithmic domains, without any fine-tuning or r...  \n",
            "1     orithmic domains, without any fine-tuning or r...  \n",
            "2     orithmic domains, without any fine-tuning or r...  \n",
            "3     orithmic domains, without any fine-tuning or r...  \n",
            "4     orithmic domains, without any fine-tuning or r...  \n",
            "...                                                 ...  \n",
            "1995  nt and Retrieval Agent). Four representative L...  \n",
            "1996  nt and Retrieval Agent). Four representative L...  \n",
            "1997  nt and Retrieval Agent). Four representative L...  \n",
            "1998  nt and Retrieval Agent). Four representative L...  \n",
            "1999  d refine their outputs by leveraging their pre...  \n",
            "\n",
            "[1956 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGOu-joX6dIt"
      },
      "source": [
        "#### Evaluate Q&A Pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "672adf23a2d74ed78b20af1139c908db",
            "ba641046177a41ccb22d485fbab9feae",
            "264b6cca88b1467f8cf5394ec60b07b6",
            "ab85aa12dc0c4a8caa787fb3c0365995",
            "d41dd2ba35c4431bb0980e4ad2733bef",
            "293e152ef31b4d3382195c58e4573545",
            "5e05e3a600884e88bfff517634f94ae1",
            "8f4885414fd14a11bbc5dbd2aa54dc39",
            "ef73c478b42a426ab1aa49e613068bb6",
            "ad9b7fa0c30f49d8ad086e8fbd2c4116",
            "cd0f83162e264bc8aee11e60d10b35ac"
          ]
        },
        "id": "VZ2kq7B85YPQ",
        "outputId": "18faa3ac-50e0-406e-8f14-44723fbe1d70"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 1956/1956 [38:32<00:00,  1.18s/it] \n"
          ]
        }
      ],
      "source": [
        "from ragas import evaluate\n",
        "from ragas.metrics import answer_relevancy\n",
        "from datasets import Dataset\n",
        "\n",
        "# Fix 1: Prepare dataset with RAGAS-required columns\n",
        "df[\"retrieved_contexts\"] = df[\"context\"].apply(lambda x: [x])  # Convert context to list of contexts\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# Fix 2: Rename columns to match RAGAS expectations\n",
        "dataset = dataset.rename_column(\"context\", \"ground_truths\")  # Optional: Only if using ground_truths\n",
        "dataset = dataset.rename_column(\"retrieved_contexts\", \"contexts\")  # Required for faithfulness metric\n",
        "\n",
        "# Evaluate\n",
        "score = evaluate(\n",
        "    dataset,\n",
        "    metrics=[answer_relevancy],\n",
        "    column_map={\n",
        "        \"question\": \"question\",\n",
        "        \"answer\": \"answer\",\n",
        "        \"contexts\": \"contexts\"  # Explicitly map retrieved contexts\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "score_df = score.to_pandas()  # Convert RAGAS scores to DataFrame\n",
        "score_df.to_csv(os.path.join(OUTPUT_FOLDER, \"score_df2.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                             user_input  \\\n",
            "0     What are the potential societal benefits of us...   \n",
            "1     How does the paper \"Program synthesis with lar...   \n",
            "2     What is the focus of the research by Chen et a...   \n",
            "3     Describe the approach taken by Chen et al. in ...   \n",
            "4     What is the significance of teaching large lan...   \n",
            "...                                                 ...   \n",
            "1951  What framework is SARA built on, and what is i...   \n",
            "1952  Which datasets are used to evaluate the method...   \n",
            "1953  How does SARA's performance compare to other b...   \n",
            "1954  What evaluation metrics are used for HotpotQA,...   \n",
            "1955  What problem does the proposed framework aim t...   \n",
            "\n",
            "                                     retrieved_contexts  \\\n",
            "0     [orithmic domains, without any fine-tuning or ...   \n",
            "1     [orithmic domains, without any fine-tuning or ...   \n",
            "2     [orithmic domains, without any fine-tuning or ...   \n",
            "3     [orithmic domains, without any fine-tuning or ...   \n",
            "4     [orithmic domains, without any fine-tuning or ...   \n",
            "...                                                 ...   \n",
            "1951  [nt and Retrieval Agent). Four representative ...   \n",
            "1952  [nt and Retrieval Agent). Four representative ...   \n",
            "1953  [nt and Retrieval Agent). Four representative ...   \n",
            "1954  [nt and Retrieval Agent). Four representative ...   \n",
            "1955  [d refine their outputs by leveraging their pr...   \n",
            "\n",
            "                                               response  answer_relevancy  \n",
            "0     The potential societal benefits include creati...          0.900291  \n",
            "1     The paper explores the use of large language m...          0.916672  \n",
            "2     The research focuses on improving code generat...          0.912555  \n",
            "3     The approach involves leveraging a divide-and-...          0.817271  \n",
            "4     Teaching large language models to self-debug c...          0.935802  \n",
            "...                                                 ...               ...  \n",
            "1951  SARA is built on the open-source multi-agent f...          0.937393  \n",
            "1952  The datasets used are HotpotQA for multi-hop q...          0.869603  \n",
            "1953  SARA consistently outperforms all baseline met...          0.986902  \n",
            "1954  For HotpotQA, a GPT-4 judge assesses answer co...          0.924059  \n",
            "1955  The proposed framework addresses the \"when to ...          0.983718  \n",
            "\n",
            "[1956 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "print(score_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-LsA5Nj6g__"
      },
      "source": [
        "#### Filter Q&A Pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkuS4_WQ8MPn",
        "outputId": "2035d376-70ac-4c2e-b3ee-21cb7c304e14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of rows with answer_relevancy less than 0.8: 106\n"
          ]
        }
      ],
      "source": [
        "low_relevancy_count = len(score_df[score_df['answer_relevancy'] < 0.8])\n",
        "print(f\"Number of rows with answer_relevancy less than 0.8: {low_relevancy_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of rows after filtering: 1850\n"
          ]
        }
      ],
      "source": [
        "filtered_score_df = score_df[score_df['answer_relevancy'] >= 0.8]\n",
        "print(f\"Number of rows after filtering: {len(filtered_score_df)}\")\n",
        "\n",
        "filtered_score_df.to_csv(os.path.join(OUTPUT_FOLDER, \"filtered_score_df2.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYs5cQjS8SOe",
        "outputId": "f0ab2cf8-3022-4e02-cf97-44131e9f456c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                             user_input  \\\n",
            "0     What are the potential societal benefits of us...   \n",
            "1     How does the paper \"Program synthesis with lar...   \n",
            "2     What is the focus of the research by Chen et a...   \n",
            "3     Describe the approach taken by Chen et al. in ...   \n",
            "4     What is the significance of teaching large lan...   \n",
            "...                                                 ...   \n",
            "1845  What framework is SARA built on, and what is i...   \n",
            "1846  Which datasets are used to evaluate the method...   \n",
            "1847  How does SARA's performance compare to other b...   \n",
            "1848  What evaluation metrics are used for HotpotQA,...   \n",
            "1849  What problem does the proposed framework aim t...   \n",
            "\n",
            "                                     retrieved_contexts  \\\n",
            "0     ['orithmic domains, without any fine-tuning or...   \n",
            "1     ['orithmic domains, without any fine-tuning or...   \n",
            "2     ['orithmic domains, without any fine-tuning or...   \n",
            "3     ['orithmic domains, without any fine-tuning or...   \n",
            "4     ['orithmic domains, without any fine-tuning or...   \n",
            "...                                                 ...   \n",
            "1845  ['nt and Retrieval Agent). Four representative...   \n",
            "1846  ['nt and Retrieval Agent). Four representative...   \n",
            "1847  ['nt and Retrieval Agent). Four representative...   \n",
            "1848  ['nt and Retrieval Agent). Four representative...   \n",
            "1849  ['d refine their outputs by leveraging their p...   \n",
            "\n",
            "                                               response  answer_relevancy  \n",
            "0     The potential societal benefits include creati...          0.900291  \n",
            "1     The paper explores the use of large language m...          0.916672  \n",
            "2     The research focuses on improving code generat...          0.912555  \n",
            "3     The approach involves leveraging a divide-and-...          0.817271  \n",
            "4     Teaching large language models to self-debug c...          0.935802  \n",
            "...                                                 ...               ...  \n",
            "1845  SARA is built on the open-source multi-agent f...          0.937393  \n",
            "1846  The datasets used are HotpotQA for multi-hop q...          0.869603  \n",
            "1847  SARA consistently outperforms all baseline met...          0.986902  \n",
            "1848  For HotpotQA, a GPT-4 judge assesses answer co...          0.924059  \n",
            "1849  The proposed framework addresses the \"when to ...          0.983718  \n",
            "\n",
            "[1850 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "filtered_score_df = pd.read_csv(os.path.join(OUTPUT_FOLDER, \"filtered_score_df2.csv\"))\n",
        "# Output scores\n",
        "print(filtered_score_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "A7-jJK0ilYbF",
        "outputId": "3433eb2a-28f7-40be-fbef-4f68f3cfc43c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkBIjW3r6vDK"
      },
      "source": [
        "#### Loaded a Pre-trained Model\n",
        "\n",
        "**Method followed will be finetuning after quantization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# model_name = \"Qwen/Qwen2.5-3B-Instruct\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement triton (from versions: none)\n",
            "ERROR: No matching distribution found for triton\n"
          ]
        }
      ],
      "source": [
        "%pip install triton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhUqQpFTmavl",
        "outputId": "8aa55c52-318d-4469-8698-1e134ae8b3e9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Gayanuka Amarasuriya\\AppData\\Local\\Temp\\ipykernel_25668\\985647689.py:1: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
            "\n",
            "Please restructure your imports with 'import unsloth' at the top of your file.\n",
            "  from unsloth import FastLanguageModel\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'triton'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m max_seq_length = \u001b[32m2048\u001b[39m \u001b[38;5;66;03m# Choose any! We auto support RoPE Scaling internally!\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Gayanuka Amarasuriya\\Documents\\Competitions\\IntelliHack_5.0\\Code\\Intellihack_Sidemen_3\\.venv\\Lib\\site-packages\\unsloth\\__init__.py:141\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    139\u001b[39m \u001b[38;5;66;03m# For Gradio HF Spaces?\u001b[39;00m\n\u001b[32m    140\u001b[39m \u001b[38;5;66;03m# if \"SPACE_AUTHOR_NAME\" not in os.environ and \"SPACE_REPO_NAME\" not in os.environ:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtriton\u001b[39;00m\n\u001b[32m    142\u001b[39m libcuda_dirs = \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m Version(triton.__version__) >= Version(\u001b[33m\"\u001b[39m\u001b[33m3.0.0\u001b[39m\u001b[33m\"\u001b[39m):\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'triton'"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen2.5-3B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdXIafor7AJt"
      },
      "source": [
        "#### Applied LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IFLj760umatP"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 14,966,784 || all params: 3,100,905,472 || trainable%: 0.4827\n"
          ]
        }
      ],
      "source": [
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1sQ2_4d2marK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRoCiIA9oECx"
      },
      "source": [
        "#### Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_yO7_hl5maoi"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"data/processed_data/filtered_score_df2.csv\") # Changed load_csv to read_csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "z5EB0IMLmakL",
        "outputId": "7171b017-0b82-430a-f9f6-1c704c16d5d0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "user_input",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "retrieved_contexts",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "response",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "answer_relevancy",
                  "rawType": "float64",
                  "type": "float"
                }
              ],
              "conversionMethod": "pd.DataFrame",
              "ref": "9a602619-29e6-4876-a5e4-e3d2c0624d9a",
              "rows": [
                [
                  "0",
                  "What are the potential societal benefits of using large language models in algorithmic domains as mentioned in the text?",
                  "['orithmic domains, without any fine-tuning or re-training. Potential societal benefits include more trust- worthy AI systems for education, technical problem-solving, and decision-support applications. There are many broader societal consequences of our work, none of which we feel must be specifically highlighted here. References Austin, J., Odena, A., Nye, M. I., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C. J., Terry, M., Le, Q. V ., and Sutton, C. Program synthesis with large langu...']",
                  "The potential societal benefits include creating more trustworthy AI systems for education, technical problem-solving, and decision-support applications.",
                  "0.9002912729848314"
                ],
                [
                  "1",
                  "How does the paper \"Program synthesis with large language models\" contribute to the field, according to the references?",
                  "['orithmic domains, without any fine-tuning or re-training. Potential societal benefits include more trust- worthy AI systems for education, technical problem-solving, and decision-support applications. There are many broader societal consequences of our work, none of which we feel must be specifically highlighted here. References Austin, J., Odena, A., Nye, M. I., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C. J., Terry, M., Le, Q. V ., and Sutton, C. Program synthesis with large langu...']",
                  "The paper explores the use of large language models for program synthesis, potentially enhancing automated coding capabilities without requiring fine-tuning or re-training.",
                  "0.916671665340186"
                ],
                [
                  "2",
                  "What is the focus of the research by Chen et al. in \"Codet: Code generation with generated tests\"?",
                  "['orithmic domains, without any fine-tuning or re-training. Potential societal benefits include more trust- worthy AI systems for education, technical problem-solving, and decision-support applications. There are many broader societal consequences of our work, none of which we feel must be specifically highlighted here. References Austin, J., Odena, A., Nye, M. I., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C. J., Terry, M., Le, Q. V ., and Sutton, C. Program synthesis with large langu...']",
                  "The research focuses on improving code generation by incorporating generated tests, which can enhance the reliability and accuracy of automatically generated code.",
                  "0.9125550055779358"
                ],
                [
                  "3",
                  "Describe the approach taken by Chen et al. in \"Divide-and-conquer meets consensus\" for code generation.",
                  "['orithmic domains, without any fine-tuning or re-training. Potential societal benefits include more trust- worthy AI systems for education, technical problem-solving, and decision-support applications. There are many broader societal consequences of our work, none of which we feel must be specifically highlighted here. References Austin, J., Odena, A., Nye, M. I., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C. J., Terry, M., Le, Q. V ., and Sutton, C. Program synthesis with large langu...']",
                  "The approach involves leveraging a divide-and-conquer strategy combined with consensus mechanisms to enhance function-based code generation.",
                  "0.8172711908649445"
                ],
                [
                  "4",
                  "What is the significance of teaching large language models to self-debug as discussed by Chen et al.?",
                  "['orithmic domains, without any fine-tuning or re-training. Potential societal benefits include more trust- worthy AI systems for education, technical problem-solving, and decision-support applications. There are many broader societal consequences of our work, none of which we feel must be specifically highlighted here. References Austin, J., Odena, A., Nye, M. I., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C. J., Terry, M., Le, Q. V ., and Sutton, C. Program synthesis with large langu...']",
                  "Teaching large language models to self-debug can improve their ability to autonomously identify and correct errors, leading to more robust and reliable AI systems.",
                  "0.9358021674219148"
                ]
              ],
              "shape": {
                "columns": 4,
                "rows": 5
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>answer_relevancy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What are the potential societal benefits of us...</td>\n",
              "      <td>['orithmic domains, without any fine-tuning or...</td>\n",
              "      <td>The potential societal benefits include creati...</td>\n",
              "      <td>0.900291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How does the paper \"Program synthesis with lar...</td>\n",
              "      <td>['orithmic domains, without any fine-tuning or...</td>\n",
              "      <td>The paper explores the use of large language m...</td>\n",
              "      <td>0.916672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What is the focus of the research by Chen et a...</td>\n",
              "      <td>['orithmic domains, without any fine-tuning or...</td>\n",
              "      <td>The research focuses on improving code generat...</td>\n",
              "      <td>0.912555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Describe the approach taken by Chen et al. in ...</td>\n",
              "      <td>['orithmic domains, without any fine-tuning or...</td>\n",
              "      <td>The approach involves leveraging a divide-and-...</td>\n",
              "      <td>0.817271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What is the significance of teaching large lan...</td>\n",
              "      <td>['orithmic domains, without any fine-tuning or...</td>\n",
              "      <td>Teaching large language models to self-debug c...</td>\n",
              "      <td>0.935802</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0  What are the potential societal benefits of us...   \n",
              "1  How does the paper \"Program synthesis with lar...   \n",
              "2  What is the focus of the research by Chen et a...   \n",
              "3  Describe the approach taken by Chen et al. in ...   \n",
              "4  What is the significance of teaching large lan...   \n",
              "\n",
              "                                  retrieved_contexts  \\\n",
              "0  ['orithmic domains, without any fine-tuning or...   \n",
              "1  ['orithmic domains, without any fine-tuning or...   \n",
              "2  ['orithmic domains, without any fine-tuning or...   \n",
              "3  ['orithmic domains, without any fine-tuning or...   \n",
              "4  ['orithmic domains, without any fine-tuning or...   \n",
              "\n",
              "                                            response  answer_relevancy  \n",
              "0  The potential societal benefits include creati...          0.900291  \n",
              "1  The paper explores the use of large language m...          0.916672  \n",
              "2  The research focuses on improving code generat...          0.912555  \n",
              "3  The approach involves leveraging a divide-and-...          0.817271  \n",
              "4  Teaching large language models to self-debug c...          0.935802  "
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC9r9gjmoeyH"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8PQzXBEQmaYN"
      },
      "outputs": [],
      "source": [
        "df = df.drop(columns=[\"retrieved_contexts\", \"answer_relevancy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "A0kjua7HpArg",
        "outputId": "d31373ac-14e9-484e-cce8-f07c193064e3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "user_input",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "response",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "conversionMethod": "pd.DataFrame",
              "ref": "a8d6995e-9238-490c-b54f-a3c82e08b7a9",
              "rows": [
                [
                  "0",
                  "What are the potential societal benefits of using large language models in algorithmic domains as mentioned in the text?",
                  "The potential societal benefits include creating more trustworthy AI systems for education, technical problem-solving, and decision-support applications."
                ],
                [
                  "1",
                  "How does the paper \"Program synthesis with large language models\" contribute to the field, according to the references?",
                  "The paper explores the use of large language models for program synthesis, potentially enhancing automated coding capabilities without requiring fine-tuning or re-training."
                ],
                [
                  "2",
                  "What is the focus of the research by Chen et al. in \"Codet: Code generation with generated tests\"?",
                  "The research focuses on improving code generation by incorporating generated tests, which can enhance the reliability and accuracy of automatically generated code."
                ],
                [
                  "3",
                  "Describe the approach taken by Chen et al. in \"Divide-and-conquer meets consensus\" for code generation.",
                  "The approach involves leveraging a divide-and-conquer strategy combined with consensus mechanisms to enhance function-based code generation."
                ],
                [
                  "4",
                  "What is the significance of teaching large language models to self-debug as discussed by Chen et al.?",
                  "Teaching large language models to self-debug can improve their ability to autonomously identify and correct errors, leading to more robust and reliable AI systems."
                ]
              ],
              "shape": {
                "columns": 2,
                "rows": 5
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>response</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What are the potential societal benefits of us...</td>\n",
              "      <td>The potential societal benefits include creati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How does the paper \"Program synthesis with lar...</td>\n",
              "      <td>The paper explores the use of large language m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What is the focus of the research by Chen et a...</td>\n",
              "      <td>The research focuses on improving code generat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Describe the approach taken by Chen et al. in ...</td>\n",
              "      <td>The approach involves leveraging a divide-and-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What is the significance of teaching large lan...</td>\n",
              "      <td>Teaching large language models to self-debug c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0  What are the potential societal benefits of us...   \n",
              "1  How does the paper \"Program synthesis with lar...   \n",
              "2  What is the focus of the research by Chen et a...   \n",
              "3  Describe the approach taken by Chen et al. in ...   \n",
              "4  What is the significance of teaching large lan...   \n",
              "\n",
              "                                            response  \n",
              "0  The potential societal benefits include creati...  \n",
              "1  The paper explores the use of large language m...  \n",
              "2  The research focuses on improving code generat...  \n",
              "3  The approach involves leveraging a divide-and-...  \n",
              "4  Teaching large language models to self-debug c...  "
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyNofbN2pCI-",
        "outputId": "8e160d29-31cf-498d-93ca-fa3706381950"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['user_input', 'response'],\n",
            "    num_rows: 1850\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "dataset = Dataset.from_pandas(df)\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA-DDCZC7M8p"
      },
      "source": [
        "#### Tokenized the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "37c0e593a39544b5b5be32f4c762220d",
            "8b8b8012dc1144aebb02b020c0cc4947",
            "8098190d587f4359be3485bbba49826d",
            "d9a01f21854a4b59b6c3933c5e8155ab",
            "72151dd451384642aacd4bf4b464b7f5",
            "38b857b8af9e401caf3f4b8db85c2485",
            "05601d2c3c6a40aa9d45cd098791947d",
            "ce9df8a395cf4443909265e23a71a528",
            "0004a708844242dc986a0169945de001",
            "4f6980bf057e470e90b1e6b2499b29b7",
            "c8e37f16fe5f4d1cb3553e73df914f61"
          ]
        },
        "id": "bcUZfu4Bp-g6",
        "outputId": "6b0c213a-5840-459f-b843-cc625d3eeb91"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 1850/1850 [00:01<00:00, 930.94 examples/s]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"user_input\"],\n",
        "        text_target=examples[\"response\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=2048,\n",
        "    )\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xjsX_7Z7Tpq"
      },
      "source": [
        "#### Set Up Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./qwen-finetuned\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    warmup_steps=50,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-4,\n",
        "    bf16=True,  # Use bfloat16 if available\n",
        "    gradient_checkpointing=True,\n",
        "    logging_steps=10,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Fine-Tuned the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "XVeAkW1-qL5W",
        "outputId": "d4313eea-a543-4e4c-9e14-3b4ea4cc80d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Gayanuka Amarasuriya\\AppData\\Local\\Temp\\ipykernel_25668\\4084770052.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "device() received an invalid combination of arguments - got (NoneType), but expected one of:\n * (torch.device device)\n      didn't match because some of the arguments have invalid types: (!NoneType!)\n * (str type, int index = -1)\n",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m trainer = Trainer(\n\u001b[32m      2\u001b[39m     model=model,\n\u001b[32m      3\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m     tokenizer=tokenizer\n\u001b[32m      7\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Gayanuka Amarasuriya\\Documents\\Competitions\\IntelliHack_5.0\\Code\\Intellihack_Sidemen_3\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2241\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2239\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2240\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Gayanuka Amarasuriya\\Documents\\Competitions\\IntelliHack_5.0\\Code\\Intellihack_Sidemen_3\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2365\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2363\u001b[39m         model = \u001b[38;5;28mself\u001b[39m.accelerator.prepare(\u001b[38;5;28mself\u001b[39m.model)\n\u001b[32m   2364\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2365\u001b[39m         model, \u001b[38;5;28mself\u001b[39m.optimizer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2366\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2367\u001b[39m     \u001b[38;5;66;03m# to handle cases wherein we pass \"DummyScheduler\" such as when it is specified in DeepSpeed config.\u001b[39;00m\n\u001b[32m   2368\u001b[39m     model, \u001b[38;5;28mself\u001b[39m.optimizer, \u001b[38;5;28mself\u001b[39m.lr_scheduler = \u001b[38;5;28mself\u001b[39m.accelerator.prepare(\n\u001b[32m   2369\u001b[39m         \u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.optimizer, \u001b[38;5;28mself\u001b[39m.lr_scheduler\n\u001b[32m   2370\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Gayanuka Amarasuriya\\Documents\\Competitions\\IntelliHack_5.0\\Code\\Intellihack_Sidemen_3\\.venv\\Lib\\site-packages\\accelerate\\accelerator.py:1389\u001b[39m, in \u001b[36mAccelerator.prepare\u001b[39m\u001b[34m(self, device_placement, *args)\u001b[39m\n\u001b[32m   1387\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fp8_backend == \u001b[33m\"\u001b[39m\u001b[33mMSAMP\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1388\u001b[39m         args, device_placement = \u001b[38;5;28mself\u001b[39m._prepare_msamp(*args, device_placement=device_placement)\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     result = \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m   1390\u001b[39m         \u001b[38;5;28mself\u001b[39m._prepare_one(obj, first_pass=\u001b[38;5;28;01mTrue\u001b[39;00m, device_placement=d) \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args, device_placement)\n\u001b[32m   1391\u001b[39m     )\n\u001b[32m   1392\u001b[39m     result = \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m._prepare_one(obj, device_placement=d) \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, device_placement))\n\u001b[32m   1393\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tpu_should_fix_optimizer:\n\u001b[32m   1394\u001b[39m     \u001b[38;5;66;03m# 2. grabbing new model parameters\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Gayanuka Amarasuriya\\Documents\\Competitions\\IntelliHack_5.0\\Code\\Intellihack_Sidemen_3\\.venv\\Lib\\site-packages\\accelerate\\accelerator.py:1390\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   1387\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fp8_backend == \u001b[33m\"\u001b[39m\u001b[33mMSAMP\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1388\u001b[39m         args, device_placement = \u001b[38;5;28mself\u001b[39m._prepare_msamp(*args, device_placement=device_placement)\n\u001b[32m   1389\u001b[39m     result = \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m-> \u001b[39m\u001b[32m1390\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_pass\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[43m=\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args, device_placement)\n\u001b[32m   1391\u001b[39m     )\n\u001b[32m   1392\u001b[39m     result = \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m._prepare_one(obj, device_placement=d) \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, device_placement))\n\u001b[32m   1393\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tpu_should_fix_optimizer:\n\u001b[32m   1394\u001b[39m     \u001b[38;5;66;03m# 2. grabbing new model parameters\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Gayanuka Amarasuriya\\Documents\\Competitions\\IntelliHack_5.0\\Code\\Intellihack_Sidemen_3\\.venv\\Lib\\site-packages\\accelerate\\accelerator.py:1263\u001b[39m, in \u001b[36mAccelerator._prepare_one\u001b[39m\u001b[34m(self, obj, first_pass, device_placement)\u001b[39m\n\u001b[32m   1261\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.prepare_data_loader(obj, device_placement=device_placement)\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch.nn.Module):\n\u001b[32m-> \u001b[39m\u001b[32m1263\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepare_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1264\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch.optim.Optimizer):\n\u001b[32m   1265\u001b[39m     optimizer = \u001b[38;5;28mself\u001b[39m.prepare_optimizer(obj, device_placement=device_placement)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Gayanuka Amarasuriya\\Documents\\Competitions\\IntelliHack_5.0\\Code\\Intellihack_Sidemen_3\\.venv\\Lib\\site-packages\\accelerate\\accelerator.py:1488\u001b[39m, in \u001b[36mAccelerator.prepare_model\u001b[39m\u001b[34m(self, model, device_placement, evaluation_mode)\u001b[39m\n\u001b[32m   1485\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device.type == \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_bitsandbytes_multi_backend_available():\n\u001b[32m   1486\u001b[39m     \u001b[38;5;66;03m# bnb with multi-backend supports CPU which don't need to check index.\u001b[39;00m\n\u001b[32m   1487\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1488\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_device_index\u001b[49m\u001b[43m)\u001b[49m != \u001b[38;5;28mself\u001b[39m.device:\n\u001b[32m   1489\u001b[39m     \u001b[38;5;66;03m# if on the first device (GPU 0) we don't care\u001b[39;00m\n\u001b[32m   1490\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.device.index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (current_device_index != \u001b[32m0\u001b[39m):\n\u001b[32m   1491\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1492\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt train a model that has been loaded in 8-bit or 4-bit precision on a different device than the one \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1493\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33myou\u001b[39m\u001b[33m'\u001b[39m\u001b[33mre training on. Make sure you loaded the model on the correct device using for example `device_map=\u001b[39m\u001b[33m{\u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:torch.cuda.current_device()}` or `device_map=\u001b[39m\u001b[33m{\u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:torch.xpu.current_device()}`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1494\u001b[39m         )\n",
            "\u001b[31mTypeError\u001b[39m: device() received an invalid combination of arguments - got (NoneType), but expected one of:\n * (torch.device device)\n      didn't match because some of the arguments have invalid types: (!NoneType!)\n * (str type, int index = -1)\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwdX9g5yVSBm",
        "outputId": "a1b59ebb-24cd-4748-b74b-0a9a6d8ee33c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1785.0773 seconds used for training.\n",
            "29.75 minutes used for training.\n",
            "Peak reserved memory = 4.012 GB.\n",
            "Peak reserved memory for training = 1.596 GB.\n",
            "Peak reserved memory % of max memory = 27.217 %.\n",
            "Peak reserved memory for training % of max memory = 10.827 %.\n"
          ]
        }
      ],
      "source": [
        "#@title Show final memory and time stats\n",
        "import torch\n",
        "\n",
        "# Get initial GPU memory usage\n",
        "start_gpu_memory = round(torch.cuda.memory_allocated() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(torch.cuda.get_device_properties(0).total_memory / 1024 / 1024 / 1024, 3) # Assuming you're using GPU 0\n",
        "\n",
        "\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "# Access training runtime from trainer.state\n",
        "print(f\"{trainer.state.log_history[-1]['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer.state.log_history[-1]['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOZj0h-e7lp3"
      },
      "source": [
        "#### Saved the Fine-Tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0pZOtsbqQ-B"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"qwen-finetuned\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNTet3WxaQoz",
        "outputId": "90b191b8-3751-4c8e-abd9-e7bcd9d22fc8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('qwen-finetuned_tokenizer/tokenizer_config.json',\n",
              " 'qwen-finetuned_tokenizer/special_tokens_map.json',\n",
              " 'qwen-finetuned_tokenizer/vocab.json',\n",
              " 'qwen-finetuned_tokenizer/merges.txt',\n",
              " 'qwen-finetuned_tokenizer/added_tokens.json',\n",
              " 'qwen-finetuned_tokenizer/tokenizer.json')"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.save_pretrained(\"qwen-finetuned_tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc1Ksno9av0S",
        "outputId": "82734c6d-4187-4d51-ac68-e02fb58e3b77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaAaEw1cbAPv",
        "outputId": "04e072d5-cdfd-4917-def0-7ab99faca2dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdata\u001b[0m/   \u001b[01;34mhuggingface_tokenizers_cache\u001b[0m/  \u001b[01;34mqwen-finetuned_tokenizer\u001b[0m/  \u001b[01;34munsloth_compiled_cache\u001b[0m/\n",
            "\u001b[01;34mdrive\u001b[0m/  \u001b[01;34mqwen-finetuned\u001b[0m/                \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPq6S9RTbPv5",
        "outputId": "b9ecbfab-1646-460c-b008-2ece84ebd095"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: qwen-finetuned_tokenizer/ (stored 0%)\n",
            "  adding: qwen-finetuned_tokenizer/added_tokens.json (deflated 67%)\n",
            "  adding: qwen-finetuned_tokenizer/vocab.json (deflated 61%)\n",
            "  adding: qwen-finetuned_tokenizer/tokenizer.json (deflated 81%)\n",
            "  adding: qwen-finetuned_tokenizer/tokenizer_config.json (deflated 83%)\n",
            "  adding: qwen-finetuned_tokenizer/special_tokens_map.json (deflated 69%)\n",
            "  adding: qwen-finetuned_tokenizer/merges.txt (deflated 57%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r qwen-finetuned_tokenizer.zip qwen-finetuned_tokenizer/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "MMPKG5SDbhfq",
        "outputId": "8b0802cc-ca97-404c-9fad-2c64c465a7e6"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_aebbb20c-ad54-4795-ae3b-4090eb437d56\", \"qwen-finetuned_tokenizer.zip\", 3945828)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download('qwen-finetuned_tokenizer.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPJ7bisthFhC",
        "outputId": "94e311e1-f062-4a19-913a-1a42f955936d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.3.9: Fast Qwen2 patching. Transformers: 4.48.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "model_path = \"qwen-finetuned\"\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyaLPyQshQWz",
        "outputId": "c8f2a4ea-85a1-4229-ff94-ba8e23b56524"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: You have 1 CPUs. Using `safe_serialization` is 10x slower.\n",
            "We shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\n",
            "To force `safe_serialization`, set it to `None` instead.\n",
            "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
            "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
            "Unsloth: Will remove a cached repo with size 2.4G\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 4.51 out of 12.67 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 36/36 [00:02<00:00, 16.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Unsloth: Saving model/pytorch_model-00001-of-00002.bin...\n",
            "Unsloth: Saving model/pytorch_model-00002-of-00002.bin...\n",
            "Done.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Converting qwen2 model. Can use fast conversion = False.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
            "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] might take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
            "Unsloth: CMAKE detected. Finalizing some steps for installation.\n",
            "Unsloth: [1] Converting model at model into f16 GGUF format.\n",
            "The output location will be /content/model/unsloth.F16.gguf\n",
            "This might take 3 minutes...\n",
            "INFO:hf-to-gguf:Loading model: model\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'pytorch_model.bin.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00001-of-00002.bin'\n",
            "INFO:hf-to-gguf:token_embd.weight,         torch.float16 --> F16, shape = {2048, 151936}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.bias,         torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.bias,         torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.bias,         torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.bias,         torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.bias,         torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.bias,         torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.bias,         torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.bias,         torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.bias,         torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.bias,         torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.28.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.28.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.28.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.28.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.28.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.28.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.28.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00002-of-00002.bin'\n",
            "INFO:hf-to-gguf:blk.28.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.28.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.28.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.28.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.28.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.29.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.29.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.29.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.29.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.29.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.29.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.29.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.29.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.29.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.29.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.29.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.29.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.30.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.30.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.30.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.30.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.30.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.30.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.30.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.30.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.30.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.30.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.30.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.30.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.31.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.31.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.31.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.31.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.31.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.31.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.31.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.31.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.31.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.31.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.31.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.31.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.32.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.32.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.32.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.32.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.32.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.32.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.32.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.32.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.32.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.32.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.32.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.32.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.33.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.33.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.33.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.33.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.33.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.33.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.33.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.33.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.33.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.33.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.33.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.33.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.34.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.34.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.34.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.34.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.34.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.34.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.34.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.34.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.34.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.34.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.34.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.34.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.35.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.35.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.35.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.35.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.35.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.35.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.35.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.35.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.35.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.35.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.35.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.35.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:output_norm.weight,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 32768\n",
            "INFO:hf-to-gguf:gguf: embedding length = 2048\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 11008\n",
            "INFO:hf-to-gguf:gguf: head count = 16\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 2\n",
            "INFO:hf-to-gguf:gguf: rope theta = 1000000.0\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n",
            "INFO:hf-to-gguf:gguf: file type = 1\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "2025-03-08 20:59:10.617212: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1741467550.646057   25603 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1741467550.655860   25603 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "INFO:gguf.vocab:Adding 151387 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type eos to 151645\n",
            "INFO:gguf.vocab:Setting special token type pad to 151654\n",
            "INFO:gguf.vocab:Setting add_bos_token to False\n",
            "INFO:gguf.vocab:Setting chat_template to {%- if tools %}\n",
            "    {{- '<|im_start|>system\\n' }}\n",
            "    {%- if messages[0]['role'] == 'system' %}\n",
            "        {{- messages[0]['content'] }}\n",
            "    {%- else %}\n",
            "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
            "    {%- endif %}\n",
            "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
            "    {%- for tool in tools %}\n",
            "        {{- \"\\n\" }}\n",
            "        {{- tool | tojson }}\n",
            "    {%- endfor %}\n",
            "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
            "{%- else %}\n",
            "    {%- if messages[0]['role'] == 'system' %}\n",
            "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
            "    {%- else %}\n",
            "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
            "    {%- endif %}\n",
            "{%- endif %}\n",
            "{%- for message in messages %}\n",
            "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
            "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
            "    {%- elif message.role == \"assistant\" %}\n",
            "        {{- '<|im_start|>' + message.role }}\n",
            "        {%- if message.content %}\n",
            "            {{- '\\n' + message.content }}\n",
            "        {%- endif %}\n",
            "        {%- for tool_call in message.tool_calls %}\n",
            "            {%- if tool_call.function is defined %}\n",
            "                {%- set tool_call = tool_call.function %}\n",
            "            {%- endif %}\n",
            "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
            "            {{- tool_call.name }}\n",
            "            {{- '\", \"arguments\": ' }}\n",
            "            {{- tool_call.arguments | tojson }}\n",
            "            {{- '}\\n</tool_call>' }}\n",
            "        {%- endfor %}\n",
            "        {{- '<|im_end|>\\n' }}\n",
            "    {%- elif message.role == \"tool\" %}\n",
            "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
            "            {{- '<|im_start|>user' }}\n",
            "        {%- endif %}\n",
            "        {{- '\\n<tool_response>\\n' }}\n",
            "        {{- message.content }}\n",
            "        {{- '\\n</tool_response>' }}\n",
            "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
            "            {{- '<|im_end|>\\n' }}\n",
            "        {%- endif %}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|im_start|>assistant\\n' }}\n",
            "{%- endif %}\n",
            "\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/content/model/unsloth.F16.gguf: n_tensors = 434, total_size = 6.2G\n",
            "Writing: 100%|██████████| 6.17G/6.17G [01:34<00:00, 65.4Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to /content/model/unsloth.F16.gguf\n",
            "Unsloth: Conversion completed! Output location: /content/model/unsloth.F16.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\n",
            "main: build = 4857 (0fd7ca7a)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/model/unsloth.F16.gguf' to '/content/model/unsloth.Q4_K_M.gguf' as Q4_K_M using 4 threads\n",
            "llama_model_loader: loaded meta data with 26 key-value pairs and 434 tensors from /content/model/unsloth.F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 3b Instruct Unsloth Bnb 4bit\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = instruct-unsloth-bnb-4bit\n",
            "llama_model_loader: - kv   5:                           general.basename str              = qwen2.5\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
            "llama_model_loader: - kv   7:                          qwen2.block_count u32              = 36\n",
            "llama_model_loader: - kv   8:                       qwen2.context_length u32              = 32768\n",
            "llama_model_loader: - kv   9:                     qwen2.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv  10:                  qwen2.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv  11:                 qwen2.attention.head_count u32              = 16\n",
            "llama_model_loader: - kv  12:              qwen2.attention.head_count_kv u32              = 2\n",
            "llama_model_loader: - kv  13:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  14:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  15:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\n",
            "llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151654\n",
            "llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
            "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  181 tensors\n",
            "llama_model_loader: - type  f16:  253 tensors\n",
            "[   1/ 434]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   2/ 434]                    token_embd.weight - [ 2048, 151936,     1,     1], type =    f16, converting to q6_K .. size =   593.50 MiB ->   243.43 MiB\n",
            "[   3/ 434]                    blk.0.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[   4/ 434]                  blk.0.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[   5/ 434]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   6/ 434]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[   7/ 434]                    blk.0.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   8/ 434]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[   9/ 434]                    blk.0.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  10/ 434]                  blk.0.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[  11/ 434]                blk.0.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[  12/ 434]                blk.0.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  13/ 434]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  14/ 434]                  blk.0.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  15/ 434]                    blk.1.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  16/ 434]                  blk.1.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  17/ 434]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  18/ 434]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  19/ 434]                    blk.1.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  20/ 434]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  21/ 434]                    blk.1.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  22/ 434]                  blk.1.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[  23/ 434]                blk.1.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[  24/ 434]                blk.1.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  25/ 434]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  26/ 434]                  blk.1.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  27/ 434]                    blk.2.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  28/ 434]                  blk.2.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  29/ 434]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  30/ 434]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  31/ 434]                    blk.2.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  32/ 434]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  33/ 434]                    blk.2.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  34/ 434]                  blk.2.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[  35/ 434]                blk.2.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[  36/ 434]                blk.2.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  37/ 434]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  38/ 434]                  blk.2.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  39/ 434]                    blk.3.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  40/ 434]                  blk.3.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  41/ 434]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  42/ 434]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  43/ 434]                    blk.3.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  44/ 434]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  45/ 434]                    blk.3.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  46/ 434]                  blk.3.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[  47/ 434]                blk.3.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[  48/ 434]                blk.3.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  49/ 434]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  50/ 434]                  blk.3.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  51/ 434]                    blk.4.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  52/ 434]                  blk.4.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  53/ 434]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  54/ 434]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  55/ 434]                    blk.4.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  56/ 434]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  57/ 434]                    blk.4.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  58/ 434]                  blk.4.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  59/ 434]                blk.4.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  60/ 434]                blk.4.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  61/ 434]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  62/ 434]                  blk.4.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  63/ 434]                    blk.5.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  64/ 434]                  blk.5.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  65/ 434]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  66/ 434]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  67/ 434]                    blk.5.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  68/ 434]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  69/ 434]                    blk.5.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  70/ 434]                  blk.5.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  71/ 434]                blk.5.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  72/ 434]                blk.5.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  73/ 434]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  74/ 434]                  blk.5.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  75/ 434]                    blk.6.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  76/ 434]                  blk.6.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  77/ 434]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  78/ 434]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  79/ 434]                    blk.6.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  80/ 434]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  81/ 434]                    blk.6.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  82/ 434]                  blk.6.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[  83/ 434]                blk.6.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[  84/ 434]                blk.6.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  85/ 434]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  86/ 434]                  blk.6.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  87/ 434]                    blk.7.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  88/ 434]                  blk.7.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  89/ 434]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  90/ 434]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  91/ 434]                    blk.7.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  92/ 434]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  93/ 434]                    blk.7.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  94/ 434]                  blk.7.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  95/ 434]                blk.7.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  96/ 434]                blk.7.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  97/ 434]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  98/ 434]                  blk.7.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  99/ 434]                    blk.8.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 100/ 434]                  blk.8.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 101/ 434]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 102/ 434]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 103/ 434]                    blk.8.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 104/ 434]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 105/ 434]                    blk.8.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 106/ 434]                  blk.8.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 107/ 434]                blk.8.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 108/ 434]                blk.8.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 109/ 434]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 110/ 434]                  blk.8.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 111/ 434]                    blk.9.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 112/ 434]                  blk.9.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 113/ 434]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 114/ 434]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 115/ 434]                    blk.9.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 116/ 434]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 117/ 434]                    blk.9.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 118/ 434]                  blk.9.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[ 119/ 434]                blk.9.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[ 120/ 434]                blk.9.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 121/ 434]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 122/ 434]                  blk.9.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 123/ 434]                   blk.10.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 124/ 434]                 blk.10.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 125/ 434]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 126/ 434]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 127/ 434]                   blk.10.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 128/ 434]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 129/ 434]                   blk.10.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 130/ 434]                 blk.10.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 131/ 434]               blk.10.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 132/ 434]               blk.10.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 133/ 434]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 134/ 434]                 blk.10.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 135/ 434]                   blk.11.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 136/ 434]                 blk.11.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 137/ 434]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 138/ 434]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 139/ 434]                   blk.11.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 140/ 434]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 141/ 434]                   blk.11.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 142/ 434]                 blk.11.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 143/ 434]               blk.11.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 144/ 434]               blk.11.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 145/ 434]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 146/ 434]                 blk.11.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 147/ 434]                   blk.12.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 148/ 434]                 blk.12.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 149/ 434]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 150/ 434]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 151/ 434]                   blk.12.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 152/ 434]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 153/ 434]                   blk.12.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 154/ 434]                 blk.12.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[ 155/ 434]               blk.12.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[ 156/ 434]               blk.12.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 157/ 434]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 158/ 434]                 blk.12.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 159/ 434]                   blk.13.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 160/ 434]                 blk.13.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 161/ 434]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 162/ 434]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 163/ 434]                   blk.13.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 164/ 434]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 165/ 434]                   blk.13.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 166/ 434]                 blk.13.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 167/ 434]               blk.13.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 168/ 434]               blk.13.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 169/ 434]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 170/ 434]                 blk.13.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 171/ 434]                   blk.14.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 172/ 434]                 blk.14.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 173/ 434]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 174/ 434]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 175/ 434]                   blk.14.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 176/ 434]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 177/ 434]                   blk.14.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 178/ 434]                 blk.14.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 179/ 434]               blk.14.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 180/ 434]               blk.14.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 181/ 434]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 182/ 434]                 blk.14.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 183/ 434]                   blk.15.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 184/ 434]                 blk.15.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 185/ 434]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 186/ 434]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 187/ 434]                   blk.15.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 188/ 434]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 189/ 434]                   blk.15.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 190/ 434]                 blk.15.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[ 191/ 434]               blk.15.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[ 192/ 434]               blk.15.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 193/ 434]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 194/ 434]                 blk.15.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 195/ 434]                   blk.16.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 196/ 434]                 blk.16.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 197/ 434]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 198/ 434]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 199/ 434]                   blk.16.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 200/ 434]                 blk.16.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 201/ 434]                   blk.16.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 202/ 434]                 blk.16.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 203/ 434]               blk.16.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 204/ 434]               blk.16.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 205/ 434]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 206/ 434]                 blk.16.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 207/ 434]                   blk.17.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 208/ 434]                 blk.17.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 209/ 434]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 210/ 434]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 211/ 434]                   blk.17.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 212/ 434]                 blk.17.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 213/ 434]                   blk.17.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 214/ 434]                 blk.17.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 215/ 434]               blk.17.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 216/ 434]               blk.17.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 217/ 434]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 218/ 434]                 blk.17.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 219/ 434]                   blk.18.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 220/ 434]                 blk.18.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 221/ 434]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 222/ 434]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 223/ 434]                   blk.18.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 224/ 434]                 blk.18.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 225/ 434]                   blk.18.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 226/ 434]                 blk.18.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[ 227/ 434]               blk.18.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[ 228/ 434]               blk.18.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 229/ 434]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 230/ 434]                 blk.18.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 231/ 434]                   blk.19.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 232/ 434]                 blk.19.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 233/ 434]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 234/ 434]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 235/ 434]                   blk.19.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 236/ 434]                 blk.19.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 237/ 434]                   blk.19.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 238/ 434]                 blk.19.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 239/ 434]               blk.19.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 240/ 434]               blk.19.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 241/ 434]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 242/ 434]                 blk.19.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 243/ 434]                   blk.20.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 244/ 434]                 blk.20.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 245/ 434]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 246/ 434]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 247/ 434]                   blk.20.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 248/ 434]                 blk.20.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 249/ 434]                   blk.20.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 250/ 434]                 blk.20.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 251/ 434]               blk.20.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 252/ 434]               blk.20.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 253/ 434]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 254/ 434]                 blk.20.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 255/ 434]                   blk.21.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 256/ 434]                 blk.21.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 257/ 434]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 258/ 434]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 259/ 434]                   blk.21.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 260/ 434]                 blk.21.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 261/ 434]                   blk.21.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 262/ 434]                 blk.21.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[ 263/ 434]               blk.21.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[ 264/ 434]               blk.21.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 265/ 434]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 266/ 434]                 blk.21.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 267/ 434]                   blk.22.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 268/ 434]                 blk.22.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 269/ 434]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 270/ 434]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 271/ 434]                   blk.22.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 272/ 434]                 blk.22.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 273/ 434]                   blk.22.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 274/ 434]                 blk.22.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 275/ 434]               blk.22.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 276/ 434]               blk.22.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 277/ 434]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 278/ 434]                 blk.22.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 279/ 434]                   blk.23.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 280/ 434]                 blk.23.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 281/ 434]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 282/ 434]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 283/ 434]                   blk.23.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 284/ 434]                 blk.23.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 285/ 434]                   blk.23.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 286/ 434]                 blk.23.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 287/ 434]               blk.23.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 288/ 434]               blk.23.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 289/ 434]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 290/ 434]                 blk.23.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 291/ 434]                   blk.24.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 292/ 434]                 blk.24.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 293/ 434]              blk.24.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 294/ 434]            blk.24.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 295/ 434]                   blk.24.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 296/ 434]                 blk.24.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 297/ 434]                   blk.24.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 298/ 434]                 blk.24.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[ 299/ 434]               blk.24.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[ 300/ 434]               blk.24.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 301/ 434]               blk.24.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 302/ 434]                 blk.24.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 303/ 434]                   blk.25.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 304/ 434]                 blk.25.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 305/ 434]              blk.25.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 306/ 434]            blk.25.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 307/ 434]                   blk.25.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 308/ 434]                 blk.25.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 309/ 434]                   blk.25.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 310/ 434]                 blk.25.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 311/ 434]               blk.25.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 312/ 434]               blk.25.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 313/ 434]               blk.25.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 314/ 434]                 blk.25.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 315/ 434]                   blk.26.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 316/ 434]                 blk.26.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 317/ 434]              blk.26.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 318/ 434]            blk.26.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 319/ 434]                   blk.26.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 320/ 434]                 blk.26.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 321/ 434]                   blk.26.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 322/ 434]                 blk.26.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 323/ 434]               blk.26.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 324/ 434]               blk.26.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 325/ 434]               blk.26.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 326/ 434]                 blk.26.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 327/ 434]                   blk.27.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 328/ 434]                 blk.27.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 329/ 434]              blk.27.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 330/ 434]            blk.27.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 331/ 434]                   blk.27.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 332/ 434]                 blk.27.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 333/ 434]                   blk.27.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 334/ 434]                 blk.27.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[ 335/ 434]               blk.27.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[ 336/ 434]               blk.27.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 337/ 434]               blk.27.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 338/ 434]                 blk.27.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 339/ 434]                   blk.28.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 340/ 434]                 blk.28.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 341/ 434]              blk.28.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 342/ 434]            blk.28.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 343/ 434]                   blk.28.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 344/ 434]                 blk.28.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 345/ 434]                   blk.28.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 346/ 434]                 blk.28.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 347/ 434]               blk.28.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 348/ 434]               blk.28.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 349/ 434]               blk.28.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 350/ 434]                 blk.28.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 351/ 434]                   blk.29.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 352/ 434]                 blk.29.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 353/ 434]              blk.29.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 354/ 434]            blk.29.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 355/ 434]                   blk.29.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 356/ 434]                 blk.29.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 357/ 434]                   blk.29.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 358/ 434]                 blk.29.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 359/ 434]               blk.29.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 360/ 434]               blk.29.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 361/ 434]               blk.29.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 362/ 434]                 blk.29.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 363/ 434]                   blk.30.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 364/ 434]                 blk.30.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 365/ 434]              blk.30.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 366/ 434]            blk.30.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 367/ 434]                   blk.30.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 368/ 434]                 blk.30.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 369/ 434]                   blk.30.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 370/ 434]                 blk.30.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[ 371/ 434]               blk.30.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[ 372/ 434]               blk.30.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 373/ 434]               blk.30.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 374/ 434]                 blk.30.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 375/ 434]                   blk.31.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 376/ 434]                 blk.31.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 377/ 434]              blk.31.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 378/ 434]            blk.31.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 379/ 434]                   blk.31.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 380/ 434]                 blk.31.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 381/ 434]                   blk.31.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 382/ 434]                 blk.31.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[ 383/ 434]               blk.31.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[ 384/ 434]               blk.31.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 385/ 434]               blk.31.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 386/ 434]                 blk.31.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 387/ 434]                   blk.32.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 388/ 434]                 blk.32.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 389/ 434]              blk.32.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 390/ 434]            blk.32.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 391/ 434]                   blk.32.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 392/ 434]                 blk.32.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 393/ 434]                   blk.32.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 394/ 434]                 blk.32.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[ 395/ 434]               blk.32.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[ 396/ 434]               blk.32.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 397/ 434]               blk.32.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 398/ 434]                 blk.32.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 399/ 434]                   blk.33.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 400/ 434]                 blk.33.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 401/ 434]              blk.33.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 402/ 434]            blk.33.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 403/ 434]                   blk.33.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 404/ 434]                 blk.33.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 405/ 434]                   blk.33.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 406/ 434]                 blk.33.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[ 407/ 434]               blk.33.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[ 408/ 434]               blk.33.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 409/ 434]               blk.33.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 410/ 434]                 blk.33.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 411/ 434]                   blk.34.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 412/ 434]                 blk.34.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 413/ 434]              blk.34.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 414/ 434]            blk.34.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 415/ 434]                   blk.34.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 416/ 434]                 blk.34.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 417/ 434]                   blk.34.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 418/ 434]                 blk.34.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[ 419/ 434]               blk.34.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[ 420/ 434]               blk.34.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 421/ 434]               blk.34.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 422/ 434]                 blk.34.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 423/ 434]                   blk.35.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 424/ 434]                 blk.35.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 425/ 434]              blk.35.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 426/ 434]            blk.35.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 427/ 434]                   blk.35.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 428/ 434]                 blk.35.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 429/ 434]                   blk.35.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 430/ 434]                 blk.35.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[ 431/ 434]               blk.35.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[ 432/ 434]               blk.35.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 433/ 434]               blk.35.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 434/ 434]                 blk.35.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "llama_model_quantize_impl: model size  =  5886.42 MB\n",
            "llama_model_quantize_impl: quant size  =  1834.82 MB\n",
            "\n",
            "main: quantize time = 327691.83 ms\n",
            "main:    total time = 327691.83 ms\n",
            "Unsloth: Conversion completed! Output location: /content/model/unsloth.Q4_K_M.gguf\n"
          ]
        }
      ],
      "source": [
        "model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"q4_k_m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRu_R3RqsDVT",
        "outputId": "c2281c7f-df10-4526-d87d-0b54a720574f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is Deepseek R1? a a,, large and with, with and with  with,. language model,. that of\n"
          ]
        }
      ],
      "source": [
        "def generate_answer(question):\n",
        "    inputs = tokenizer(question, return_tensors=\"pt\").to(\"cuda\")\n",
        "    output = model.generate(\n",
        "      **inputs,\n",
        "      max_new_tokens=3000,\n",
        "      # num_beams=5,  # Number of beams to use\n",
        "      early_stopping=True  # Stop when the model generates an end-of-sequence token\n",
        "    )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(generate_answer(\"What is Deepseek R1?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VI_9XWjoEHT",
        "outputId": "272d5071-9bc3-4e1d-88f0-a72a9f4d2eac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is Deepseek R1? a, of and, for,,.,,,,,\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "qa_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=2000,\n",
        "    temperature=0.2,\n",
        ")\n",
        "print(qa_pipeline(\"What is Deepseek R1?\")[0][\"generated_text\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0004a708844242dc986a0169945de001": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "05601d2c3c6a40aa9d45cd098791947d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "264b6cca88b1467f8cf5394ec60b07b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f4885414fd14a11bbc5dbd2aa54dc39",
            "max": 916,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ef73c478b42a426ab1aa49e613068bb6",
            "value": 916
          }
        },
        "293e152ef31b4d3382195c58e4573545": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37c0e593a39544b5b5be32f4c762220d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8b8b8012dc1144aebb02b020c0cc4947",
              "IPY_MODEL_8098190d587f4359be3485bbba49826d",
              "IPY_MODEL_d9a01f21854a4b59b6c3933c5e8155ab"
            ],
            "layout": "IPY_MODEL_72151dd451384642aacd4bf4b464b7f5"
          }
        },
        "38b857b8af9e401caf3f4b8db85c2485": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f6980bf057e470e90b1e6b2499b29b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e05e3a600884e88bfff517634f94ae1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "672adf23a2d74ed78b20af1139c908db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ba641046177a41ccb22d485fbab9feae",
              "IPY_MODEL_264b6cca88b1467f8cf5394ec60b07b6",
              "IPY_MODEL_ab85aa12dc0c4a8caa787fb3c0365995"
            ],
            "layout": "IPY_MODEL_d41dd2ba35c4431bb0980e4ad2733bef"
          }
        },
        "72151dd451384642aacd4bf4b464b7f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8098190d587f4359be3485bbba49826d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce9df8a395cf4443909265e23a71a528",
            "max": 879,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0004a708844242dc986a0169945de001",
            "value": 879
          }
        },
        "8b8b8012dc1144aebb02b020c0cc4947": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38b857b8af9e401caf3f4b8db85c2485",
            "placeholder": "​",
            "style": "IPY_MODEL_05601d2c3c6a40aa9d45cd098791947d",
            "value": "Map: 100%"
          }
        },
        "8f4885414fd14a11bbc5dbd2aa54dc39": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab85aa12dc0c4a8caa787fb3c0365995": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad9b7fa0c30f49d8ad086e8fbd2c4116",
            "placeholder": "​",
            "style": "IPY_MODEL_cd0f83162e264bc8aee11e60d10b35ac",
            "value": " 916/916 [08:56&lt;00:00,  1.24it/s]"
          }
        },
        "ad9b7fa0c30f49d8ad086e8fbd2c4116": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba641046177a41ccb22d485fbab9feae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_293e152ef31b4d3382195c58e4573545",
            "placeholder": "​",
            "style": "IPY_MODEL_5e05e3a600884e88bfff517634f94ae1",
            "value": "Evaluating: 100%"
          }
        },
        "c8e37f16fe5f4d1cb3553e73df914f61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd0f83162e264bc8aee11e60d10b35ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce9df8a395cf4443909265e23a71a528": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d41dd2ba35c4431bb0980e4ad2733bef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9a01f21854a4b59b6c3933c5e8155ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f6980bf057e470e90b1e6b2499b29b7",
            "placeholder": "​",
            "style": "IPY_MODEL_c8e37f16fe5f4d1cb3553e73df914f61",
            "value": " 879/879 [00:00&lt;00:00, 1799.38 examples/s]"
          }
        },
        "ef73c478b42a426ab1aa49e613068bb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
